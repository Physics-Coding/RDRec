nohup: ignoring input
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of Solomon were not initialized from the model checkpoint at t5-small and are newly initialized: ['conv1d.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/beauty/
model_version                            0
task_num                                 5
prompt_num                               3
lr                                       0.0005
ratio                                    1:1:1:1
epochs                                   100
batch_size                               64
cuda                                     True
log_interval                             200
checkpoint                               ./checkpoint/beauty_1111/
endure_times                             5
exp_len                                  20
negative_num                             99
----------------------------------------ARGUMENTS----------------------------------------
[2025-05-13 18:14:07.784549]: Loading data
[2025-05-13 18:14:19.318365]: Start training
[2025-05-13 18:14:19.318466]: epoch 1
[2025-05-13 18:16:01.561525]: text loss 1.6825 |   200/ 6642 batches
[2025-05-13 18:17:44.006936]: text loss 1.4106 |   400/ 6642 batches
[2025-05-13 18:19:26.252173]: text loss 1.3350 |   600/ 6642 batches
[2025-05-13 18:21:09.081774]: text loss 1.3430 |   800/ 6642 batches
[2025-05-13 18:22:51.036967]: text loss 1.2980 |  1000/ 6642 batches
[2025-05-13 18:24:33.030471]: text loss 1.2990 |  1200/ 6642 batches
[2025-05-13 18:26:15.537602]: text loss 1.2612 |  1400/ 6642 batches
[2025-05-13 18:27:57.360158]: text loss 1.2694 |  1600/ 6642 batches
[2025-05-13 18:29:39.457623]: text loss 1.2655 |  1800/ 6642 batches
[2025-05-13 18:31:21.250769]: text loss 1.2169 |  2000/ 6642 batches
[2025-05-13 18:33:03.391968]: text loss 1.2541 |  2200/ 6642 batches
[2025-05-13 18:34:45.097236]: text loss 1.2284 |  2400/ 6642 batches
[2025-05-13 18:36:27.663052]: text loss 1.2379 |  2600/ 6642 batches
[2025-05-13 18:38:09.292647]: text loss 1.2380 |  2800/ 6642 batches
[2025-05-13 18:39:51.417617]: text loss 1.2138 |  3000/ 6642 batches
[2025-05-13 18:41:33.214907]: text loss 1.2106 |  3200/ 6642 batches
[2025-05-13 18:43:15.673632]: text loss 1.2346 |  3400/ 6642 batches
[2025-05-13 18:44:57.867664]: text loss 1.2111 |  3600/ 6642 batches
[2025-05-13 18:46:39.882749]: text loss 1.2184 |  3800/ 6642 batches
[2025-05-13 18:48:21.960289]: text loss 1.2202 |  4000/ 6642 batches
[2025-05-13 18:50:03.792813]: text loss 1.1979 |  4200/ 6642 batches
[2025-05-13 18:51:45.404386]: text loss 1.2032 |  4400/ 6642 batches
[2025-05-13 18:53:27.353020]: text loss 1.2207 |  4600/ 6642 batches
[2025-05-13 18:55:10.374798]: text loss 1.1884 |  4800/ 6642 batches
[2025-05-13 18:56:52.585385]: text loss 1.1765 |  5000/ 6642 batches
[2025-05-13 18:58:35.444063]: text loss 1.2037 |  5200/ 6642 batches
[2025-05-13 19:00:17.602026]: text loss 1.1928 |  5400/ 6642 batches
[2025-05-13 19:02:00.058031]: text loss 1.1594 |  5600/ 6642 batches
[2025-05-13 19:03:43.390910]: text loss 1.1878 |  5800/ 6642 batches
[2025-05-13 19:05:25.288651]: text loss 1.2376 |  6000/ 6642 batches
[2025-05-13 19:07:06.748056]: text loss 1.1675 |  6200/ 6642 batches
[2025-05-13 19:08:48.747396]: text loss 1.1602 |  6400/ 6642 batches
[2025-05-13 19:10:30.364425]: text loss 1.1336 |  6600/ 6642 batches
[2025-05-13 19:10:51.651869]: text loss 1.0734 |  6642/ 6642 batches
[2025-05-13 19:10:51.652557]: validation
[2025-05-13 19:10:54.561503]: explanation loss 2.3487
[2025-05-13 19:10:59.761917]: rationale loss 1.4010
[2025-05-13 19:11:10.181994]: sequential loss 2.3661
[2025-05-13 19:12:18.241608]: top-N loss 1.4818
[2025-05-13 19:12:18.710451]: epoch 2
[2025-05-13 19:13:39.516412]: text loss 1.1591 |  6800/ 6642 batches
[2025-05-13 19:15:22.225081]: text loss 1.1514 |  7000/ 6642 batches
[2025-05-13 19:17:04.373372]: text loss 1.1439 |  7200/ 6642 batches
[2025-05-13 19:18:46.414246]: text loss 1.1547 |  7400/ 6642 batches
[2025-05-13 19:20:28.244052]: text loss 1.1539 |  7600/ 6642 batches
[2025-05-13 19:22:10.140179]: text loss 1.1517 |  7800/ 6642 batches
[2025-05-13 19:23:52.211141]: text loss 1.1399 |  8000/ 6642 batches
[2025-05-13 19:25:34.355630]: text loss 1.1564 |  8200/ 6642 batches
[2025-05-13 19:27:16.388518]: text loss 1.1425 |  8400/ 6642 batches
[2025-05-13 19:28:59.063063]: text loss 1.1383 |  8600/ 6642 batches
[2025-05-13 19:30:40.833119]: text loss 1.1308 |  8800/ 6642 batches
[2025-05-13 19:32:22.699856]: text loss 1.1305 |  9000/ 6642 batches
[2025-05-13 19:34:04.530749]: text loss 1.1417 |  9200/ 6642 batches
[2025-05-13 19:35:46.316067]: text loss 1.1332 |  9400/ 6642 batches
[2025-05-13 19:37:28.088547]: text loss 1.1337 |  9600/ 6642 batches
[2025-05-13 19:39:09.753880]: text loss 1.1396 |  9800/ 6642 batches
[2025-05-13 19:40:51.879394]: text loss 1.1296 | 10000/ 6642 batches
[2025-05-13 19:42:34.387034]: text loss 1.1418 | 10200/ 6642 batches
[2025-05-13 19:44:16.284979]: text loss 1.1374 | 10400/ 6642 batches
[2025-05-13 19:45:58.850904]: text loss 1.1197 | 10600/ 6642 batches
[2025-05-13 19:47:40.601813]: text loss 1.1318 | 10800/ 6642 batches
[2025-05-13 19:49:22.333365]: text loss 1.1265 | 11000/ 6642 batches
[2025-05-13 19:51:04.070948]: text loss 1.1424 | 11200/ 6642 batches
[2025-05-13 19:52:45.735393]: text loss 1.1208 | 11400/ 6642 batches
[2025-05-13 19:54:28.487787]: text loss 1.1182 | 11600/ 6642 batches
[2025-05-13 19:56:10.314482]: text loss 1.1339 | 11800/ 6642 batches
[2025-05-13 19:57:53.019047]: text loss 1.1332 | 12000/ 6642 batches
[2025-05-13 19:59:34.740970]: text loss 1.1259 | 12200/ 6642 batches
[2025-05-13 20:01:16.988922]: text loss 1.1149 | 12400/ 6642 batches
[2025-05-13 20:02:59.261706]: text loss 1.1262 | 12600/ 6642 batches
[2025-05-13 20:04:40.949637]: text loss 1.1124 | 12800/ 6642 batches
[2025-05-13 20:06:22.307332]: text loss 1.1257 | 13000/ 6642 batches
[2025-05-13 20:08:03.807455]: text loss 1.1122 | 13200/ 6642 batches
[2025-05-13 20:08:46.428538]: text loss 1.1280 | 13284/ 6642 batches
[2025-05-13 20:08:46.429283]: validation
[2025-05-13 20:08:49.328137]: explanation loss 2.9084
[2025-05-13 20:08:54.508926]: rationale loss 1.3268
[2025-05-13 20:09:04.887834]: sequential loss 2.3131
[2025-05-13 20:10:13.150711]: top-N loss 1.4323
[2025-05-13 20:10:13.150799]: Endured 1 time(s)
[2025-05-13 20:10:13.150823]: epoch 3
[2025-05-13 20:11:12.156923]: text loss 1.1061 | 13400/ 6642 batches
[2025-05-13 20:12:53.862306]: text loss 1.1137 | 13600/ 6642 batches
[2025-05-13 20:14:35.802673]: text loss 1.1001 | 13800/ 6642 batches
[2025-05-13 20:16:18.498740]: text loss 1.1038 | 14000/ 6642 batches
[2025-05-13 20:18:00.560881]: text loss 1.1104 | 14200/ 6642 batches
[2025-05-13 20:19:43.036728]: text loss 1.0965 | 14400/ 6642 batches
[2025-05-13 20:21:24.292823]: text loss 1.0930 | 14600/ 6642 batches
[2025-05-13 20:23:06.539833]: text loss 1.0993 | 14800/ 6642 batches
[2025-05-13 20:24:48.211146]: text loss 1.1074 | 15000/ 6642 batches
[2025-05-13 20:26:30.121213]: text loss 1.1051 | 15200/ 6642 batches
[2025-05-13 20:28:12.042464]: text loss 1.1060 | 15400/ 6642 batches
[2025-05-13 20:29:54.193829]: text loss 1.1012 | 15600/ 6642 batches
[2025-05-13 20:31:36.451643]: text loss 1.0855 | 15800/ 6642 batches
[2025-05-13 20:33:18.394012]: text loss 1.1024 | 16000/ 6642 batches
[2025-05-13 20:34:59.981001]: text loss 1.0979 | 16200/ 6642 batches
[2025-05-13 20:36:42.469133]: text loss 1.0983 | 16400/ 6642 batches
[2025-05-13 20:38:24.077953]: text loss 1.0974 | 16600/ 6642 batches
[2025-05-13 20:40:06.162359]: text loss 1.0879 | 16800/ 6642 batches
[2025-05-13 20:41:48.200818]: text loss 1.0929 | 17000/ 6642 batches
[2025-05-13 20:43:30.899711]: text loss 1.1017 | 17200/ 6642 batches
[2025-05-13 20:45:12.602113]: text loss 1.0835 | 17400/ 6642 batches
[2025-05-13 20:46:54.858210]: text loss 1.0939 | 17600/ 6642 batches
[2025-05-13 20:48:37.105264]: text loss 1.0979 | 17800/ 6642 batches
[2025-05-13 20:50:18.720649]: text loss 1.0870 | 18000/ 6642 batches
[2025-05-13 20:52:00.353817]: text loss 1.1000 | 18200/ 6642 batches
[2025-05-13 20:53:42.997384]: text loss 1.0909 | 18400/ 6642 batches
[2025-05-13 20:55:25.040434]: text loss 1.0852 | 18600/ 6642 batches
[2025-05-13 20:57:08.449260]: text loss 1.0942 | 18800/ 6642 batches
[2025-05-13 20:58:50.378923]: text loss 1.0933 | 19000/ 6642 batches
[2025-05-13 21:00:32.075455]: text loss 1.0834 | 19200/ 6642 batches
[2025-05-13 21:02:13.863152]: text loss 1.0829 | 19400/ 6642 batches
[2025-05-13 21:03:55.827561]: text loss 1.0848 | 19600/ 6642 batches
[2025-05-13 21:05:37.814667]: text loss 1.0972 | 19800/ 6642 batches
[2025-05-13 21:06:42.006985]: text loss 1.0957 | 19926/ 6642 batches
[2025-05-13 21:06:42.007773]: validation
[2025-05-13 21:06:44.935689]: explanation loss 2.9844
[2025-05-13 21:06:50.142362]: rationale loss 1.2918
[2025-05-13 21:07:00.467282]: sequential loss 2.3269
[2025-05-13 21:08:08.215218]: top-N loss 1.3766
[2025-05-13 21:08:08.215307]: Endured 2 time(s)
[2025-05-13 21:08:08.215332]: epoch 4
[2025-05-13 21:08:45.821157]: text loss 1.0793 | 20000/ 6642 batches
[2025-05-13 21:10:28.886465]: text loss 1.0756 | 20200/ 6642 batches
[2025-05-13 21:12:11.034082]: text loss 1.0674 | 20400/ 6642 batches
[2025-05-13 21:13:52.740275]: text loss 1.0682 | 20600/ 6642 batches
[2025-05-13 21:15:34.826058]: text loss 1.0763 | 20800/ 6642 batches
[2025-05-13 21:17:16.823738]: text loss 1.0788 | 21000/ 6642 batches
[2025-05-13 21:18:58.892323]: text loss 1.0753 | 21200/ 6642 batches
[2025-05-13 21:20:41.051652]: text loss 1.0693 | 21400/ 6642 batches
[2025-05-13 21:22:23.061191]: text loss 1.0727 | 21600/ 6642 batches
[2025-05-13 21:24:04.963967]: text loss 1.0729 | 21800/ 6642 batches
[2025-05-13 21:25:46.583756]: text loss 1.0775 | 22000/ 6642 batches
[2025-05-13 21:27:28.886560]: text loss 1.0708 | 22200/ 6642 batches
[2025-05-13 21:29:11.161050]: text loss 1.0710 | 22400/ 6642 batches
[2025-05-13 21:30:52.983678]: text loss 1.0640 | 22600/ 6642 batches
[2025-05-13 21:32:34.708789]: text loss 1.0691 | 22800/ 6642 batches
[2025-05-13 21:34:16.724130]: text loss 1.0721 | 23000/ 6642 batches
[2025-05-13 21:35:58.767919]: text loss 1.0680 | 23200/ 6642 batches
[2025-05-13 21:37:40.638599]: text loss 1.0688 | 23400/ 6642 batches
[2025-05-13 21:39:22.795931]: text loss 1.0751 | 23600/ 6642 batches
[2025-05-13 21:41:04.773808]: text loss 1.0716 | 23800/ 6642 batches
[2025-05-13 21:42:47.323148]: text loss 1.0769 | 24000/ 6642 batches
[2025-05-13 21:44:29.420525]: text loss 1.0714 | 24200/ 6642 batches
[2025-05-13 21:46:11.452826]: text loss 1.0682 | 24400/ 6642 batches
[2025-05-13 21:47:53.641642]: text loss 1.0686 | 24600/ 6642 batches
[2025-05-13 21:49:35.697303]: text loss 1.0662 | 24800/ 6642 batches
[2025-05-13 21:51:17.648553]: text loss 1.0714 | 25000/ 6642 batches
[2025-05-13 21:52:59.326133]: text loss 1.0649 | 25200/ 6642 batches
[2025-05-13 21:54:40.910744]: text loss 1.0726 | 25400/ 6642 batches
[2025-05-13 21:56:23.218083]: text loss 1.0728 | 25600/ 6642 batches
[2025-05-13 21:58:05.542622]: text loss 1.0719 | 25800/ 6642 batches
[2025-05-13 21:59:47.401936]: text loss 1.0591 | 26000/ 6642 batches
[2025-05-13 22:01:29.338368]: text loss 1.0619 | 26200/ 6642 batches
[2025-05-13 22:03:11.165359]: text loss 1.0665 | 26400/ 6642 batches
[2025-05-13 22:04:37.300581]: text loss 1.0690 | 26568/ 6642 batches
[2025-05-13 22:04:37.302189]: validation
[2025-05-13 22:04:40.197915]: explanation loss 2.8599
[2025-05-13 22:04:45.387416]: rationale loss 1.3004
[2025-05-13 22:04:55.674265]: sequential loss 2.2984
[2025-05-13 22:06:03.619013]: top-N loss 1.3476
[2025-05-13 22:06:03.619100]: Endured 3 time(s)
[2025-05-13 22:06:03.619121]: epoch 5
[2025-05-13 22:06:20.100020]: text loss 1.0548 | 26600/ 6642 batches
[2025-05-13 22:08:01.786197]: text loss 1.0439 | 26800/ 6642 batches
[2025-05-13 22:09:43.657596]: text loss 1.0485 | 27000/ 6642 batches
[2025-05-13 22:11:25.845207]: text loss 1.0569 | 27200/ 6642 batches
[2025-05-13 22:13:08.074183]: text loss 1.0512 | 27400/ 6642 batches
[2025-05-13 22:14:51.014452]: text loss 1.0604 | 27600/ 6642 batches
[2025-05-13 22:16:33.305267]: text loss 1.0515 | 27800/ 6642 batches
[2025-05-13 22:18:15.269291]: text loss 1.0534 | 28000/ 6642 batches
[2025-05-13 22:19:57.364411]: text loss 1.0631 | 28200/ 6642 batches
[2025-05-13 22:21:39.175530]: text loss 1.0535 | 28400/ 6642 batches
[2025-05-13 22:23:21.636604]: text loss 1.0549 | 28600/ 6642 batches
[2025-05-13 22:25:06.497760]: text loss 1.0531 | 28800/ 6642 batches
[2025-05-13 22:26:50.513794]: text loss 1.0515 | 29000/ 6642 batches
[2025-05-13 22:28:32.516060]: text loss 1.0500 | 29200/ 6642 batches
[2025-05-13 22:30:14.560769]: text loss 1.0536 | 29400/ 6642 batches
[2025-05-13 22:31:56.426591]: text loss 1.0458 | 29600/ 6642 batches
[2025-05-13 22:33:38.414927]: text loss 1.0488 | 29800/ 6642 batches
[2025-05-13 22:35:20.762111]: text loss 1.0530 | 30000/ 6642 batches
[2025-05-13 22:37:02.820429]: text loss 1.0572 | 30200/ 6642 batches
[2025-05-13 22:38:44.686858]: text loss 1.0478 | 30400/ 6642 batches
[2025-05-13 22:40:26.307026]: text loss 1.0450 | 30600/ 6642 batches
[2025-05-13 22:42:08.437719]: text loss 1.0510 | 30800/ 6642 batches
[2025-05-13 22:43:50.114512]: text loss 1.0454 | 31000/ 6642 batches
[2025-05-13 22:45:32.069805]: text loss 1.0515 | 31200/ 6642 batches
[2025-05-13 22:47:14.166797]: text loss 1.0461 | 31400/ 6642 batches
[2025-05-13 22:48:57.039156]: text loss 1.0448 | 31600/ 6642 batches
[2025-05-13 22:50:39.297333]: text loss 1.0365 | 31800/ 6642 batches
[2025-05-13 22:52:21.671637]: text loss 1.0468 | 32000/ 6642 batches
[2025-05-13 22:54:03.688112]: text loss 1.0555 | 32200/ 6642 batches
[2025-05-13 22:55:46.303724]: text loss 1.0393 | 32400/ 6642 batches
[2025-05-13 22:57:28.138266]: text loss 1.0455 | 32600/ 6642 batches
[2025-05-13 22:59:10.512312]: text loss 1.0502 | 32800/ 6642 batches
[2025-05-13 23:00:53.112119]: text loss 1.0425 | 33000/ 6642 batches
[2025-05-13 23:02:35.425991]: text loss 1.0593 | 33200/ 6642 batches
[2025-05-13 23:02:40.499751]: text loss 1.0608 | 33210/ 6642 batches
[2025-05-13 23:02:40.500444]: validation
[2025-05-13 23:02:43.396416]: explanation loss 2.1538
[2025-05-13 23:02:48.611565]: rationale loss 1.2730
[2025-05-13 23:02:59.042207]: sequential loss 2.2508
[2025-05-13 23:04:07.036373]: top-N loss 1.3117
[2025-05-13 23:04:07.427276]: epoch 6
[2025-05-13 23:05:44.003897]: text loss 1.0322 | 33400/ 6642 batches
[2025-05-13 23:07:25.850214]: text loss 1.0287 | 33600/ 6642 batches
[2025-05-13 23:09:07.601274]: text loss 1.0243 | 33800/ 6642 batches
[2025-05-13 23:10:49.809091]: text loss 1.0420 | 34000/ 6642 batches
[2025-05-13 23:12:31.922539]: text loss 1.0263 | 34200/ 6642 batches
[2025-05-13 23:14:14.306282]: text loss 1.0335 | 34400/ 6642 batches
[2025-05-13 23:15:56.024935]: text loss 1.0302 | 34600/ 6642 batches
[2025-05-13 23:17:37.843015]: text loss 1.0279 | 34800/ 6642 batches
[2025-05-13 23:19:19.965086]: text loss 1.0294 | 35000/ 6642 batches
[2025-05-13 23:21:01.837469]: text loss 1.0393 | 35200/ 6642 batches
[2025-05-13 23:22:43.941172]: text loss 1.0289 | 35400/ 6642 batches
[2025-05-13 23:24:26.039496]: text loss 1.0277 | 35600/ 6642 batches
[2025-05-13 23:26:08.029300]: text loss 1.0291 | 35800/ 6642 batches
[2025-05-13 23:27:49.631957]: text loss 1.0341 | 36000/ 6642 batches
[2025-05-13 23:29:31.132585]: text loss 1.0313 | 36200/ 6642 batches
[2025-05-13 23:31:13.112563]: text loss 1.0320 | 36400/ 6642 batches
[2025-05-13 23:32:55.300796]: text loss 1.0402 | 36600/ 6642 batches
[2025-05-13 23:34:36.791851]: text loss 1.0296 | 36800/ 6642 batches
[2025-05-13 23:36:18.629787]: text loss 1.0278 | 37000/ 6642 batches
[2025-05-13 23:38:00.274002]: text loss 1.0362 | 37200/ 6642 batches
[2025-05-13 23:39:42.214646]: text loss 1.0281 | 37400/ 6642 batches
[2025-05-13 23:41:24.047018]: text loss 1.0375 | 37600/ 6642 batches
[2025-05-13 23:43:05.666707]: text loss 1.0279 | 37800/ 6642 batches
[2025-05-13 23:44:47.442828]: text loss 1.0320 | 38000/ 6642 batches
[2025-05-13 23:46:29.042081]: text loss 1.0373 | 38200/ 6642 batches
[2025-05-13 23:48:11.397913]: text loss 1.0302 | 38400/ 6642 batches
[2025-05-13 23:49:53.424739]: text loss 1.0348 | 38600/ 6642 batches
[2025-05-13 23:51:34.873479]: text loss 1.0403 | 38800/ 6642 batches
[2025-05-13 23:53:16.977299]: text loss 1.0300 | 39000/ 6642 batches
[2025-05-13 23:54:58.519252]: text loss 1.0450 | 39200/ 6642 batches
[2025-05-13 23:56:40.420035]: text loss 1.0211 | 39400/ 6642 batches
[2025-05-13 23:58:22.256904]: text loss 1.0409 | 39600/ 6642 batches
[2025-05-14 00:00:04.110565]: text loss 1.0294 | 39800/ 6642 batches
[2025-05-14 00:00:30.468473]: text loss 1.0236 | 39852/ 6642 batches
[2025-05-14 00:00:30.469206]: validation
[2025-05-14 00:00:33.339640]: explanation loss 2.1279
[2025-05-14 00:00:38.551462]: rationale loss 1.2716
[2025-05-14 00:00:48.980592]: sequential loss 2.2395
[2025-05-14 00:01:56.986821]: top-N loss 1.2977
[2025-05-14 00:01:57.391159]: epoch 7
[2025-05-14 00:03:12.912051]: text loss 1.0074 | 40000/ 6642 batches
[2025-05-14 00:04:54.983414]: text loss 1.0171 | 40200/ 6642 batches
[2025-05-14 00:06:37.034756]: text loss 1.0213 | 40400/ 6642 batches
[2025-05-14 00:08:19.306804]: text loss 1.0160 | 40600/ 6642 batches
[2025-05-14 00:10:01.578537]: text loss 1.0169 | 40800/ 6642 batches
[2025-05-14 00:11:43.319761]: text loss 1.0104 | 41000/ 6642 batches
[2025-05-14 00:13:25.314773]: text loss 1.0144 | 41200/ 6642 batches
[2025-05-14 00:15:07.533738]: text loss 1.0113 | 41400/ 6642 batches
[2025-05-14 00:16:49.429595]: text loss 1.0254 | 41600/ 6642 batches
[2025-05-14 00:18:32.151983]: text loss 1.0207 | 41800/ 6642 batches
[2025-05-14 00:20:13.886361]: text loss 1.0102 | 42000/ 6642 batches
[2025-05-14 00:21:55.710819]: text loss 1.0238 | 42200/ 6642 batches
[2025-05-14 00:23:37.188600]: text loss 1.0140 | 42400/ 6642 batches
[2025-05-14 00:25:19.867709]: text loss 1.0167 | 42600/ 6642 batches
[2025-05-14 00:27:01.464243]: text loss 1.0224 | 42800/ 6642 batches
[2025-05-14 00:28:43.397462]: text loss 1.0108 | 43000/ 6642 batches
[2025-05-14 00:30:26.462463]: text loss 1.0049 | 43200/ 6642 batches
[2025-05-14 00:32:08.131535]: text loss 1.0137 | 43400/ 6642 batches
[2025-05-14 00:33:50.415681]: text loss 1.0114 | 43600/ 6642 batches
[2025-05-14 00:35:31.958749]: text loss 1.0130 | 43800/ 6642 batches
[2025-05-14 00:37:14.337349]: text loss 1.0290 | 44000/ 6642 batches
[2025-05-14 00:38:56.379405]: text loss 1.0159 | 44200/ 6642 batches
[2025-05-14 00:40:38.260404]: text loss 1.0186 | 44400/ 6642 batches
[2025-05-14 00:42:20.338266]: text loss 1.0206 | 44600/ 6642 batches
[2025-05-14 00:44:01.888754]: text loss 1.0128 | 44800/ 6642 batches
[2025-05-14 00:45:44.144533]: text loss 1.0164 | 45000/ 6642 batches
[2025-05-14 00:47:26.336892]: text loss 1.0232 | 45200/ 6642 batches
[2025-05-14 00:49:08.239792]: text loss 1.0119 | 45400/ 6642 batches
[2025-05-14 00:50:50.016743]: text loss 1.0126 | 45600/ 6642 batches
[2025-05-14 00:52:32.259804]: text loss 1.0140 | 45800/ 6642 batches
[2025-05-14 00:54:14.471348]: text loss 1.0142 | 46000/ 6642 batches
[2025-05-14 00:55:56.914341]: text loss 1.0156 | 46200/ 6642 batches
[2025-05-14 00:57:39.048150]: text loss 1.0188 | 46400/ 6642 batches
[2025-05-14 00:58:26.890525]: text loss 1.0226 | 46494/ 6642 batches
[2025-05-14 00:58:26.891229]: validation
[2025-05-14 00:58:29.771681]: explanation loss 2.1270
[2025-05-14 00:58:34.979448]: rationale loss 1.2850
[2025-05-14 00:58:45.411348]: sequential loss 2.2084
[2025-05-14 00:59:53.601866]: top-N loss 1.2754
[2025-05-14 00:59:54.091811]: epoch 8
[2025-05-14 01:00:48.011482]: text loss 0.9979 | 46600/ 6642 batches
[2025-05-14 01:02:29.885078]: text loss 0.9911 | 46800/ 6642 batches
[2025-05-14 01:04:11.573534]: text loss 0.9947 | 47000/ 6642 batches
[2025-05-14 01:05:53.429684]: text loss 1.0046 | 47200/ 6642 batches
[2025-05-14 01:07:35.546026]: text loss 1.0008 | 47400/ 6642 batches
[2025-05-14 01:09:17.702211]: text loss 1.0016 | 47600/ 6642 batches
[2025-05-14 01:10:59.644384]: text loss 0.9986 | 47800/ 6642 batches
[2025-05-14 01:12:42.178394]: text loss 0.9988 | 48000/ 6642 batches
[2025-05-14 01:14:24.779173]: text loss 1.0030 | 48200/ 6642 batches
[2025-05-14 01:16:07.846860]: text loss 1.0069 | 48400/ 6642 batches
[2025-05-14 01:17:49.579901]: text loss 1.0024 | 48600/ 6642 batches
[2025-05-14 01:19:31.595889]: text loss 1.0032 | 48800/ 6642 batches
[2025-05-14 01:21:14.240756]: text loss 1.0041 | 49000/ 6642 batches
[2025-05-14 01:22:57.216865]: text loss 1.0018 | 49200/ 6642 batches
[2025-05-14 01:24:39.848710]: text loss 0.9987 | 49400/ 6642 batches
[2025-05-14 01:26:22.901384]: text loss 0.9936 | 49600/ 6642 batches
[2025-05-14 01:28:05.945707]: text loss 1.0022 | 49800/ 6642 batches
[2025-05-14 01:29:48.813155]: text loss 1.0064 | 50000/ 6642 batches
[2025-05-14 01:31:30.440667]: text loss 1.0112 | 50200/ 6642 batches
[2025-05-14 01:33:11.497975]: text loss 1.0028 | 50400/ 6642 batches
[2025-05-14 01:34:52.748237]: text loss 1.0086 | 50600/ 6642 batches
[2025-05-14 01:36:33.917907]: text loss 0.9916 | 50800/ 6642 batches
[2025-05-14 01:38:15.336744]: text loss 1.0067 | 51000/ 6642 batches
[2025-05-14 01:39:56.681237]: text loss 1.0101 | 51200/ 6642 batches
[2025-05-14 01:41:39.780130]: text loss 0.9979 | 51400/ 6642 batches
[2025-05-14 01:43:22.982224]: text loss 1.0095 | 51600/ 6642 batches
[2025-05-14 01:45:06.238813]: text loss 1.0068 | 51800/ 6642 batches
[2025-05-14 01:46:49.557951]: text loss 0.9978 | 52000/ 6642 batches
[2025-05-14 01:48:32.729374]: text loss 0.9962 | 52200/ 6642 batches
[2025-05-14 01:50:16.019014]: text loss 1.0012 | 52400/ 6642 batches
[2025-05-14 01:51:59.166695]: text loss 1.0072 | 52600/ 6642 batches
[2025-05-14 01:53:42.281091]: text loss 1.0043 | 52800/ 6642 batches
[2025-05-14 01:55:25.441729]: text loss 1.0024 | 53000/ 6642 batches
[2025-05-14 01:56:35.552566]: text loss 1.0042 | 53136/ 6642 batches
[2025-05-14 01:56:35.553241]: validation
[2025-05-14 01:56:38.432633]: explanation loss 2.1127
[2025-05-14 01:56:43.628993]: rationale loss 1.2701
[2025-05-14 01:56:53.955415]: sequential loss 2.2177
[2025-05-14 01:58:01.261032]: top-N loss 1.2639
[2025-05-14 01:58:01.699709]: epoch 9
[2025-05-14 01:58:34.724981]: text loss 0.9817 | 53200/ 6642 batches
[2025-05-14 02:00:17.714912]: text loss 0.9844 | 53400/ 6642 batches
[2025-05-14 02:02:00.754920]: text loss 0.9909 | 53600/ 6642 batches
[2025-05-14 02:03:43.886309]: text loss 0.9810 | 53800/ 6642 batches
[2025-05-14 02:05:27.023807]: text loss 0.9818 | 54000/ 6642 batches
[2025-05-14 02:07:09.510208]: text loss 0.9776 | 54200/ 6642 batches
[2025-05-14 02:08:52.248490]: text loss 0.9822 | 54400/ 6642 batches
[2025-05-14 02:10:35.379380]: text loss 0.9862 | 54600/ 6642 batches
[2025-05-14 02:12:17.974928]: text loss 0.9857 | 54800/ 6642 batches
[2025-05-14 02:13:59.059164]: text loss 0.9831 | 55000/ 6642 batches
[2025-05-14 02:15:41.140137]: text loss 0.9883 | 55200/ 6642 batches
[2025-05-14 02:17:23.169894]: text loss 1.0011 | 55400/ 6642 batches
[2025-05-14 02:19:06.315637]: text loss 0.9880 | 55600/ 6642 batches
[2025-05-14 02:20:47.681560]: text loss 0.9835 | 55800/ 6642 batches
[2025-05-14 02:22:29.019022]: text loss 0.9856 | 56000/ 6642 batches
[2025-05-14 02:24:10.383551]: text loss 0.9934 | 56200/ 6642 batches
[2025-05-14 02:25:53.221639]: text loss 0.9893 | 56400/ 6642 batches
[2025-05-14 02:27:36.255275]: text loss 0.9872 | 56600/ 6642 batches
[2025-05-14 02:29:19.229303]: text loss 0.9913 | 56800/ 6642 batches
[2025-05-14 02:31:02.225374]: text loss 0.9846 | 57000/ 6642 batches
[2025-05-14 02:32:44.639210]: text loss 0.9941 | 57200/ 6642 batches
[2025-05-14 02:34:27.313122]: text loss 0.9883 | 57400/ 6642 batches
[2025-05-14 02:36:08.979916]: text loss 0.9866 | 57600/ 6642 batches
[2025-05-14 02:37:50.294461]: text loss 0.9882 | 57800/ 6642 batches
[2025-05-14 02:39:33.345906]: text loss 0.9985 | 58000/ 6642 batches
[2025-05-14 02:41:16.406289]: text loss 0.9882 | 58200/ 6642 batches
[2025-05-14 02:42:59.505912]: text loss 0.9882 | 58400/ 6642 batches
[2025-05-14 02:44:42.710937]: text loss 0.9867 | 58600/ 6642 batches
[2025-05-14 02:46:25.785467]: text loss 0.9988 | 58800/ 6642 batches
[2025-05-14 02:48:08.852955]: text loss 0.9892 | 59000/ 6642 batches
[2025-05-14 02:49:51.894045]: text loss 0.9897 | 59200/ 6642 batches
[2025-05-14 02:51:34.949519]: text loss 0.9938 | 59400/ 6642 batches
[2025-05-14 02:53:17.990886]: text loss 0.9852 | 59600/ 6642 batches
[2025-05-14 02:54:49.675210]: text loss 0.9854 | 59778/ 6642 batches
[2025-05-14 02:54:49.675854]: validation
[2025-05-14 02:54:52.542325]: explanation loss 2.0914
[2025-05-14 02:54:57.744054]: rationale loss 1.2387
[2025-05-14 02:55:08.020745]: sequential loss 2.2062
[2025-05-14 02:56:15.600300]: top-N loss 1.2507
[2025-05-14 02:56:16.034511]: epoch 10
[2025-05-14 02:56:27.221336]: text loss 0.9724 | 59800/ 6642 batches
[2025-05-14 02:58:08.337762]: text loss 0.9731 | 60000/ 6642 batches
[2025-05-14 02:59:49.607862]: text loss 0.9773 | 60200/ 6642 batches
[2025-05-14 03:01:31.436443]: text loss 0.9763 | 60400/ 6642 batches
[2025-05-14 03:03:14.530476]: text loss 0.9775 | 60600/ 6642 batches
[2025-05-14 03:04:57.565881]: text loss 0.9691 | 60800/ 6642 batches
[2025-05-14 03:06:40.552102]: text loss 0.9701 | 61000/ 6642 batches
[2025-05-14 03:08:21.869835]: text loss 0.9716 | 61200/ 6642 batches
[2025-05-14 03:10:04.466811]: text loss 0.9820 | 61400/ 6642 batches
[2025-05-14 03:11:47.652205]: text loss 0.9745 | 61600/ 6642 batches
[2025-05-14 03:13:30.648482]: text loss 0.9701 | 61800/ 6642 batches
[2025-05-14 03:15:13.748670]: text loss 0.9668 | 62000/ 6642 batches
[2025-05-14 03:16:56.862414]: text loss 0.9736 | 62200/ 6642 batches
[2025-05-14 03:18:39.857755]: text loss 0.9732 | 62400/ 6642 batches
[2025-05-14 03:20:22.865432]: text loss 0.9869 | 62600/ 6642 batches
[2025-05-14 03:22:05.445092]: text loss 0.9833 | 62800/ 6642 batches
[2025-05-14 03:23:48.469474]: text loss 0.9876 | 63000/ 6642 batches
[2025-05-14 03:25:31.538908]: text loss 0.9685 | 63200/ 6642 batches
[2025-05-14 03:27:14.677884]: text loss 0.9793 | 63400/ 6642 batches
[2025-05-14 03:28:56.797349]: text loss 0.9774 | 63600/ 6642 batches
[2025-05-14 03:30:39.684198]: text loss 0.9695 | 63800/ 6642 batches
[2025-05-14 03:32:20.916846]: text loss 0.9689 | 64000/ 6642 batches
[2025-05-14 03:34:02.542513]: text loss 0.9704 | 64200/ 6642 batches
[2025-05-14 03:35:43.912408]: text loss 0.9775 | 64400/ 6642 batches
[2025-05-14 03:37:25.319065]: text loss 0.9782 | 64600/ 6642 batches
[2025-05-14 03:39:06.697246]: text loss 0.9779 | 64800/ 6642 batches
[2025-05-14 03:40:49.696411]: text loss 0.9753 | 65000/ 6642 batches
[2025-05-14 03:42:32.759146]: text loss 0.9775 | 65200/ 6642 batches
[2025-05-14 03:44:15.806425]: text loss 0.9753 | 65400/ 6642 batches
[2025-05-14 03:45:58.840812]: text loss 0.9838 | 65600/ 6642 batches
[2025-05-14 03:47:41.862346]: text loss 0.9753 | 65800/ 6642 batches
[2025-05-14 03:49:24.877357]: text loss 0.9750 | 66000/ 6642 batches
[2025-05-14 03:51:07.853798]: text loss 0.9687 | 66200/ 6642 batches
[2025-05-14 03:52:50.790101]: text loss 0.9815 | 66400/ 6642 batches
[2025-05-14 03:53:01.086517]: text loss 0.9786 | 66420/ 6642 batches
[2025-05-14 03:53:01.087159]: validation
[2025-05-14 03:53:03.954554]: explanation loss 2.0862
[2025-05-14 03:53:09.140693]: rationale loss 1.2491
[2025-05-14 03:53:19.425605]: sequential loss 2.2243
[2025-05-14 03:54:26.708327]: top-N loss 1.2494
[2025-05-14 03:54:26.708409]: Endured 4 time(s)
[2025-05-14 03:54:26.708429]: epoch 11
[2025-05-14 03:55:57.755413]: text loss 0.9606 | 66600/ 6642 batches
[2025-05-14 03:57:40.397092]: text loss 0.9632 | 66800/ 6642 batches
[2025-05-14 03:59:23.400296]: text loss 0.9578 | 67000/ 6642 batches
[2025-05-14 04:01:06.549883]: text loss 0.9579 | 67200/ 6642 batches
[2025-05-14 04:02:49.785479]: text loss 0.9506 | 67400/ 6642 batches
[2025-05-14 04:04:32.940573]: text loss 0.9603 | 67600/ 6642 batches
[2025-05-14 04:06:16.027512]: text loss 0.9596 | 67800/ 6642 batches
[2025-05-14 04:07:59.080991]: text loss 0.9589 | 68000/ 6642 batches
[2025-05-14 04:09:42.101098]: text loss 0.9592 | 68200/ 6642 batches
[2025-05-14 04:11:25.155051]: text loss 0.9679 | 68400/ 6642 batches
[2025-05-14 04:13:08.216655]: text loss 0.9596 | 68600/ 6642 batches
[2025-05-14 04:14:51.213311]: text loss 0.9580 | 68800/ 6642 batches
[2025-05-14 04:16:34.218347]: text loss 0.9684 | 69000/ 6642 batches
[2025-05-14 04:18:17.228975]: text loss 0.9666 | 69200/ 6642 batches
[2025-05-14 04:20:00.215479]: text loss 0.9644 | 69400/ 6642 batches
[2025-05-14 04:21:43.242556]: text loss 0.9625 | 69600/ 6642 batches
[2025-05-14 04:23:26.373031]: text loss 0.9696 | 69800/ 6642 batches
[2025-05-14 04:25:09.593251]: text loss 0.9676 | 70000/ 6642 batches
[2025-05-14 04:26:52.782122]: text loss 0.9715 | 70200/ 6642 batches
[2025-05-14 04:28:35.893205]: text loss 0.9625 | 70400/ 6642 batches
[2025-05-14 04:30:18.923923]: text loss 0.9651 | 70600/ 6642 batches
[2025-05-14 04:32:02.145874]: text loss 0.9562 | 70800/ 6642 batches
[2025-05-14 04:33:43.678876]: text loss 0.9739 | 71000/ 6642 batches
[2025-05-14 04:35:24.935167]: text loss 0.9639 | 71200/ 6642 batches
[2025-05-14 04:37:06.206560]: text loss 0.9686 | 71400/ 6642 batches
[2025-05-14 04:38:47.445844]: text loss 0.9605 | 71600/ 6642 batches
[2025-05-14 04:40:28.749762]: text loss 0.9635 | 71800/ 6642 batches
[2025-05-14 04:42:10.967428]: text loss 0.9650 | 72000/ 6642 batches
[2025-05-14 04:43:53.954909]: text loss 0.9712 | 72200/ 6642 batches
[2025-05-14 04:45:36.984384]: text loss 0.9685 | 72400/ 6642 batches
[2025-05-14 04:47:20.113910]: text loss 0.9642 | 72600/ 6642 batches
[2025-05-14 04:49:03.380015]: text loss 0.9603 | 72800/ 6642 batches
[2025-05-14 04:50:46.558930]: text loss 0.9596 | 73000/ 6642 batches
[2025-05-14 04:51:18.525226]: text loss 0.9553 | 73062/ 6642 batches
[2025-05-14 04:51:18.525931]: validation
[2025-05-14 04:51:21.393996]: explanation loss 2.0767
[2025-05-14 04:51:26.591452]: rationale loss 1.2603
[2025-05-14 04:51:36.891544]: sequential loss 2.2005
[2025-05-14 04:52:44.268711]: top-N loss 1.2360
[2025-05-14 04:52:44.753688]: epoch 12
[2025-05-14 04:53:54.576747]: text loss 0.9522 | 73200/ 6642 batches
[2025-05-14 04:55:35.821988]: text loss 0.9429 | 73400/ 6642 batches
[2025-05-14 04:57:17.055094]: text loss 0.9479 | 73600/ 6642 batches
[2025-05-14 04:58:59.744874]: text loss 0.9476 | 73800/ 6642 batches
[2025-05-14 05:00:42.519661]: text loss 0.9508 | 74000/ 6642 batches
[2025-05-14 05:02:25.340630]: text loss 0.9489 | 74200/ 6642 batches
[2025-05-14 05:04:08.128885]: text loss 0.9469 | 74400/ 6642 batches
[2025-05-14 05:05:51.009535]: text loss 0.9443 | 74600/ 6642 batches
[2025-05-14 05:07:33.942822]: text loss 0.9541 | 74800/ 6642 batches
[2025-05-14 05:09:16.936802]: text loss 0.9572 | 75000/ 6642 batches
[2025-05-14 05:11:00.115213]: text loss 0.9451 | 75200/ 6642 batches
[2025-05-14 05:12:43.289313]: text loss 0.9555 | 75400/ 6642 batches
[2025-05-14 05:14:26.433335]: text loss 0.9496 | 75600/ 6642 batches
[2025-05-14 05:16:09.564077]: text loss 0.9530 | 75800/ 6642 batches
[2025-05-14 05:17:52.631155]: text loss 0.9551 | 76000/ 6642 batches
[2025-05-14 05:19:35.699775]: text loss 0.9503 | 76200/ 6642 batches
[2025-05-14 05:21:18.727121]: text loss 0.9443 | 76400/ 6642 batches
[2025-05-14 05:23:01.797832]: text loss 0.9502 | 76600/ 6642 batches
[2025-05-14 05:24:44.799513]: text loss 0.9520 | 76800/ 6642 batches
[2025-05-14 05:26:27.750693]: text loss 0.9562 | 77000/ 6642 batches
[2025-05-14 05:28:10.769237]: text loss 0.9525 | 77200/ 6642 batches
[2025-05-14 05:29:53.769747]: text loss 0.9539 | 77400/ 6642 batches
[2025-05-14 05:31:36.770472]: text loss 0.9537 | 77600/ 6642 batches
[2025-05-14 05:33:19.821674]: text loss 0.9456 | 77800/ 6642 batches
[2025-05-14 05:35:02.914241]: text loss 0.9545 | 78000/ 6642 batches
[2025-05-14 05:36:45.988831]: text loss 0.9521 | 78200/ 6642 batches
[2025-05-14 05:38:28.938838]: text loss 0.9514 | 78400/ 6642 batches
[2025-05-14 05:40:11.881005]: text loss 0.9677 | 78600/ 6642 batches
[2025-05-14 05:41:54.831289]: text loss 0.9542 | 78800/ 6642 batches
[2025-05-14 05:43:37.813719]: text loss 0.9525 | 79000/ 6642 batches
[2025-05-14 05:45:20.727833]: text loss 0.9506 | 79200/ 6642 batches
[2025-05-14 05:47:03.733199]: text loss 0.9543 | 79400/ 6642 batches
[2025-05-14 05:48:46.680531]: text loss 0.9636 | 79600/ 6642 batches
[2025-05-14 05:49:40.224361]: text loss 0.9491 | 79704/ 6642 batches
[2025-05-14 05:49:40.224954]: validation
[2025-05-14 05:49:43.087305]: explanation loss 2.6116
[2025-05-14 05:49:48.278445]: rationale loss 1.2373
[2025-05-14 05:49:58.574319]: sequential loss 2.1982
[2025-05-14 05:51:05.855388]: top-N loss 1.2186
[2025-05-14 05:51:05.855543]: Endured 5 time(s)
[2025-05-14 05:51:05.855564]: Cannot endure it anymore | Exiting from early stop
