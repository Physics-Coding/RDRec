nohup: ignoring input
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of Solomon were not initialized from the model checkpoint at t5-small and are newly initialized: ['conv1d.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/toys/
model_version                            0
task_num                                 5
prompt_num                               3
lr                                       0.0005
ratio                                    1:1:1:1
epochs                                   100
batch_size                               64
cuda                                     True
log_interval                             200
checkpoint                               ./checkpoint/toy_1111/
endure_times                             5
exp_len                                  20
negative_num                             99
----------------------------------------ARGUMENTS----------------------------------------
[2025-05-13 18:17:41.041423]: Loading data
[2025-05-13 18:17:51.747116]: Start training
[2025-05-13 18:17:51.747197]: epoch 1
[2025-05-13 18:19:34.293254]: text loss 1.6916 |   200/ 5122 batches
[2025-05-13 18:21:16.112311]: text loss 1.3549 |   400/ 5122 batches
[2025-05-13 18:22:57.616577]: text loss 1.3090 |   600/ 5122 batches
[2025-05-13 18:24:39.165661]: text loss 1.2964 |   800/ 5122 batches
[2025-05-13 18:26:19.928873]: text loss 1.2644 |  1000/ 5122 batches
[2025-05-13 18:28:01.236120]: text loss 1.2489 |  1200/ 5122 batches
[2025-05-13 18:29:42.025052]: text loss 1.2313 |  1400/ 5122 batches
[2025-05-13 18:31:23.374399]: text loss 1.2478 |  1600/ 5122 batches
[2025-05-13 18:33:04.425105]: text loss 1.2464 |  1800/ 5122 batches
[2025-05-13 18:34:45.618428]: text loss 1.2461 |  2000/ 5122 batches
[2025-05-13 18:36:28.445707]: text loss 1.2197 |  2200/ 5122 batches
[2025-05-13 18:38:11.104038]: text loss 1.2336 |  2400/ 5122 batches
[2025-05-13 18:39:52.075021]: text loss 1.2353 |  2600/ 5122 batches
[2025-05-13 18:41:33.133718]: text loss 1.2147 |  2800/ 5122 batches
[2025-05-13 18:43:14.188395]: text loss 1.2187 |  3000/ 5122 batches
[2025-05-13 18:44:56.895278]: text loss 1.2506 |  3200/ 5122 batches
[2025-05-13 18:46:37.956459]: text loss 1.2225 |  3400/ 5122 batches
[2025-05-13 18:48:18.954307]: text loss 1.2578 |  3600/ 5122 batches
[2025-05-13 18:50:00.202134]: text loss 1.1998 |  3800/ 5122 batches
[2025-05-13 18:51:41.741642]: text loss 1.2513 |  4000/ 5122 batches
[2025-05-13 18:53:23.127986]: text loss 1.2390 |  4200/ 5122 batches
[2025-05-13 18:55:04.434771]: text loss 1.2317 |  4400/ 5122 batches
[2025-05-13 18:56:45.106296]: text loss 1.2500 |  4600/ 5122 batches
[2025-05-13 18:58:26.092820]: text loss 1.2423 |  4800/ 5122 batches
[2025-05-13 19:00:07.888946]: text loss 1.2483 |  5000/ 5122 batches
[2025-05-13 19:01:09.605196]: text loss 1.2569 |  5122/ 5122 batches
[2025-05-13 19:01:09.606194]: validation
[2025-05-13 19:01:11.784189]: explanation loss 2.3840
[2025-05-13 19:01:15.664702]: rationale loss 1.3867
[2025-05-13 19:01:24.709469]: sequential loss 2.3513
[2025-05-13 19:02:23.169740]: top-N loss 1.5259
[2025-05-13 19:02:23.642949]: epoch 2
[2025-05-13 19:03:03.557176]: text loss 1.1735 |  5200/ 5122 batches
[2025-05-13 19:04:45.224295]: text loss 1.1775 |  5400/ 5122 batches
[2025-05-13 19:06:26.688842]: text loss 1.1800 |  5600/ 5122 batches
[2025-05-13 19:08:07.584253]: text loss 1.1686 |  5800/ 5122 batches
[2025-05-13 19:09:49.332059]: text loss 1.1704 |  6000/ 5122 batches
[2025-05-13 19:11:30.130329]: text loss 1.1681 |  6200/ 5122 batches
[2025-05-13 19:13:10.931887]: text loss 1.1690 |  6400/ 5122 batches
[2025-05-13 19:14:51.852319]: text loss 1.1728 |  6600/ 5122 batches
[2025-05-13 19:16:32.858860]: text loss 1.1616 |  6800/ 5122 batches
[2025-05-13 19:18:13.666920]: text loss 1.1617 |  7000/ 5122 batches
[2025-05-13 19:19:54.652398]: text loss 1.1574 |  7200/ 5122 batches
[2025-05-13 19:21:35.483661]: text loss 1.1643 |  7400/ 5122 batches
[2025-05-13 19:23:16.293450]: text loss 1.1517 |  7600/ 5122 batches
[2025-05-13 19:24:57.041083]: text loss 1.1577 |  7800/ 5122 batches
[2025-05-13 19:26:38.369051]: text loss 1.1619 |  8000/ 5122 batches
[2025-05-13 19:28:19.109135]: text loss 1.1601 |  8200/ 5122 batches
[2025-05-13 19:29:59.958167]: text loss 1.1417 |  8400/ 5122 batches
[2025-05-13 19:31:40.806813]: text loss 1.1411 |  8600/ 5122 batches
[2025-05-13 19:33:21.946812]: text loss 1.1479 |  8800/ 5122 batches
[2025-05-13 19:35:02.840662]: text loss 1.1530 |  9000/ 5122 batches
[2025-05-13 19:36:45.037544]: text loss 1.1474 |  9200/ 5122 batches
[2025-05-13 19:38:26.631401]: text loss 1.1440 |  9400/ 5122 batches
[2025-05-13 19:40:08.619794]: text loss 1.1431 |  9600/ 5122 batches
[2025-05-13 19:41:50.990329]: text loss 1.1432 |  9800/ 5122 batches
[2025-05-13 19:43:31.851125]: text loss 1.1417 | 10000/ 5122 batches
[2025-05-13 19:45:12.566356]: text loss 1.1500 | 10200/ 5122 batches
[2025-05-13 19:45:34.814418]: text loss 1.1296 | 10244/ 5122 batches
[2025-05-13 19:45:34.815147]: validation
[2025-05-13 19:45:36.971588]: explanation loss 2.1960
[2025-05-13 19:45:40.873265]: rationale loss 1.3229
[2025-05-13 19:45:50.006876]: sequential loss 2.3213
[2025-05-13 19:46:48.756519]: top-N loss 1.4809
[2025-05-13 19:46:49.230836]: epoch 3
[2025-05-13 19:48:08.369463]: text loss 1.1277 | 10400/ 5122 batches
[2025-05-13 19:49:49.756960]: text loss 1.1305 | 10600/ 5122 batches
[2025-05-13 19:51:30.657916]: text loss 1.1300 | 10800/ 5122 batches
[2025-05-13 19:53:11.861753]: text loss 1.1309 | 11000/ 5122 batches
[2025-05-13 19:54:53.096316]: text loss 1.1262 | 11200/ 5122 batches
[2025-05-13 19:56:34.048509]: text loss 1.1218 | 11400/ 5122 batches
[2025-05-13 19:58:15.149338]: text loss 1.1251 | 11600/ 5122 batches
[2025-05-13 19:59:55.761387]: text loss 1.1125 | 11800/ 5122 batches
[2025-05-13 20:01:36.580223]: text loss 1.1210 | 12000/ 5122 batches
[2025-05-13 20:03:17.229290]: text loss 1.1252 | 12200/ 5122 batches
[2025-05-13 20:04:57.926829]: text loss 1.1249 | 12400/ 5122 batches
[2025-05-13 20:06:38.707791]: text loss 1.1260 | 12600/ 5122 batches
[2025-05-13 20:08:19.647040]: text loss 1.1146 | 12800/ 5122 batches
[2025-05-13 20:10:00.495717]: text loss 1.1255 | 13000/ 5122 batches
[2025-05-13 20:11:41.219972]: text loss 1.1224 | 13200/ 5122 batches
[2025-05-13 20:13:22.463086]: text loss 1.1229 | 13400/ 5122 batches
[2025-05-13 20:15:03.407221]: text loss 1.1139 | 13600/ 5122 batches
[2025-05-13 20:16:44.449564]: text loss 1.1177 | 13800/ 5122 batches
[2025-05-13 20:18:25.533402]: text loss 1.1076 | 14000/ 5122 batches
[2025-05-13 20:20:06.414269]: text loss 1.1202 | 14200/ 5122 batches
[2025-05-13 20:21:48.308844]: text loss 1.1152 | 14400/ 5122 batches
[2025-05-13 20:23:29.690025]: text loss 1.1204 | 14600/ 5122 batches
[2025-05-13 20:25:10.493827]: text loss 1.1118 | 14800/ 5122 batches
[2025-05-13 20:26:51.102657]: text loss 1.1085 | 15000/ 5122 batches
[2025-05-13 20:28:32.549619]: text loss 1.1119 | 15200/ 5122 batches
[2025-05-13 20:29:56.384434]: text loss 1.1079 | 15366/ 5122 batches
[2025-05-13 20:29:56.385164]: validation
[2025-05-13 20:29:58.546930]: explanation loss 2.1719
[2025-05-13 20:30:02.442555]: rationale loss 1.2859
[2025-05-13 20:30:11.522476]: sequential loss 2.2858
[2025-05-13 20:31:10.129822]: top-N loss 1.4451
[2025-05-13 20:31:10.548937]: epoch 4
[2025-05-13 20:31:27.685148]: text loss 1.0827 | 15400/ 5122 batches
[2025-05-13 20:33:09.147983]: text loss 1.0993 | 15600/ 5122 batches
[2025-05-13 20:34:50.213809]: text loss 1.0925 | 15800/ 5122 batches
[2025-05-13 20:36:31.239588]: text loss 1.0905 | 16000/ 5122 batches
[2025-05-13 20:38:11.953207]: text loss 1.1046 | 16200/ 5122 batches
[2025-05-13 20:39:52.837968]: text loss 1.1001 | 16400/ 5122 batches
[2025-05-13 20:41:33.387845]: text loss 1.1026 | 16600/ 5122 batches
[2025-05-13 20:43:14.211413]: text loss 1.0931 | 16800/ 5122 batches
[2025-05-13 20:44:54.842181]: text loss 1.0905 | 17000/ 5122 batches
[2025-05-13 20:46:35.377163]: text loss 1.0987 | 17200/ 5122 batches
[2025-05-13 20:48:16.411190]: text loss 1.0949 | 17400/ 5122 batches
[2025-05-13 20:49:57.096519]: text loss 1.0924 | 17600/ 5122 batches
[2025-05-13 20:51:37.924519]: text loss 1.1039 | 17800/ 5122 batches
[2025-05-13 20:53:18.562541]: text loss 1.0932 | 18000/ 5122 batches
[2025-05-13 20:54:59.677427]: text loss 1.0885 | 18200/ 5122 batches
[2025-05-13 20:56:40.800203]: text loss 1.1005 | 18400/ 5122 batches
[2025-05-13 20:58:21.793951]: text loss 1.0923 | 18600/ 5122 batches
[2025-05-13 21:00:02.664839]: text loss 1.0938 | 18800/ 5122 batches
[2025-05-13 21:01:43.309230]: text loss 1.0989 | 19000/ 5122 batches
[2025-05-13 21:03:24.071114]: text loss 1.0892 | 19200/ 5122 batches
[2025-05-13 21:05:05.150066]: text loss 1.0903 | 19400/ 5122 batches
[2025-05-13 21:06:45.822487]: text loss 1.0930 | 19600/ 5122 batches
[2025-05-13 21:08:26.705223]: text loss 1.0861 | 19800/ 5122 batches
[2025-05-13 21:10:07.626901]: text loss 1.0878 | 20000/ 5122 batches
[2025-05-13 21:11:49.118164]: text loss 1.1014 | 20200/ 5122 batches
[2025-05-13 21:13:30.032770]: text loss 1.0953 | 20400/ 5122 batches
[2025-05-13 21:14:14.331103]: text loss 1.1135 | 20488/ 5122 batches
[2025-05-13 21:14:14.331758]: validation
[2025-05-13 21:14:16.503969]: explanation loss 2.1592
[2025-05-13 21:14:20.398247]: rationale loss 1.2682
[2025-05-13 21:14:29.477591]: sequential loss 2.2909
[2025-05-13 21:15:28.110614]: top-N loss 1.4173
[2025-05-13 21:15:28.505539]: epoch 5
[2025-05-13 21:16:24.798152]: text loss 1.0795 | 20600/ 5122 batches
[2025-05-13 21:18:05.820181]: text loss 1.0793 | 20800/ 5122 batches
[2025-05-13 21:19:46.599124]: text loss 1.0824 | 21000/ 5122 batches
[2025-05-13 21:21:27.984885]: text loss 1.0757 | 21200/ 5122 batches
[2025-05-13 21:23:10.196097]: text loss 1.0745 | 21400/ 5122 batches
[2025-05-13 21:24:50.812916]: text loss 1.0668 | 21600/ 5122 batches
[2025-05-13 21:26:31.804421]: text loss 1.0777 | 21800/ 5122 batches
[2025-05-13 21:28:12.745240]: text loss 1.0668 | 22000/ 5122 batches
[2025-05-13 21:29:54.146328]: text loss 1.0827 | 22200/ 5122 batches
[2025-05-13 21:31:35.534419]: text loss 1.0765 | 22400/ 5122 batches
[2025-05-13 21:33:17.369691]: text loss 1.0827 | 22600/ 5122 batches
[2025-05-13 21:34:58.614509]: text loss 1.0769 | 22800/ 5122 batches
[2025-05-13 21:36:39.853386]: text loss 1.0654 | 23000/ 5122 batches
[2025-05-13 21:38:21.205697]: text loss 1.0726 | 23200/ 5122 batches
[2025-05-13 21:40:02.048582]: text loss 1.0751 | 23400/ 5122 batches
[2025-05-13 21:41:42.969851]: text loss 1.0782 | 23600/ 5122 batches
[2025-05-13 21:43:23.973935]: text loss 1.0764 | 23800/ 5122 batches
[2025-05-13 21:45:05.786569]: text loss 1.0745 | 24000/ 5122 batches
[2025-05-13 21:46:47.242832]: text loss 1.0806 | 24200/ 5122 batches
[2025-05-13 21:48:28.054724]: text loss 1.0707 | 24400/ 5122 batches
[2025-05-13 21:50:09.547065]: text loss 1.0809 | 24600/ 5122 batches
[2025-05-13 21:51:50.831513]: text loss 1.0745 | 24800/ 5122 batches
[2025-05-13 21:53:31.746981]: text loss 1.0729 | 25000/ 5122 batches
[2025-05-13 21:55:12.815568]: text loss 1.0772 | 25200/ 5122 batches
[2025-05-13 21:56:53.479129]: text loss 1.0721 | 25400/ 5122 batches
[2025-05-13 21:58:34.069831]: text loss 1.0725 | 25600/ 5122 batches
[2025-05-13 21:58:39.082782]: text loss 1.0901 | 25610/ 5122 batches
[2025-05-13 21:58:39.083503]: validation
[2025-05-13 21:58:41.239349]: explanation loss 2.1758
[2025-05-13 21:58:45.126914]: rationale loss 1.2562
[2025-05-13 21:58:54.205703]: sequential loss 2.2838
[2025-05-13 21:59:53.002532]: top-N loss 1.3795
[2025-05-13 21:59:53.404940]: epoch 6
[2025-05-13 22:01:29.125220]: text loss 1.0480 | 25800/ 5122 batches
[2025-05-13 22:03:10.001916]: text loss 1.0557 | 26000/ 5122 batches
[2025-05-13 22:04:50.609295]: text loss 1.0584 | 26200/ 5122 batches
[2025-05-13 22:06:31.726867]: text loss 1.0627 | 26400/ 5122 batches
[2025-05-13 22:08:12.679607]: text loss 1.0614 | 26600/ 5122 batches
[2025-05-13 22:09:53.998173]: text loss 1.0642 | 26800/ 5122 batches
[2025-05-13 22:11:35.546195]: text loss 1.0613 | 27000/ 5122 batches
[2025-05-13 22:13:16.553649]: text loss 1.0617 | 27200/ 5122 batches
[2025-05-13 22:14:57.634651]: text loss 1.0673 | 27400/ 5122 batches
[2025-05-13 22:16:38.448829]: text loss 1.0524 | 27600/ 5122 batches
[2025-05-13 22:18:19.144713]: text loss 1.0620 | 27800/ 5122 batches
[2025-05-13 22:20:00.056850]: text loss 1.0602 | 28000/ 5122 batches
[2025-05-13 22:21:41.730246]: text loss 1.0628 | 28200/ 5122 batches
[2025-05-13 22:23:22.970323]: text loss 1.0689 | 28400/ 5122 batches
[2025-05-13 22:25:07.473076]: text loss 1.0508 | 28600/ 5122 batches
[2025-05-13 22:26:50.395779]: text loss 1.0637 | 28800/ 5122 batches
[2025-05-13 22:28:31.390981]: text loss 1.0491 | 29000/ 5122 batches
[2025-05-13 22:30:12.233786]: text loss 1.0551 | 29200/ 5122 batches
[2025-05-13 22:31:52.980501]: text loss 1.0632 | 29400/ 5122 batches
[2025-05-13 22:33:34.200914]: text loss 1.0592 | 29600/ 5122 batches
[2025-05-13 22:35:15.316297]: text loss 1.0683 | 29800/ 5122 batches
[2025-05-13 22:36:56.070631]: text loss 1.0594 | 30000/ 5122 batches
[2025-05-13 22:38:36.795917]: text loss 1.0535 | 30200/ 5122 batches
[2025-05-13 22:40:17.992479]: text loss 1.0619 | 30400/ 5122 batches
[2025-05-13 22:41:58.906662]: text loss 1.0563 | 30600/ 5122 batches
[2025-05-13 22:43:05.488097]: text loss 1.0564 | 30732/ 5122 batches
[2025-05-13 22:43:05.488838]: validation
[2025-05-13 22:43:07.658563]: explanation loss 2.1298
[2025-05-13 22:43:11.562514]: rationale loss 1.2512
[2025-05-13 22:43:20.792398]: sequential loss 2.2555
[2025-05-13 22:44:19.257801]: top-N loss 1.3636
[2025-05-13 22:44:19.672400]: epoch 7
[2025-05-13 22:44:54.134916]: text loss 1.0525 | 30800/ 5122 batches
[2025-05-13 22:46:35.280317]: text loss 1.0459 | 31000/ 5122 batches
[2025-05-13 22:48:16.606456]: text loss 1.0440 | 31200/ 5122 batches
[2025-05-13 22:49:57.418959]: text loss 1.0416 | 31400/ 5122 batches
[2025-05-13 22:51:39.833873]: text loss 1.0390 | 31600/ 5122 batches
[2025-05-13 22:53:21.458245]: text loss 1.0470 | 31800/ 5122 batches
[2025-05-13 22:55:02.376243]: text loss 1.0453 | 32000/ 5122 batches
[2025-05-13 22:56:43.437188]: text loss 1.0507 | 32200/ 5122 batches
[2025-05-13 22:58:24.529574]: text loss 1.0507 | 32400/ 5122 batches
[2025-05-13 23:00:05.494288]: text loss 1.0463 | 32600/ 5122 batches
[2025-05-13 23:01:46.691676]: text loss 1.0442 | 32800/ 5122 batches
[2025-05-13 23:03:27.751748]: text loss 1.0449 | 33000/ 5122 batches
[2025-05-13 23:05:08.427749]: text loss 1.0392 | 33200/ 5122 batches
[2025-05-13 23:06:49.613187]: text loss 1.0445 | 33400/ 5122 batches
[2025-05-13 23:08:31.879778]: text loss 1.0402 | 33600/ 5122 batches
[2025-05-13 23:10:13.922722]: text loss 1.0554 | 33800/ 5122 batches
[2025-05-13 23:11:55.858851]: text loss 1.0466 | 34000/ 5122 batches
[2025-05-13 23:13:36.581739]: text loss 1.0471 | 34200/ 5122 batches
[2025-05-13 23:15:17.691042]: text loss 1.0475 | 34400/ 5122 batches
[2025-05-13 23:16:59.701759]: text loss 1.0425 | 34600/ 5122 batches
[2025-05-13 23:18:40.544496]: text loss 1.0486 | 34800/ 5122 batches
[2025-05-13 23:20:21.335436]: text loss 1.0483 | 35000/ 5122 batches
[2025-05-13 23:22:02.817493]: text loss 1.0453 | 35200/ 5122 batches
[2025-05-13 23:23:43.925402]: text loss 1.0426 | 35400/ 5122 batches
[2025-05-13 23:25:24.743503]: text loss 1.0454 | 35600/ 5122 batches
[2025-05-13 23:27:05.644178]: text loss 1.0409 | 35800/ 5122 batches
[2025-05-13 23:27:32.969209]: text loss 1.0527 | 35854/ 5122 batches
[2025-05-13 23:27:32.969966]: validation
[2025-05-13 23:27:35.147979]: explanation loss 2.1255
[2025-05-13 23:27:39.025802]: rationale loss 1.2590
[2025-05-13 23:27:48.101733]: sequential loss 2.2521
[2025-05-13 23:28:46.779602]: top-N loss 1.3404
[2025-05-13 23:28:47.185227]: epoch 8
[2025-05-13 23:30:00.705204]: text loss 1.0225 | 36000/ 5122 batches
[2025-05-13 23:31:41.155350]: text loss 1.0247 | 36200/ 5122 batches
[2025-05-13 23:33:21.943558]: text loss 1.0309 | 36400/ 5122 batches
[2025-05-13 23:35:02.809176]: text loss 1.0309 | 36600/ 5122 batches
[2025-05-13 23:36:44.088327]: text loss 1.0340 | 36800/ 5122 batches
[2025-05-13 23:38:24.934741]: text loss 1.0446 | 37000/ 5122 batches
[2025-05-13 23:40:06.687189]: text loss 1.0272 | 37200/ 5122 batches
[2025-05-13 23:41:47.656533]: text loss 1.0331 | 37400/ 5122 batches
[2025-05-13 23:43:28.490606]: text loss 1.0275 | 37600/ 5122 batches
[2025-05-13 23:45:09.018729]: text loss 1.0367 | 37800/ 5122 batches
[2025-05-13 23:46:49.788298]: text loss 1.0393 | 38000/ 5122 batches
[2025-05-13 23:48:30.871727]: text loss 1.0308 | 38200/ 5122 batches
[2025-05-13 23:50:11.926381]: text loss 1.0315 | 38400/ 5122 batches
[2025-05-13 23:51:53.029997]: text loss 1.0332 | 38600/ 5122 batches
[2025-05-13 23:53:33.797060]: text loss 1.0393 | 38800/ 5122 batches
[2025-05-13 23:55:14.750924]: text loss 1.0366 | 39000/ 5122 batches
[2025-05-13 23:56:55.534054]: text loss 1.0361 | 39200/ 5122 batches
[2025-05-13 23:58:36.509543]: text loss 1.0289 | 39400/ 5122 batches
[2025-05-14 00:00:18.805421]: text loss 1.0334 | 39600/ 5122 batches
[2025-05-14 00:01:59.456982]: text loss 1.0407 | 39800/ 5122 batches
[2025-05-14 00:03:40.107885]: text loss 1.0355 | 40000/ 5122 batches
[2025-05-14 00:05:21.817284]: text loss 1.0306 | 40200/ 5122 batches
[2025-05-14 00:07:02.877785]: text loss 1.0338 | 40400/ 5122 batches
[2025-05-14 00:08:43.533177]: text loss 1.0379 | 40600/ 5122 batches
[2025-05-14 00:10:24.423889]: text loss 1.0274 | 40800/ 5122 batches
[2025-05-14 00:11:53.330061]: text loss 1.0304 | 40976/ 5122 batches
[2025-05-14 00:11:53.330779]: validation
[2025-05-14 00:11:55.489994]: explanation loss 2.1685
[2025-05-14 00:11:59.376547]: rationale loss 1.2457
[2025-05-14 00:12:08.452230]: sequential loss 2.2651
[2025-05-14 00:13:07.076012]: top-N loss 1.3373
[2025-05-14 00:13:07.076105]: Endured 1 time(s)
[2025-05-14 00:13:07.076130]: epoch 9
[2025-05-14 00:13:19.160261]: text loss 1.0244 | 41000/ 5122 batches
[2025-05-14 00:15:00.330865]: text loss 1.0217 | 41200/ 5122 batches
[2025-05-14 00:16:42.100467]: text loss 1.0141 | 41400/ 5122 batches
[2025-05-14 00:18:23.101197]: text loss 1.0156 | 41600/ 5122 batches
[2025-05-14 00:20:03.907343]: text loss 1.0185 | 41800/ 5122 batches
[2025-05-14 00:21:45.006406]: text loss 1.0090 | 42000/ 5122 batches
[2025-05-14 00:23:25.475079]: text loss 1.0143 | 42200/ 5122 batches
[2025-05-14 00:25:06.369496]: text loss 1.0166 | 42400/ 5122 batches
[2025-05-14 00:26:47.206543]: text loss 1.0249 | 42600/ 5122 batches
[2025-05-14 00:28:28.383594]: text loss 1.0247 | 42800/ 5122 batches
[2025-05-14 00:30:09.117446]: text loss 1.0227 | 43000/ 5122 batches
[2025-05-14 00:31:49.976092]: text loss 1.0214 | 43200/ 5122 batches
[2025-05-14 00:33:31.051773]: text loss 1.0207 | 43400/ 5122 batches
[2025-05-14 00:35:11.850632]: text loss 1.0204 | 43600/ 5122 batches
[2025-05-14 00:36:53.353258]: text loss 1.0163 | 43800/ 5122 batches
[2025-05-14 00:38:34.729250]: text loss 1.0318 | 44000/ 5122 batches
[2025-05-14 00:40:15.704384]: text loss 1.0197 | 44200/ 5122 batches
[2025-05-14 00:41:56.313005]: text loss 1.0227 | 44400/ 5122 batches
[2025-05-14 00:43:37.216792]: text loss 1.0273 | 44600/ 5122 batches
[2025-05-14 00:45:18.302267]: text loss 1.0197 | 44800/ 5122 batches
[2025-05-14 00:46:58.881807]: text loss 1.0301 | 45000/ 5122 batches
[2025-05-14 00:48:40.208975]: text loss 1.0276 | 45200/ 5122 batches
[2025-05-14 00:50:21.270082]: text loss 1.0193 | 45400/ 5122 batches
[2025-05-14 00:52:02.296275]: text loss 1.0208 | 45600/ 5122 batches
[2025-05-14 00:53:43.654482]: text loss 1.0362 | 45800/ 5122 batches
[2025-05-14 00:55:25.363095]: text loss 1.0237 | 46000/ 5122 batches
[2025-05-14 00:56:14.736074]: text loss 1.0241 | 46098/ 5122 batches
[2025-05-14 00:56:14.737205]: validation
[2025-05-14 00:56:16.968481]: explanation loss 2.1235
[2025-05-14 00:56:20.861360]: rationale loss 1.2454
[2025-05-14 00:56:29.930313]: sequential loss 2.2521
[2025-05-14 00:57:28.930161]: top-N loss 1.3255
[2025-05-14 00:57:29.429682]: epoch 10
[2025-05-14 00:58:21.322154]: text loss 1.0068 | 46200/ 5122 batches
[2025-05-14 01:00:02.059601]: text loss 1.0041 | 46400/ 5122 batches
[2025-05-14 01:01:42.834632]: text loss 1.0004 | 46600/ 5122 batches
[2025-05-14 01:03:24.110476]: text loss 1.0035 | 46800/ 5122 batches
[2025-05-14 01:05:05.576984]: text loss 1.0090 | 47000/ 5122 batches
[2025-05-14 01:06:45.944296]: text loss 1.0097 | 47200/ 5122 batches
[2025-05-14 01:08:26.484937]: text loss 1.0056 | 47400/ 5122 batches
[2025-05-14 01:10:07.361683]: text loss 1.0104 | 47600/ 5122 batches
[2025-05-14 01:11:48.365937]: text loss 1.0096 | 47800/ 5122 batches
[2025-05-14 01:13:30.054902]: text loss 1.0202 | 48000/ 5122 batches
[2025-05-14 01:15:10.961132]: text loss 1.0104 | 48200/ 5122 batches
[2025-05-14 01:16:52.772339]: text loss 1.0162 | 48400/ 5122 batches
[2025-05-14 01:18:34.690576]: text loss 1.0051 | 48600/ 5122 batches
[2025-05-14 01:20:16.532877]: text loss 1.0123 | 48800/ 5122 batches
[2025-05-14 01:21:57.355629]: text loss 1.0112 | 49000/ 5122 batches
[2025-05-14 01:23:39.411750]: text loss 1.0057 | 49200/ 5122 batches
[2025-05-14 01:25:21.231536]: text loss 1.0172 | 49400/ 5122 batches
[2025-05-14 01:27:02.543155]: text loss 1.0158 | 49600/ 5122 batches
[2025-05-14 01:28:44.300599]: text loss 1.0090 | 49800/ 5122 batches
[2025-05-14 01:30:24.515892]: text loss 1.0228 | 50000/ 5122 batches
[2025-05-14 01:32:05.885024]: text loss 1.0181 | 50200/ 5122 batches
[2025-05-14 01:33:47.899921]: text loss 1.0126 | 50400/ 5122 batches
[2025-05-14 01:35:29.932005]: text loss 1.0058 | 50600/ 5122 batches
[2025-05-14 01:37:11.983567]: text loss 1.0135 | 50800/ 5122 batches
[2025-05-14 01:38:54.198001]: text loss 1.0140 | 51000/ 5122 batches
[2025-05-14 01:40:36.269028]: text loss 1.0168 | 51200/ 5122 batches
[2025-05-14 01:40:46.477105]: text loss 1.0135 | 51220/ 5122 batches
[2025-05-14 01:40:46.477863]: validation
[2025-05-14 01:40:48.643600]: explanation loss 2.1296
[2025-05-14 01:40:52.529059]: rationale loss 1.2331
[2025-05-14 01:41:01.561706]: sequential loss 2.2605
[2025-05-14 01:41:59.702913]: top-N loss 1.3109
[2025-05-14 01:42:00.143503]: epoch 11
[2025-05-14 01:43:31.261425]: text loss 0.9995 | 51400/ 5122 batches
[2025-05-14 01:45:13.794417]: text loss 1.0009 | 51600/ 5122 batches
[2025-05-14 01:46:55.899234]: text loss 0.9829 | 51800/ 5122 batches
[2025-05-14 01:48:37.915326]: text loss 0.9908 | 52000/ 5122 batches
[2025-05-14 01:50:19.947334]: text loss 0.9851 | 52200/ 5122 batches
[2025-05-14 01:52:01.897697]: text loss 0.9996 | 52400/ 5122 batches
[2025-05-14 01:53:43.792900]: text loss 0.9975 | 52600/ 5122 batches
[2025-05-14 01:55:25.658283]: text loss 0.9996 | 52800/ 5122 batches
[2025-05-14 01:57:07.525416]: text loss 0.9986 | 53000/ 5122 batches
[2025-05-14 01:58:49.412070]: text loss 0.9986 | 53200/ 5122 batches
[2025-05-14 02:00:31.290433]: text loss 1.0006 | 53400/ 5122 batches
[2025-05-14 02:02:13.559675]: text loss 1.0006 | 53600/ 5122 batches
[2025-05-14 02:03:54.309701]: text loss 1.0073 | 53800/ 5122 batches
[2025-05-14 02:05:35.119862]: text loss 1.0022 | 54000/ 5122 batches
[2025-05-14 02:07:17.009640]: text loss 1.0065 | 54200/ 5122 batches
[2025-05-14 02:08:58.900807]: text loss 1.0046 | 54400/ 5122 batches
[2025-05-14 02:10:40.783317]: text loss 1.0139 | 54600/ 5122 batches
[2025-05-14 02:12:22.692632]: text loss 1.0020 | 54800/ 5122 batches
[2025-05-14 02:14:04.521356]: text loss 1.0029 | 55000/ 5122 batches
[2025-05-14 02:15:46.394166]: text loss 1.0035 | 55200/ 5122 batches
[2025-05-14 02:17:28.267853]: text loss 0.9977 | 55400/ 5122 batches
[2025-05-14 02:19:10.171911]: text loss 1.0029 | 55600/ 5122 batches
[2025-05-14 02:20:52.046638]: text loss 0.9987 | 55800/ 5122 batches
[2025-05-14 02:22:33.813490]: text loss 1.0050 | 56000/ 5122 batches
[2025-05-14 02:24:15.657584]: text loss 1.0089 | 56200/ 5122 batches
[2025-05-14 02:25:27.911343]: text loss 1.0031 | 56342/ 5122 batches
[2025-05-14 02:25:27.911891]: validation
[2025-05-14 02:25:30.056362]: explanation loss 2.1394
[2025-05-14 02:25:33.925993]: rationale loss 1.2406
[2025-05-14 02:25:42.910125]: sequential loss 2.2936
[2025-05-14 02:26:40.958627]: top-N loss 1.2988
[2025-05-14 02:26:40.958713]: Endured 2 time(s)
[2025-05-14 02:26:40.958739]: epoch 12
[2025-05-14 02:27:09.999806]: text loss 0.9769 | 56400/ 5122 batches
[2025-05-14 02:28:50.145490]: text loss 0.9813 | 56600/ 5122 batches
[2025-05-14 02:30:30.252979]: text loss 0.9843 | 56800/ 5122 batches
[2025-05-14 02:32:10.359131]: text loss 0.9892 | 57000/ 5122 batches
[2025-05-14 02:33:50.781447]: text loss 0.9859 | 57200/ 5122 batches
[2025-05-14 02:35:31.046038]: text loss 0.9871 | 57400/ 5122 batches
[2025-05-14 02:37:11.244922]: text loss 0.9821 | 57600/ 5122 batches
[2025-05-14 02:38:51.762787]: text loss 0.9909 | 57800/ 5122 batches
[2025-05-14 02:40:33.417171]: text loss 0.9906 | 58000/ 5122 batches
[2025-05-14 02:42:15.140899]: text loss 0.9904 | 58200/ 5122 batches
[2025-05-14 02:43:57.168545]: text loss 0.9919 | 58400/ 5122 batches
[2025-05-14 02:45:39.104117]: text loss 0.9887 | 58600/ 5122 batches
[2025-05-14 02:47:20.295026]: text loss 0.9920 | 58800/ 5122 batches
[2025-05-14 02:49:00.371327]: text loss 0.9960 | 59000/ 5122 batches
[2025-05-14 02:50:40.424738]: text loss 0.9897 | 59200/ 5122 batches
[2025-05-14 02:52:21.463738]: text loss 0.9878 | 59400/ 5122 batches
[2025-05-14 02:54:03.141796]: text loss 0.9978 | 59600/ 5122 batches
[2025-05-14 02:55:44.905510]: text loss 0.9921 | 59800/ 5122 batches
[2025-05-14 02:57:26.689902]: text loss 0.9930 | 60000/ 5122 batches
[2025-05-14 02:59:08.454227]: text loss 0.9850 | 60200/ 5122 batches
[2025-05-14 03:00:50.301565]: text loss 0.9959 | 60400/ 5122 batches
[2025-05-14 03:02:32.178577]: text loss 1.0004 | 60600/ 5122 batches
[2025-05-14 03:04:14.087877]: text loss 0.9959 | 60800/ 5122 batches
[2025-05-14 03:05:55.642423]: text loss 0.9912 | 61000/ 5122 batches
[2025-05-14 03:07:37.527421]: text loss 0.9952 | 61200/ 5122 batches
[2025-05-14 03:09:19.517143]: text loss 0.9878 | 61400/ 5122 batches
[2025-05-14 03:09:52.139965]: text loss 0.9925 | 61464/ 5122 batches
[2025-05-14 03:09:52.140599]: validation
[2025-05-14 03:09:54.291456]: explanation loss 2.1201
[2025-05-14 03:09:58.172710]: rationale loss 1.2414
[2025-05-14 03:10:07.167949]: sequential loss 2.2835
[2025-05-14 03:11:05.366926]: top-N loss 1.2960
[2025-05-14 03:11:05.367010]: Endured 3 time(s)
[2025-05-14 03:11:05.367034]: epoch 13
[2025-05-14 03:12:13.502047]: text loss 0.9727 | 61600/ 5122 batches
[2025-05-14 03:13:53.737860]: text loss 0.9755 | 61800/ 5122 batches
[2025-05-14 03:15:37.585270]: text loss 0.9740 | 62000/ 5122 batches
[2025-05-14 03:17:20.548641]: text loss 0.9735 | 62200/ 5122 batches
[2025-05-14 03:19:02.509755]: text loss 0.9746 | 62400/ 5122 batches
[2025-05-14 03:20:44.703819]: text loss 0.9802 | 62600/ 5122 batches
[2025-05-14 03:22:26.912471]: text loss 0.9821 | 62800/ 5122 batches
[2025-05-14 03:24:08.997955]: text loss 0.9756 | 63000/ 5122 batches
[2025-05-14 03:25:51.043609]: text loss 0.9756 | 63200/ 5122 batches
[2025-05-14 03:27:33.230990]: text loss 0.9749 | 63400/ 5122 batches
[2025-05-14 03:29:14.825628]: text loss 0.9828 | 63600/ 5122 batches
[2025-05-14 03:30:55.433600]: text loss 0.9784 | 63800/ 5122 batches
[2025-05-14 03:32:35.587991]: text loss 0.9751 | 64000/ 5122 batches
[2025-05-14 03:34:15.779808]: text loss 0.9822 | 64200/ 5122 batches
[2025-05-14 03:35:56.194235]: text loss 0.9812 | 64400/ 5122 batches
[2025-05-14 03:37:38.325860]: text loss 0.9780 | 64600/ 5122 batches
[2025-05-14 03:39:20.422893]: text loss 0.9783 | 64800/ 5122 batches
[2025-05-14 03:41:01.720197]: text loss 0.9827 | 65000/ 5122 batches
[2025-05-14 03:42:43.632781]: text loss 0.9882 | 65200/ 5122 batches
[2025-05-14 03:44:25.553796]: text loss 0.9820 | 65400/ 5122 batches
[2025-05-14 03:46:07.421616]: text loss 0.9771 | 65600/ 5122 batches
[2025-05-14 03:47:49.221853]: text loss 0.9848 | 65800/ 5122 batches
[2025-05-14 03:49:31.065236]: text loss 0.9919 | 66000/ 5122 batches
[2025-05-14 03:51:12.964020]: text loss 0.9814 | 66200/ 5122 batches
[2025-05-14 03:52:54.873182]: text loss 0.9879 | 66400/ 5122 batches
[2025-05-14 03:54:29.639401]: text loss 0.9804 | 66586/ 5122 batches
[2025-05-14 03:54:29.640065]: validation
[2025-05-14 03:54:31.788516]: explanation loss 2.1259
[2025-05-14 03:54:35.660000]: rationale loss 1.2398
[2025-05-14 03:54:44.662039]: sequential loss 2.2905
[2025-05-14 03:55:42.676421]: top-N loss 1.2927
[2025-05-14 03:55:42.676509]: Endured 4 time(s)
[2025-05-14 03:55:42.676534]: epoch 14
[2025-05-14 03:55:49.731194]: text loss 0.9926 | 66600/ 5122 batches
[2025-05-14 03:57:29.881808]: text loss 0.9664 | 66800/ 5122 batches
[2025-05-14 03:59:11.458530]: text loss 0.9610 | 67000/ 5122 batches
[2025-05-14 04:00:53.541151]: text loss 0.9671 | 67200/ 5122 batches
[2025-05-14 04:02:35.531452]: text loss 0.9617 | 67400/ 5122 batches
[2025-05-14 04:04:17.515218]: text loss 0.9658 | 67600/ 5122 batches
[2025-05-14 04:05:59.505360]: text loss 0.9673 | 67800/ 5122 batches
[2025-05-14 04:07:41.355634]: text loss 0.9661 | 68000/ 5122 batches
[2025-05-14 04:09:23.264663]: text loss 0.9643 | 68200/ 5122 batches
[2025-05-14 04:11:05.256778]: text loss 0.9640 | 68400/ 5122 batches
[2025-05-14 04:12:47.212435]: text loss 0.9709 | 68600/ 5122 batches
[2025-05-14 04:14:29.100397]: text loss 0.9797 | 68800/ 5122 batches
[2025-05-14 04:16:10.976061]: text loss 0.9635 | 69000/ 5122 batches
[2025-05-14 04:17:52.791072]: text loss 0.9784 | 69200/ 5122 batches
[2025-05-14 04:19:34.599431]: text loss 0.9745 | 69400/ 5122 batches
[2025-05-14 04:21:16.414199]: text loss 0.9642 | 69600/ 5122 batches
[2025-05-14 04:22:58.325820]: text loss 0.9622 | 69800/ 5122 batches
[2025-05-14 04:24:40.227246]: text loss 0.9732 | 70000/ 5122 batches
[2025-05-14 04:26:22.113979]: text loss 0.9681 | 70200/ 5122 batches
[2025-05-14 04:28:04.026808]: text loss 0.9712 | 70400/ 5122 batches
[2025-05-14 04:29:45.897565]: text loss 0.9715 | 70600/ 5122 batches
[2025-05-14 04:31:27.710910]: text loss 0.9762 | 70800/ 5122 batches
[2025-05-14 04:33:08.982074]: text loss 0.9750 | 71000/ 5122 batches
[2025-05-14 04:34:49.047167]: text loss 0.9775 | 71200/ 5122 batches
[2025-05-14 04:36:29.057169]: text loss 0.9698 | 71400/ 5122 batches
[2025-05-14 04:38:09.084926]: text loss 0.9782 | 71600/ 5122 batches
[2025-05-14 04:39:03.091789]: text loss 0.9755 | 71708/ 5122 batches
[2025-05-14 04:39:03.092355]: validation
[2025-05-14 04:39:05.222712]: explanation loss 2.1222
[2025-05-14 04:39:09.094894]: rationale loss 1.2411
[2025-05-14 04:39:18.096895]: sequential loss 2.2830
[2025-05-14 04:40:16.176593]: top-N loss 1.2798
[2025-05-14 04:40:16.611105]: epoch 15
[2025-05-14 04:41:03.011450]: text loss 0.9578 | 71800/ 5122 batches
[2025-05-14 04:42:44.939768]: text loss 0.9444 | 72000/ 5122 batches
[2025-05-14 04:44:26.844382]: text loss 0.9555 | 72200/ 5122 batches
[2025-05-14 04:46:08.805145]: text loss 0.9539 | 72400/ 5122 batches
[2025-05-14 04:47:50.786696]: text loss 0.9569 | 72600/ 5122 batches
[2025-05-14 04:49:32.724940]: text loss 0.9574 | 72800/ 5122 batches
[2025-05-14 04:51:14.589049]: text loss 0.9557 | 73000/ 5122 batches
[2025-05-14 04:52:56.398403]: text loss 0.9593 | 73200/ 5122 batches
[2025-05-14 04:54:38.163878]: text loss 0.9558 | 73400/ 5122 batches
[2025-05-14 04:56:19.898516]: text loss 0.9533 | 73600/ 5122 batches
[2025-05-14 04:58:01.617857]: text loss 0.9642 | 73800/ 5122 batches
[2025-05-14 04:59:43.442220]: text loss 0.9624 | 74000/ 5122 batches
[2025-05-14 05:01:25.297970]: text loss 0.9573 | 74200/ 5122 batches
[2025-05-14 05:03:05.653637]: text loss 0.9660 | 74400/ 5122 batches
[2025-05-14 05:04:45.773077]: text loss 0.9602 | 74600/ 5122 batches
[2025-05-14 05:06:25.957654]: text loss 0.9584 | 74800/ 5122 batches
[2025-05-14 05:08:06.300115]: text loss 0.9675 | 75000/ 5122 batches
[2025-05-14 05:09:46.288836]: text loss 0.9580 | 75200/ 5122 batches
[2025-05-14 05:11:26.943653]: text loss 0.9616 | 75400/ 5122 batches
[2025-05-14 05:13:07.197662]: text loss 0.9663 | 75600/ 5122 batches
[2025-05-14 05:14:47.498099]: text loss 0.9583 | 75800/ 5122 batches
[2025-05-14 05:16:27.741121]: text loss 0.9614 | 76000/ 5122 batches
[2025-05-14 05:18:07.823262]: text loss 0.9616 | 76200/ 5122 batches
[2025-05-14 05:19:47.938004]: text loss 0.9592 | 76400/ 5122 batches
[2025-05-14 05:21:28.140408]: text loss 0.9597 | 76600/ 5122 batches
[2025-05-14 05:23:08.643406]: text loss 0.9715 | 76800/ 5122 batches
[2025-05-14 05:23:23.669017]: text loss 0.9503 | 76830/ 5122 batches
[2025-05-14 05:23:23.669630]: validation
[2025-05-14 05:23:25.803764]: explanation loss 2.1212
[2025-05-14 05:23:29.688615]: rationale loss 1.2410
[2025-05-14 05:23:38.686690]: sequential loss 2.3266
[2025-05-14 05:24:36.730581]: top-N loss 1.2747
[2025-05-14 05:24:36.730665]: Endured 5 time(s)
[2025-05-14 05:24:36.730689]: Cannot endure it anymore | Exiting from early stop
