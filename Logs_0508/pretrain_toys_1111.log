nohup: ignoring input
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/toys/
model_version                            0
task_num                                 5
prompt_num                               3
lr                                       0.0005
ratio                                    1:1:1:1
epochs                                   100
batch_size                               64
cuda                                     True
log_interval                             200
checkpoint                               ./checkpoint/toys_1111/
endure_times                             5
exp_len                                  20
negative_num                             99
----------------------------------------ARGUMENTS----------------------------------------
[2025-05-08 02:16:13.161182]: Loading data
[2025-05-08 02:16:33.052266]: Start training
[2025-05-08 02:16:33.052335]: epoch 1
[2025-05-08 02:18:14.913247]: text loss 1.6904 |   200/ 5122 batches
[2025-05-08 02:19:57.798257]: text loss 1.3569 |   400/ 5122 batches
[2025-05-08 02:21:42.302557]: text loss 1.3086 |   600/ 5122 batches
[2025-05-08 02:23:27.140964]: text loss 1.2947 |   800/ 5122 batches
[2025-05-08 02:25:10.982483]: text loss 1.2632 |  1000/ 5122 batches
[2025-05-08 02:26:54.048601]: text loss 1.2491 |  1200/ 5122 batches
[2025-05-08 02:28:36.317418]: text loss 1.2307 |  1400/ 5122 batches
[2025-05-08 02:30:19.399515]: text loss 1.2464 |  1600/ 5122 batches
[2025-05-08 02:32:02.286769]: text loss 1.2468 |  1800/ 5122 batches
[2025-05-08 02:33:45.396926]: text loss 1.2444 |  2000/ 5122 batches
[2025-05-08 02:35:28.598828]: text loss 1.2198 |  2200/ 5122 batches
[2025-05-08 02:37:11.821636]: text loss 1.2336 |  2400/ 5122 batches
[2025-05-08 02:38:55.266178]: text loss 1.2347 |  2600/ 5122 batches
[2025-05-08 02:40:38.581677]: text loss 1.2148 |  2800/ 5122 batches
[2025-05-08 02:42:22.052375]: text loss 1.2184 |  3000/ 5122 batches
[2025-05-08 02:44:05.598872]: text loss 1.2508 |  3200/ 5122 batches
[2025-05-08 02:45:49.140316]: text loss 1.2235 |  3400/ 5122 batches
[2025-05-08 02:47:32.718399]: text loss 1.2586 |  3600/ 5122 batches
[2025-05-08 02:49:16.205591]: text loss 1.1998 |  3800/ 5122 batches
[2025-05-08 02:50:59.750865]: text loss 1.2512 |  4000/ 5122 batches
[2025-05-08 02:52:43.203269]: text loss 1.2396 |  4200/ 5122 batches
[2025-05-08 02:54:26.420051]: text loss 1.2304 |  4400/ 5122 batches
[2025-05-08 02:56:09.861174]: text loss 1.2510 |  4600/ 5122 batches
[2025-05-08 02:57:53.311264]: text loss 1.2409 |  4800/ 5122 batches
[2025-05-08 02:59:36.631549]: text loss 1.2477 |  5000/ 5122 batches
[2025-05-08 03:00:39.058971]: text loss 1.2573 |  5122/ 5122 batches
[2025-05-08 03:00:39.059635]: validation
[2025-05-08 03:00:41.241651]: explanation loss 2.3725
[2025-05-08 03:00:45.191298]: rationale loss 1.3843
[2025-05-08 03:00:54.230283]: sequential loss 2.3758
[2025-05-08 03:01:52.108324]: top-N loss 1.5323
[2025-05-08 03:01:52.566725]: epoch 2
[2025-05-08 03:02:32.765982]: text loss 1.1808 |  5200/ 5122 batches
[2025-05-08 03:04:16.168717]: text loss 1.1744 |  5400/ 5122 batches
[2025-05-08 03:05:59.700471]: text loss 1.1752 |  5600/ 5122 batches
[2025-05-08 03:07:43.049544]: text loss 1.1651 |  5800/ 5122 batches
[2025-05-08 03:09:26.492068]: text loss 1.1665 |  6000/ 5122 batches
[2025-05-08 03:11:09.859781]: text loss 1.1913 |  6200/ 5122 batches
[2025-05-08 03:12:52.863939]: text loss 1.1663 |  6400/ 5122 batches
[2025-05-08 03:14:34.527551]: text loss 1.1552 |  6600/ 5122 batches
[2025-05-08 03:16:17.355573]: text loss 1.1627 |  6800/ 5122 batches
[2025-05-08 03:18:01.027062]: text loss 1.1556 |  7000/ 5122 batches
[2025-05-08 03:19:44.707756]: text loss 1.1602 |  7200/ 5122 batches
[2025-05-08 03:21:28.078356]: text loss 1.1750 |  7400/ 5122 batches
[2025-05-08 03:23:11.524519]: text loss 1.1660 |  7600/ 5122 batches
[2025-05-08 03:24:55.055802]: text loss 1.1597 |  7800/ 5122 batches
[2025-05-08 03:26:38.255959]: text loss 1.1590 |  8000/ 5122 batches
[2025-05-08 03:28:20.177878]: text loss 1.1542 |  8200/ 5122 batches
[2025-05-08 03:30:03.054243]: text loss 1.1551 |  8400/ 5122 batches
[2025-05-08 03:31:46.709316]: text loss 1.1595 |  8600/ 5122 batches
[2025-05-08 03:33:30.100996]: text loss 1.1515 |  8800/ 5122 batches
[2025-05-08 03:35:13.534377]: text loss 1.1472 |  9000/ 5122 batches
[2025-05-08 03:36:56.997044]: text loss 1.1372 |  9200/ 5122 batches
[2025-05-08 03:38:40.466693]: text loss 1.1468 |  9400/ 5122 batches
[2025-05-08 03:40:23.944576]: text loss 1.1482 |  9600/ 5122 batches
[2025-05-08 03:42:06.720683]: text loss 1.1373 |  9800/ 5122 batches
[2025-05-08 03:43:48.456072]: text loss 1.1484 | 10000/ 5122 batches
[2025-05-08 03:45:30.804840]: text loss 1.1454 | 10200/ 5122 batches
[2025-05-08 03:45:53.393147]: text loss 1.1397 | 10244/ 5122 batches
[2025-05-08 03:45:53.393879]: validation
[2025-05-08 03:45:55.580799]: explanation loss 2.1897
[2025-05-08 03:45:59.529076]: rationale loss 1.3213
[2025-05-08 03:46:08.572520]: sequential loss 2.3382
[2025-05-08 03:47:06.469869]: top-N loss 1.4830
[2025-05-08 03:47:06.923659]: epoch 3
[2025-05-08 03:48:27.409005]: text loss 1.1222 | 10400/ 5122 batches
[2025-05-08 03:50:10.859383]: text loss 1.1327 | 10600/ 5122 batches
[2025-05-08 03:51:54.361204]: text loss 1.1310 | 10800/ 5122 batches
[2025-05-08 03:53:37.812978]: text loss 1.1256 | 11000/ 5122 batches
[2025-05-08 03:55:21.303164]: text loss 1.1349 | 11200/ 5122 batches
[2025-05-08 03:57:04.756262]: text loss 1.1236 | 11400/ 5122 batches
[2025-05-08 03:58:48.358247]: text loss 1.1285 | 11600/ 5122 batches
[2025-05-08 04:00:31.850774]: text loss 1.1288 | 11800/ 5122 batches
[2025-05-08 04:02:15.570649]: text loss 1.1292 | 12000/ 5122 batches
[2025-05-08 04:03:59.422975]: text loss 1.1223 | 12200/ 5122 batches
[2025-05-08 04:05:42.923756]: text loss 1.1112 | 12400/ 5122 batches
[2025-05-08 04:07:25.812153]: text loss 1.1144 | 12600/ 5122 batches
[2025-05-08 04:09:09.425441]: text loss 1.1195 | 12800/ 5122 batches
[2025-05-08 04:10:51.239342]: text loss 1.1268 | 13000/ 5122 batches
[2025-05-08 04:12:34.028892]: text loss 1.1112 | 13200/ 5122 batches
[2025-05-08 04:14:17.591870]: text loss 1.1262 | 13400/ 5122 batches
[2025-05-08 04:16:01.110921]: text loss 1.1308 | 13600/ 5122 batches
[2025-05-08 04:17:44.591629]: text loss 1.1102 | 13800/ 5122 batches
[2025-05-08 04:19:28.088966]: text loss 1.1123 | 14000/ 5122 batches
[2025-05-08 04:21:10.748206]: text loss 1.1042 | 14200/ 5122 batches
[2025-05-08 04:22:52.682662]: text loss 1.1144 | 14400/ 5122 batches
[2025-05-08 04:24:35.082852]: text loss 1.1037 | 14600/ 5122 batches
[2025-05-08 04:26:18.601773]: text loss 1.1104 | 14800/ 5122 batches
[2025-05-08 04:28:02.108298]: text loss 1.1247 | 15000/ 5122 batches
[2025-05-08 04:29:45.656837]: text loss 1.1076 | 15200/ 5122 batches
[2025-05-08 04:31:11.579106]: text loss 1.1163 | 15366/ 5122 batches
[2025-05-08 04:31:11.579845]: validation
[2025-05-08 04:31:13.773129]: explanation loss 2.2414
[2025-05-08 04:31:17.727277]: rationale loss 1.2873
[2025-05-08 04:31:26.774465]: sequential loss 2.3197
[2025-05-08 04:32:24.623470]: top-N loss 1.4422
[2025-05-08 04:32:25.071492]: epoch 4
[2025-05-08 04:32:42.656445]: text loss 1.1014 | 15400/ 5122 batches
[2025-05-08 04:34:26.075091]: text loss 1.0909 | 15600/ 5122 batches
[2025-05-08 04:36:09.743128]: text loss 1.0956 | 15800/ 5122 batches
[2025-05-08 04:37:53.521618]: text loss 1.1070 | 16000/ 5122 batches
[2025-05-08 04:39:37.100596]: text loss 1.1054 | 16200/ 5122 batches
[2025-05-08 04:41:20.575307]: text loss 1.0964 | 16400/ 5122 batches
[2025-05-08 04:43:04.099607]: text loss 1.0953 | 16600/ 5122 batches
[2025-05-08 04:44:47.698308]: text loss 1.1021 | 16800/ 5122 batches
[2025-05-08 04:46:30.115534]: text loss 1.1012 | 17000/ 5122 batches
[2025-05-08 04:48:11.971228]: text loss 1.0905 | 17200/ 5122 batches
[2025-05-08 04:49:53.788931]: text loss 1.0945 | 17400/ 5122 batches
[2025-05-08 04:51:35.603477]: text loss 1.1041 | 17600/ 5122 batches
[2025-05-08 04:53:17.398293]: text loss 1.0827 | 17800/ 5122 batches
[2025-05-08 04:54:59.082440]: text loss 1.1010 | 18000/ 5122 batches
[2025-05-08 04:56:41.732864]: text loss 1.0999 | 18200/ 5122 batches
[2025-05-08 04:58:25.152510]: text loss 1.0774 | 18400/ 5122 batches
[2025-05-08 05:00:08.107802]: text loss 1.0924 | 18600/ 5122 batches
[2025-05-08 05:01:51.650127]: text loss 1.1004 | 18800/ 5122 batches
[2025-05-08 05:03:33.702768]: text loss 1.0931 | 19000/ 5122 batches
[2025-05-08 05:05:15.381810]: text loss 1.0915 | 19200/ 5122 batches
[2025-05-08 05:06:57.206788]: text loss 1.0894 | 19400/ 5122 batches
[2025-05-08 05:08:39.228525]: text loss 1.1034 | 19600/ 5122 batches
[2025-05-08 05:10:22.575911]: text loss 1.0937 | 19800/ 5122 batches
[2025-05-08 05:12:06.220191]: text loss 1.0861 | 20000/ 5122 batches
[2025-05-08 05:13:49.663932]: text loss 1.0956 | 20200/ 5122 batches
[2025-05-08 05:15:33.334623]: text loss 1.0900 | 20400/ 5122 batches
[2025-05-08 05:16:18.996299]: text loss 1.0971 | 20488/ 5122 batches
[2025-05-08 05:16:18.997011]: validation
[2025-05-08 05:16:21.216620]: explanation loss 2.2171
[2025-05-08 05:16:25.179560]: rationale loss 1.2743
[2025-05-08 05:16:34.353005]: sequential loss 2.3255
[2025-05-08 05:17:32.829826]: top-N loss 1.4172
[2025-05-08 05:17:33.280956]: epoch 5
[2025-05-08 05:18:30.196304]: text loss 1.0860 | 20600/ 5122 batches
[2025-05-08 05:20:13.423038]: text loss 1.0755 | 20800/ 5122 batches
[2025-05-08 05:21:55.443253]: text loss 1.0777 | 21000/ 5122 batches
[2025-05-08 05:23:37.033639]: text loss 1.0606 | 21200/ 5122 batches
[2025-05-08 05:25:18.745959]: text loss 1.0793 | 21400/ 5122 batches
[2025-05-08 05:27:00.924671]: text loss 1.0848 | 21600/ 5122 batches
[2025-05-08 05:28:44.180769]: text loss 1.0800 | 21800/ 5122 batches
[2025-05-08 05:30:27.607781]: text loss 1.0757 | 22000/ 5122 batches
[2025-05-08 05:32:10.520186]: text loss 1.0728 | 22200/ 5122 batches
[2025-05-08 05:33:52.300057]: text loss 1.0737 | 22400/ 5122 batches
[2025-05-08 05:35:34.480358]: text loss 1.0697 | 22600/ 5122 batches
[2025-05-08 05:37:17.180397]: text loss 1.0803 | 22800/ 5122 batches
[2025-05-08 05:39:00.621011]: text loss 1.0791 | 23000/ 5122 batches
[2025-05-08 05:40:43.862795]: text loss 1.0707 | 23200/ 5122 batches
[2025-05-08 05:42:27.219125]: text loss 1.0785 | 23400/ 5122 batches
[2025-05-08 05:44:10.512498]: text loss 1.0809 | 23600/ 5122 batches
[2025-05-08 05:45:53.798272]: text loss 1.0793 | 23800/ 5122 batches
[2025-05-08 05:47:37.145456]: text loss 1.0731 | 24000/ 5122 batches
[2025-05-08 05:49:19.935460]: text loss 1.0748 | 24200/ 5122 batches
[2025-05-08 05:51:02.589868]: text loss 1.0750 | 24400/ 5122 batches
[2025-05-08 05:52:46.169623]: text loss 1.0735 | 24600/ 5122 batches
[2025-05-08 05:54:28.058652]: text loss 1.0726 | 24800/ 5122 batches
[2025-05-08 05:56:09.861892]: text loss 1.0726 | 25000/ 5122 batches
[2025-05-08 05:57:52.084263]: text loss 1.0812 | 25200/ 5122 batches
[2025-05-08 05:59:33.964496]: text loss 1.0770 | 25400/ 5122 batches
[2025-05-08 06:01:16.575625]: text loss 1.0739 | 25600/ 5122 batches
[2025-05-08 06:01:21.754594]: text loss 1.0783 | 25610/ 5122 batches
[2025-05-08 06:01:21.755305]: validation
[2025-05-08 06:01:23.951528]: explanation loss 2.2214
[2025-05-08 06:01:27.906056]: rationale loss 1.2559
[2025-05-08 06:01:36.943854]: sequential loss 2.2987
[2025-05-08 06:02:34.677762]: top-N loss 1.3908
[2025-05-08 06:02:35.132569]: epoch 6
[2025-05-08 06:04:13.058440]: text loss 1.0522 | 25800/ 5122 batches
[2025-05-08 06:05:55.986069]: text loss 1.0558 | 26000/ 5122 batches
[2025-05-08 06:07:38.607640]: text loss 1.0639 | 26200/ 5122 batches
[2025-05-08 06:09:22.230268]: text loss 1.0522 | 26400/ 5122 batches
[2025-05-08 06:11:05.916362]: text loss 1.0629 | 26600/ 5122 batches
[2025-05-08 06:12:49.198974]: text loss 1.0600 | 26800/ 5122 batches
[2025-05-08 06:14:32.845194]: text loss 1.0611 | 27000/ 5122 batches
[2025-05-08 06:16:15.808067]: text loss 1.0682 | 27200/ 5122 batches
[2025-05-08 06:17:57.550747]: text loss 1.0615 | 27400/ 5122 batches
[2025-05-08 06:19:41.315908]: text loss 1.0661 | 27600/ 5122 batches
[2025-05-08 06:21:24.905250]: text loss 1.0518 | 27800/ 5122 batches
[2025-05-08 06:23:08.298896]: text loss 1.0662 | 28000/ 5122 batches
[2025-05-08 06:24:50.717362]: text loss 1.0603 | 28200/ 5122 batches
[2025-05-08 06:26:32.612659]: text loss 1.0598 | 28400/ 5122 batches
[2025-05-08 06:28:16.129614]: text loss 1.0698 | 28600/ 5122 batches
[2025-05-08 06:29:58.830074]: text loss 1.0667 | 28800/ 5122 batches
[2025-05-08 06:31:40.653506]: text loss 1.0542 | 29000/ 5122 batches
[2025-05-08 06:33:22.715996]: text loss 1.0566 | 29200/ 5122 batches
[2025-05-08 06:35:05.059438]: text loss 1.0533 | 29400/ 5122 batches
[2025-05-08 06:36:48.603576]: text loss 1.0625 | 29600/ 5122 batches
[2025-05-08 06:38:32.289836]: text loss 1.0593 | 29800/ 5122 batches
[2025-05-08 06:40:15.698685]: text loss 1.0651 | 30000/ 5122 batches
[2025-05-08 06:41:57.678098]: text loss 1.0510 | 30200/ 5122 batches
[2025-05-08 06:43:41.360222]: text loss 1.0610 | 30400/ 5122 batches
[2025-05-08 06:45:25.003953]: text loss 1.0544 | 30600/ 5122 batches
[2025-05-08 06:46:33.417545]: text loss 1.0552 | 30732/ 5122 batches
[2025-05-08 06:46:33.418194]: validation
[2025-05-08 06:46:35.622283]: explanation loss 2.1758
[2025-05-08 06:46:39.598715]: rationale loss 1.2433
[2025-05-08 06:46:48.667330]: sequential loss 2.2943
[2025-05-08 06:47:46.655929]: top-N loss 1.3623
[2025-05-08 06:47:47.098360]: epoch 7
[2025-05-08 06:48:21.693088]: text loss 1.0439 | 30800/ 5122 batches
[2025-05-08 06:50:03.876393]: text loss 1.0387 | 31000/ 5122 batches
[2025-05-08 06:51:47.305765]: text loss 1.0387 | 31200/ 5122 batches
[2025-05-08 06:53:30.807652]: text loss 1.0396 | 31400/ 5122 batches
[2025-05-08 06:55:14.556970]: text loss 1.0453 | 31600/ 5122 batches
[2025-05-08 06:56:58.217379]: text loss 1.0557 | 31800/ 5122 batches
[2025-05-08 06:58:41.873132]: text loss 1.0456 | 32000/ 5122 batches
[2025-05-08 07:00:25.220820]: text loss 1.0470 | 32200/ 5122 batches
[2025-05-08 07:02:08.641260]: text loss 1.0501 | 32400/ 5122 batches
[2025-05-08 07:03:52.106205]: text loss 1.0379 | 32600/ 5122 batches
[2025-05-08 07:05:35.924808]: text loss 1.0470 | 32800/ 5122 batches
[2025-05-08 07:07:19.066419]: text loss 1.0446 | 33000/ 5122 batches
[2025-05-08 07:09:02.028466]: text loss 1.0556 | 33200/ 5122 batches
[2025-05-08 07:10:45.276535]: text loss 1.0491 | 33400/ 5122 batches
[2025-05-08 07:12:28.660982]: text loss 1.0437 | 33600/ 5122 batches
[2025-05-08 07:14:12.370769]: text loss 1.0502 | 33800/ 5122 batches
[2025-05-08 07:15:54.585766]: text loss 1.0364 | 34000/ 5122 batches
[2025-05-08 07:17:36.590227]: text loss 1.0380 | 34200/ 5122 batches
[2025-05-08 07:19:18.183747]: text loss 1.0388 | 34400/ 5122 batches
[2025-05-08 07:21:00.172047]: text loss 1.0584 | 34600/ 5122 batches
[2025-05-08 07:22:42.071501]: text loss 1.0439 | 34800/ 5122 batches
[2025-05-08 07:24:23.915652]: text loss 1.0542 | 35000/ 5122 batches
[2025-05-08 07:26:06.229812]: text loss 1.0461 | 35200/ 5122 batches
[2025-05-08 07:27:49.947314]: text loss 1.0454 | 35400/ 5122 batches
[2025-05-08 07:29:32.427442]: text loss 1.0551 | 35600/ 5122 batches
[2025-05-08 07:31:15.314991]: text loss 1.0454 | 35800/ 5122 batches
[2025-05-08 07:31:42.901251]: text loss 1.0470 | 35854/ 5122 batches
[2025-05-08 07:31:42.901896]: validation
[2025-05-08 07:31:45.102993]: explanation loss 2.2126
[2025-05-08 07:31:49.068342]: rationale loss 1.2404
[2025-05-08 07:31:58.110199]: sequential loss 2.2775
[2025-05-08 07:32:56.107015]: top-N loss 1.3479
[2025-05-08 07:32:56.107113]: Endured 1 time(s)
[2025-05-08 07:32:56.107138]: epoch 8
[2025-05-08 07:34:10.651743]: text loss 1.0271 | 36000/ 5122 batches
[2025-05-08 07:35:52.760005]: text loss 1.0418 | 36200/ 5122 batches
[2025-05-08 07:37:34.581826]: text loss 1.0244 | 36400/ 5122 batches
[2025-05-08 07:39:16.472957]: text loss 1.0317 | 36600/ 5122 batches
[2025-05-08 07:40:59.353763]: text loss 1.0309 | 36800/ 5122 batches
[2025-05-08 07:42:40.933397]: text loss 1.0325 | 37000/ 5122 batches
[2025-05-08 07:44:24.476850]: text loss 1.0286 | 37200/ 5122 batches
[2025-05-08 07:46:08.217374]: text loss 1.0196 | 37400/ 5122 batches
[2025-05-08 07:47:51.477852]: text loss 1.0312 | 37600/ 5122 batches
[2025-05-08 07:49:33.272622]: text loss 1.0364 | 37800/ 5122 batches
[2025-05-08 07:51:15.088690]: text loss 1.0333 | 38000/ 5122 batches
[2025-05-08 07:52:58.769682]: text loss 1.0379 | 38200/ 5122 batches
[2025-05-08 07:54:40.724002]: text loss 1.0408 | 38400/ 5122 batches
[2025-05-08 07:56:23.574858]: text loss 1.0391 | 38600/ 5122 batches
[2025-05-08 07:58:07.273993]: text loss 1.0303 | 38800/ 5122 batches
[2025-05-08 07:59:50.900942]: text loss 1.0374 | 39000/ 5122 batches
[2025-05-08 08:01:34.833335]: text loss 1.0293 | 39200/ 5122 batches
[2025-05-08 08:03:17.888872]: text loss 1.0434 | 39400/ 5122 batches
[2025-05-08 08:05:01.704755]: text loss 1.0347 | 39600/ 5122 batches
[2025-05-08 08:06:45.277519]: text loss 1.0305 | 39800/ 5122 batches
[2025-05-08 08:08:29.003435]: text loss 1.0321 | 40000/ 5122 batches
[2025-05-08 08:10:12.804821]: text loss 1.0365 | 40200/ 5122 batches
[2025-05-08 08:11:56.656740]: text loss 1.0341 | 40400/ 5122 batches
[2025-05-08 08:13:40.446480]: text loss 1.0310 | 40600/ 5122 batches
[2025-05-08 08:15:24.038428]: text loss 1.0472 | 40800/ 5122 batches
[2025-05-08 08:16:55.308801]: text loss 1.0272 | 40976/ 5122 batches
[2025-05-08 08:16:55.309403]: validation
[2025-05-08 08:16:57.494763]: explanation loss 2.1731
[2025-05-08 08:17:01.460301]: rationale loss 1.2370
[2025-05-08 08:17:10.511972]: sequential loss 2.2889
[2025-05-08 08:18:08.439651]: top-N loss 1.3315
[2025-05-08 08:18:08.871749]: epoch 9
[2025-05-08 08:18:21.123414]: text loss 1.0336 | 41000/ 5122 batches
[2025-05-08 08:20:04.627398]: text loss 1.0159 | 41200/ 5122 batches
[2025-05-08 08:21:48.265321]: text loss 1.0211 | 41400/ 5122 batches
[2025-05-08 08:23:31.976438]: text loss 1.0200 | 41600/ 5122 batches
[2025-05-08 08:25:15.270523]: text loss 1.0225 | 41800/ 5122 batches
[2025-05-08 08:26:59.029430]: text loss 1.0206 | 42000/ 5122 batches
[2025-05-08 08:28:42.327755]: text loss 1.0187 | 42200/ 5122 batches
[2025-05-08 08:30:25.958459]: text loss 1.0253 | 42400/ 5122 batches
[2025-05-08 08:32:09.746909]: text loss 1.0200 | 42600/ 5122 batches
[2025-05-08 08:33:53.201722]: text loss 1.0212 | 42800/ 5122 batches
[2025-05-08 08:35:36.682284]: text loss 1.0145 | 43000/ 5122 batches
[2025-05-08 08:37:20.414203]: text loss 1.0196 | 43200/ 5122 batches
[2025-05-08 08:39:04.111770]: text loss 1.0186 | 43400/ 5122 batches
[2025-05-08 08:40:47.329410]: text loss 1.0269 | 43600/ 5122 batches
[2025-05-08 08:42:30.360884]: text loss 1.0258 | 43800/ 5122 batches
[2025-05-08 08:44:14.122111]: text loss 1.0142 | 44000/ 5122 batches
[2025-05-08 08:45:57.995570]: text loss 1.0274 | 44200/ 5122 batches
[2025-05-08 08:47:41.620429]: text loss 1.0234 | 44400/ 5122 batches
[2025-05-08 08:49:25.245112]: text loss 1.0246 | 44600/ 5122 batches
[2025-05-08 08:51:08.722102]: text loss 1.0221 | 44800/ 5122 batches
[2025-05-08 08:52:52.463994]: text loss 1.0305 | 45000/ 5122 batches
[2025-05-08 08:54:36.124460]: text loss 1.0298 | 45200/ 5122 batches
[2025-05-08 08:56:19.987030]: text loss 1.0338 | 45400/ 5122 batches
[2025-05-08 08:58:03.713748]: text loss 1.0216 | 45600/ 5122 batches
[2025-05-08 08:59:47.526329]: text loss 1.0267 | 45800/ 5122 batches
[2025-05-08 09:01:31.232464]: text loss 1.0169 | 46000/ 5122 batches
[2025-05-08 09:02:22.112547]: text loss 1.0230 | 46098/ 5122 batches
[2025-05-08 09:02:22.113239]: validation
[2025-05-08 09:02:24.306100]: explanation loss 2.1565
[2025-05-08 09:02:28.257372]: rationale loss 1.2797
[2025-05-08 09:02:37.309265]: sequential loss 2.2956
[2025-05-08 09:03:35.156454]: top-N loss 1.3264
[2025-05-08 09:03:35.156542]: Endured 2 time(s)
[2025-05-08 09:03:35.156570]: epoch 10
[2025-05-08 09:04:26.908455]: text loss 1.0207 | 46200/ 5122 batches
[2025-05-08 09:06:08.851854]: text loss 1.0096 | 46400/ 5122 batches
[2025-05-08 09:07:50.831387]: text loss 1.0028 | 46600/ 5122 batches
[2025-05-08 09:09:34.323621]: text loss 1.0069 | 46800/ 5122 batches
[2025-05-08 09:11:18.552683]: text loss 1.0100 | 47000/ 5122 batches
[2025-05-08 09:13:00.706264]: text loss 1.0138 | 47200/ 5122 batches
[2025-05-08 09:14:42.728303]: text loss 1.0095 | 47400/ 5122 batches
[2025-05-08 09:16:25.362252]: text loss 1.0109 | 47600/ 5122 batches
[2025-05-08 09:18:08.950471]: text loss 1.0083 | 47800/ 5122 batches
[2025-05-08 09:19:52.724441]: text loss 1.0171 | 48000/ 5122 batches
[2025-05-08 09:21:36.343729]: text loss 1.0115 | 48200/ 5122 batches
[2025-05-08 09:23:19.936080]: text loss 1.0137 | 48400/ 5122 batches
[2025-05-08 09:25:03.892033]: text loss 1.0111 | 48600/ 5122 batches
[2025-05-08 09:26:47.364526]: text loss 1.0068 | 48800/ 5122 batches
[2025-05-08 09:28:31.089464]: text loss 1.0127 | 49000/ 5122 batches
[2025-05-08 09:30:14.915552]: text loss 1.0081 | 49200/ 5122 batches
[2025-05-08 09:31:58.783306]: text loss 1.0068 | 49400/ 5122 batches
[2025-05-08 09:33:42.820210]: text loss 0.9970 | 49600/ 5122 batches
[2025-05-08 09:35:26.237956]: text loss 1.0159 | 49800/ 5122 batches
[2025-05-08 09:37:07.999821]: text loss 1.0159 | 50000/ 5122 batches
[2025-05-08 09:38:50.006549]: text loss 1.0167 | 50200/ 5122 batches
[2025-05-08 09:40:31.834683]: text loss 1.0171 | 50400/ 5122 batches
[2025-05-08 09:42:14.840179]: text loss 1.0111 | 50600/ 5122 batches
[2025-05-08 09:43:59.267887]: text loss 1.0139 | 50800/ 5122 batches
[2025-05-08 09:45:44.840969]: text loss 1.0137 | 51000/ 5122 batches
[2025-05-08 09:47:28.637323]: text loss 1.0172 | 51200/ 5122 batches
[2025-05-08 09:47:39.020650]: text loss 1.0319 | 51220/ 5122 batches
[2025-05-08 09:47:39.021275]: validation
[2025-05-08 09:47:41.224842]: explanation loss 2.1556
[2025-05-08 09:47:45.201375]: rationale loss 1.2369
[2025-05-08 09:47:54.269934]: sequential loss 2.2947
[2025-05-08 09:48:52.309640]: top-N loss 1.3152
[2025-05-08 09:48:52.749069]: epoch 11
[2025-05-08 09:50:24.347939]: text loss 0.9885 | 51400/ 5122 batches
[2025-05-08 09:52:06.401645]: text loss 0.9889 | 51600/ 5122 batches
[2025-05-08 09:53:48.164699]: text loss 0.9919 | 51800/ 5122 batches
[2025-05-08 09:55:30.147622]: text loss 1.0000 | 52000/ 5122 batches
[2025-05-08 09:57:12.256066]: text loss 1.0106 | 52200/ 5122 batches
[2025-05-08 09:58:54.210679]: text loss 1.0004 | 52400/ 5122 batches
[2025-05-08 10:00:37.462071]: text loss 0.9996 | 52600/ 5122 batches
[2025-05-08 10:02:21.442115]: text loss 0.9950 | 52800/ 5122 batches
[2025-05-08 10:04:05.588967]: text loss 0.9980 | 53000/ 5122 batches
[2025-05-08 10:05:47.949011]: text loss 1.0013 | 53200/ 5122 batches
[2025-05-08 10:07:29.865759]: text loss 0.9988 | 53400/ 5122 batches
[2025-05-08 10:09:11.909502]: text loss 0.9983 | 53600/ 5122 batches
[2025-05-08 10:10:53.771827]: text loss 0.9963 | 53800/ 5122 batches
[2025-05-08 10:12:35.390489]: text loss 1.0168 | 54000/ 5122 batches
[2025-05-08 10:14:18.280912]: text loss 0.9980 | 54200/ 5122 batches
[2025-05-08 10:16:01.586235]: text loss 1.0116 | 54400/ 5122 batches
[2025-05-08 10:17:43.811985]: text loss 1.0065 | 54600/ 5122 batches
[2025-05-08 10:19:25.682932]: text loss 1.0007 | 54800/ 5122 batches
[2025-05-08 10:21:07.672020]: text loss 1.0033 | 55000/ 5122 batches
[2025-05-08 10:22:50.869556]: text loss 1.0106 | 55200/ 5122 batches
[2025-05-08 10:24:34.516611]: text loss 1.0096 | 55400/ 5122 batches
[2025-05-08 10:26:17.844922]: text loss 1.0116 | 55600/ 5122 batches
[2025-05-08 10:28:01.669266]: text loss 1.0014 | 55800/ 5122 batches
[2025-05-08 10:29:45.196252]: text loss 0.9952 | 56000/ 5122 batches
[2025-05-08 10:31:28.569676]: text loss 0.9989 | 56200/ 5122 batches
[2025-05-08 10:32:42.187842]: text loss 1.0067 | 56342/ 5122 batches
[2025-05-08 10:32:42.188589]: validation
[2025-05-08 10:32:44.392602]: explanation loss 2.1589
[2025-05-08 10:32:48.374297]: rationale loss 1.2401
[2025-05-08 10:32:57.465485]: sequential loss 2.3262
[2025-05-08 10:33:55.479407]: top-N loss 1.3025
[2025-05-08 10:33:55.479503]: Endured 3 time(s)
[2025-05-08 10:33:55.479524]: epoch 12
[2025-05-08 10:34:24.932784]: text loss 0.9784 | 56400/ 5122 batches
[2025-05-08 10:36:06.960844]: text loss 0.9833 | 56600/ 5122 batches
[2025-05-08 10:37:50.850744]: text loss 0.9930 | 56800/ 5122 batches
[2025-05-08 10:39:34.224833]: text loss 0.9774 | 57000/ 5122 batches
[2025-05-08 10:41:17.525100]: text loss 0.9922 | 57200/ 5122 batches
[2025-05-08 10:43:01.645722]: text loss 0.9918 | 57400/ 5122 batches
[2025-05-08 10:44:44.789078]: text loss 0.9918 | 57600/ 5122 batches
[2025-05-08 10:46:27.922029]: text loss 0.9993 | 57800/ 5122 batches
[2025-05-08 10:48:11.329425]: text loss 0.9879 | 58000/ 5122 batches
[2025-05-08 10:49:54.658562]: text loss 0.9884 | 58200/ 5122 batches
[2025-05-08 10:51:38.626585]: text loss 0.9896 | 58400/ 5122 batches
[2025-05-08 10:53:22.162413]: text loss 0.9913 | 58600/ 5122 batches
[2025-05-08 10:55:04.967778]: text loss 0.9911 | 58800/ 5122 batches
[2025-05-08 10:56:47.802495]: text loss 1.0029 | 59000/ 5122 batches
[2025-05-08 10:58:30.191087]: text loss 0.9896 | 59200/ 5122 batches
[2025-05-08 11:00:13.315026]: text loss 0.9864 | 59400/ 5122 batches
[2025-05-08 11:01:56.014288]: text loss 0.9884 | 59600/ 5122 batches
[2025-05-08 11:03:38.036462]: text loss 1.0026 | 59800/ 5122 batches
[2025-05-08 11:05:21.632036]: text loss 0.9939 | 60000/ 5122 batches
[2025-05-08 11:07:05.463889]: text loss 0.9895 | 60200/ 5122 batches
[2025-05-08 11:08:49.290445]: text loss 0.9967 | 60400/ 5122 batches
[2025-05-08 11:10:32.905154]: text loss 0.9914 | 60600/ 5122 batches
[2025-05-08 11:12:15.450098]: text loss 0.9900 | 60800/ 5122 batches
[2025-05-08 11:13:57.688083]: text loss 0.9883 | 61000/ 5122 batches
[2025-05-08 11:15:39.625763]: text loss 0.9988 | 61200/ 5122 batches
[2025-05-08 11:17:22.341855]: text loss 0.9930 | 61400/ 5122 batches
[2025-05-08 11:17:55.787238]: text loss 0.9973 | 61464/ 5122 batches
[2025-05-08 11:17:55.788036]: validation
[2025-05-08 11:17:57.994832]: explanation loss 2.1433
[2025-05-08 11:18:01.971824]: rationale loss 1.2299
[2025-05-08 11:18:11.066111]: sequential loss 2.2879
[2025-05-08 11:19:09.776264]: top-N loss 1.2939
[2025-05-08 11:19:10.205286]: epoch 13
[2025-05-08 11:20:19.522025]: text loss 0.9639 | 61600/ 5122 batches
[2025-05-08 11:22:01.554672]: text loss 0.9716 | 61800/ 5122 batches
[2025-05-08 11:23:44.857502]: text loss 0.9752 | 62000/ 5122 batches
[2025-05-08 11:25:28.737838]: text loss 0.9645 | 62200/ 5122 batches
[2025-05-08 11:27:12.237975]: text loss 0.9777 | 62400/ 5122 batches
[2025-05-08 11:28:54.327732]: text loss 0.9805 | 62600/ 5122 batches
[2025-05-08 11:30:36.596480]: text loss 0.9766 | 62800/ 5122 batches
[2025-05-08 11:32:19.797400]: text loss 0.9776 | 63000/ 5122 batches
[2025-05-08 11:34:01.898223]: text loss 0.9832 | 63200/ 5122 batches
[2025-05-08 11:35:43.922699]: text loss 0.9843 | 63400/ 5122 batches
[2025-05-08 11:37:25.988116]: text loss 0.9874 | 63600/ 5122 batches
[2025-05-08 11:39:08.040256]: text loss 0.9833 | 63800/ 5122 batches
[2025-05-08 11:40:50.071339]: text loss 0.9841 | 64000/ 5122 batches
[2025-05-08 11:42:32.094531]: text loss 0.9879 | 64200/ 5122 batches
[2025-05-08 11:44:15.508628]: text loss 0.9870 | 64400/ 5122 batches
[2025-05-08 11:45:59.231285]: text loss 0.9830 | 64600/ 5122 batches
[2025-05-08 11:47:42.871284]: text loss 0.9887 | 64800/ 5122 batches
[2025-05-08 11:49:26.553781]: text loss 0.9800 | 65000/ 5122 batches
[2025-05-08 11:51:09.822150]: text loss 0.9807 | 65200/ 5122 batches
[2025-05-08 11:52:51.852250]: text loss 0.9800 | 65400/ 5122 batches
[2025-05-08 11:54:33.867568]: text loss 0.9823 | 65600/ 5122 batches
[2025-05-08 11:56:15.840762]: text loss 0.9810 | 65800/ 5122 batches
[2025-05-08 11:57:57.800290]: text loss 0.9860 | 66000/ 5122 batches
[2025-05-08 11:59:40.809086]: text loss 0.9814 | 66200/ 5122 batches
[2025-05-08 12:01:24.409033]: text loss 0.9935 | 66400/ 5122 batches
[2025-05-08 12:03:00.536678]: text loss 0.9882 | 66586/ 5122 batches
[2025-05-08 12:03:00.537433]: validation
[2025-05-08 12:03:02.737629]: explanation loss 2.1534
[2025-05-08 12:03:06.704710]: rationale loss 1.2354
[2025-05-08 12:03:15.780549]: sequential loss 2.2861
[2025-05-08 12:04:13.726157]: top-N loss 1.2864
[2025-05-08 12:04:13.726247]: Endured 4 time(s)
[2025-05-08 12:04:13.726269]: epoch 14
[2025-05-08 12:04:20.901217]: text loss 1.0302 | 66600/ 5122 batches
[2025-05-08 12:06:02.708853]: text loss 0.9685 | 66800/ 5122 batches
[2025-05-08 12:07:44.756487]: text loss 0.9669 | 67000/ 5122 batches
[2025-05-08 12:09:26.795175]: text loss 0.9682 | 67200/ 5122 batches
[2025-05-08 12:11:08.792487]: text loss 0.9613 | 67400/ 5122 batches
[2025-05-08 12:12:50.777939]: text loss 0.9710 | 67600/ 5122 batches
[2025-05-08 12:14:32.793438]: text loss 0.9657 | 67800/ 5122 batches
[2025-05-08 12:16:14.816667]: text loss 0.9688 | 68000/ 5122 batches
[2025-05-08 12:17:56.894822]: text loss 0.9737 | 68200/ 5122 batches
[2025-05-08 12:19:38.955067]: text loss 0.9695 | 68400/ 5122 batches
[2025-05-08 12:21:21.017075]: text loss 0.9774 | 68600/ 5122 batches
[2025-05-08 12:23:03.004335]: text loss 0.9737 | 68800/ 5122 batches
[2025-05-08 12:24:45.038347]: text loss 0.9688 | 69000/ 5122 batches
[2025-05-08 12:26:27.110810]: text loss 0.9722 | 69200/ 5122 batches
[2025-05-08 12:28:10.675154]: text loss 0.9673 | 69400/ 5122 batches
[2025-05-08 12:29:54.519941]: text loss 0.9769 | 69600/ 5122 batches
[2025-05-08 12:31:38.251685]: text loss 0.9729 | 69800/ 5122 batches
[2025-05-08 12:33:21.942729]: text loss 0.9748 | 70000/ 5122 batches
[2025-05-08 12:35:05.168908]: text loss 0.9653 | 70200/ 5122 batches
[2025-05-08 12:36:47.156553]: text loss 0.9720 | 70400/ 5122 batches
[2025-05-08 12:38:29.312222]: text loss 0.9805 | 70600/ 5122 batches
[2025-05-08 12:40:11.320933]: text loss 0.9817 | 70800/ 5122 batches
[2025-05-08 12:41:53.352250]: text loss 0.9662 | 71000/ 5122 batches
[2025-05-08 12:43:35.373325]: text loss 0.9798 | 71200/ 5122 batches
[2025-05-08 12:45:17.487016]: text loss 0.9706 | 71400/ 5122 batches
[2025-05-08 12:47:01.326016]: text loss 0.9669 | 71600/ 5122 batches
[2025-05-08 12:47:57.408982]: text loss 0.9796 | 71708/ 5122 batches
[2025-05-08 12:47:57.409693]: validation
[2025-05-08 12:47:59.610712]: explanation loss 2.1517
[2025-05-08 12:48:03.586287]: rationale loss 1.2314
[2025-05-08 12:48:12.655657]: sequential loss 2.3251
[2025-05-08 12:49:10.646308]: top-N loss 1.2852
[2025-05-08 12:49:10.646398]: Endured 5 time(s)
[2025-05-08 12:49:10.646417]: Cannot endure it anymore | Exiting from early stop
