nohup: ignoring input
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of Solomon were not initialized from the model checkpoint at t5-small and are newly initialized: ['conv1d.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/toys/
model_version                            0
task_num                                 5
prompt_num                               3
lr                                       0.0005
ratio                                    1:1:1:1
epochs                                   100
batch_size                               64
cuda                                     True
log_interval                             200
checkpoint                               ./checkpoint_0520_max/toys_1111/
endure_times                             5
exp_len                                  20
negative_num                             99
----------------------------------------ARGUMENTS----------------------------------------
[2025-05-20 16:36:34.098775]: Loading data
[2025-05-20 16:36:43.974303]: Start training
[2025-05-20 16:36:43.974386]: epoch 1
[2025-05-20 16:38:24.877690]: text loss 1.4784 |   200/ 5122 batches
[2025-05-20 16:40:05.799386]: text loss 1.1828 |   400/ 5122 batches
[2025-05-20 16:41:46.714523]: text loss 1.1495 |   600/ 5122 batches
[2025-05-20 16:43:27.761788]: text loss 1.1467 |   800/ 5122 batches
[2025-05-20 16:45:08.802396]: text loss 1.1307 |  1000/ 5122 batches
[2025-05-20 16:46:49.999284]: text loss 1.1028 |  1200/ 5122 batches
[2025-05-20 16:48:31.054922]: text loss 1.1046 |  1400/ 5122 batches
[2025-05-20 16:50:12.151016]: text loss 1.1086 |  1600/ 5122 batches
[2025-05-20 16:51:53.435137]: text loss 1.1108 |  1800/ 5122 batches
[2025-05-20 16:53:34.609713]: text loss 1.1233 |  2000/ 5122 batches
[2025-05-20 16:55:15.736781]: text loss 1.0920 |  2200/ 5122 batches
[2025-05-20 16:56:57.006019]: text loss 1.1041 |  2400/ 5122 batches
[2025-05-20 16:58:38.345243]: text loss 1.1139 |  2600/ 5122 batches
[2025-05-20 17:00:19.429252]: text loss 1.0880 |  2800/ 5122 batches
[2025-05-20 17:02:00.446338]: text loss 1.1040 |  3000/ 5122 batches
[2025-05-20 17:03:41.611089]: text loss 1.1375 |  3200/ 5122 batches
[2025-05-20 17:05:22.575748]: text loss 1.1047 |  3400/ 5122 batches
[2025-05-20 17:07:03.699279]: text loss 1.1313 |  3600/ 5122 batches
[2025-05-20 17:08:44.590085]: text loss 1.0820 |  3800/ 5122 batches
[2025-05-20 17:10:25.519086]: text loss 1.1276 |  4000/ 5122 batches
[2025-05-20 17:12:06.747780]: text loss 1.1267 |  4200/ 5122 batches
[2025-05-20 17:13:47.971937]: text loss 1.1105 |  4400/ 5122 batches
[2025-05-20 17:15:29.308015]: text loss 1.1271 |  4600/ 5122 batches
[2025-05-20 17:17:10.519425]: text loss 1.1182 |  4800/ 5122 batches
[2025-05-20 17:18:51.566752]: text loss 1.1338 |  5000/ 5122 batches
[2025-05-20 17:19:53.446118]: text loss 1.1325 |  5122/ 5122 batches
[2025-05-20 17:19:53.447199]: validation
[2025-05-20 17:19:55.641773]: explanation loss 2.3676
[2025-05-20 17:19:59.555665]: rationale loss 1.1238
[2025-05-20 17:20:08.748891]: sequential loss 2.3851
[2025-05-20 17:21:08.561364]: top-N loss 1.5342
[2025-05-20 17:21:09.006490]: epoch 2
[2025-05-20 17:21:48.350191]: text loss 1.0564 |  5200/ 5122 batches
[2025-05-20 17:23:29.665446]: text loss 1.0684 |  5400/ 5122 batches
[2025-05-20 17:25:10.781687]: text loss 1.0718 |  5600/ 5122 batches
[2025-05-20 17:26:52.225060]: text loss 1.0589 |  5800/ 5122 batches
[2025-05-20 17:28:33.184888]: text loss 1.0526 |  6000/ 5122 batches
[2025-05-20 17:30:14.379444]: text loss 1.0564 |  6200/ 5122 batches
[2025-05-20 17:31:55.610692]: text loss 1.0539 |  6400/ 5122 batches
[2025-05-20 17:33:36.914320]: text loss 1.0572 |  6600/ 5122 batches
[2025-05-20 17:35:18.331059]: text loss 1.0477 |  6800/ 5122 batches
[2025-05-20 17:36:59.449336]: text loss 1.0446 |  7000/ 5122 batches
[2025-05-20 17:38:40.585011]: text loss 1.0546 |  7200/ 5122 batches
[2025-05-20 17:40:21.938306]: text loss 1.0536 |  7400/ 5122 batches
[2025-05-20 17:42:03.176874]: text loss 1.0505 |  7600/ 5122 batches
[2025-05-20 17:43:44.295707]: text loss 1.0531 |  7800/ 5122 batches
[2025-05-20 17:45:25.555433]: text loss 1.0403 |  8000/ 5122 batches
[2025-05-20 17:47:06.520385]: text loss 1.0447 |  8200/ 5122 batches
[2025-05-20 17:48:47.673363]: text loss 1.0417 |  8400/ 5122 batches
[2025-05-20 17:50:28.832698]: text loss 1.0416 |  8600/ 5122 batches
[2025-05-20 17:52:09.943992]: text loss 1.0527 |  8800/ 5122 batches
[2025-05-20 17:53:51.178695]: text loss 1.0451 |  9000/ 5122 batches
[2025-05-20 17:55:32.289839]: text loss 1.0444 |  9200/ 5122 batches
[2025-05-20 17:57:13.518041]: text loss 1.0357 |  9400/ 5122 batches
[2025-05-20 17:58:54.622404]: text loss 1.0316 |  9600/ 5122 batches
[2025-05-20 18:00:35.783374]: text loss 1.0339 |  9800/ 5122 batches
[2025-05-20 18:02:16.726337]: text loss 1.0341 | 10000/ 5122 batches
[2025-05-20 18:03:57.459414]: text loss 1.0452 | 10200/ 5122 batches
[2025-05-20 18:04:19.689660]: text loss 1.0429 | 10244/ 5122 batches
[2025-05-20 18:04:19.690403]: validation
[2025-05-20 18:04:21.863993]: explanation loss 2.9752
[2025-05-20 18:04:25.775923]: rationale loss 1.0797
[2025-05-20 18:04:34.953900]: sequential loss 2.3486
[2025-05-20 18:05:34.468877]: top-N loss 1.4727
[2025-05-20 18:05:34.468970]: Endured 1 time(s)
[2025-05-20 18:05:34.468994]: epoch 3
[2025-05-20 18:06:53.354858]: text loss 1.0191 | 10400/ 5122 batches
[2025-05-20 18:08:34.148696]: text loss 1.0222 | 10600/ 5122 batches
[2025-05-20 18:10:15.163107]: text loss 1.0252 | 10800/ 5122 batches
[2025-05-20 18:11:56.320604]: text loss 1.0106 | 11000/ 5122 batches
[2025-05-20 18:13:37.246036]: text loss 1.0149 | 11200/ 5122 batches
[2025-05-20 18:15:18.608236]: text loss 1.0232 | 11400/ 5122 batches
[2025-05-20 18:16:59.720505]: text loss 1.0238 | 11600/ 5122 batches
[2025-05-20 18:18:40.948640]: text loss 1.0209 | 11800/ 5122 batches
[2025-05-20 18:20:22.211835]: text loss 1.0185 | 12000/ 5122 batches
[2025-05-20 18:22:03.485788]: text loss 1.0206 | 12200/ 5122 batches
[2025-05-20 18:23:44.441827]: text loss 1.0243 | 12400/ 5122 batches
[2025-05-20 18:25:25.751918]: text loss 1.0132 | 12600/ 5122 batches
[2025-05-20 18:27:06.620803]: text loss 1.0095 | 12800/ 5122 batches
[2025-05-20 18:28:47.676813]: text loss 1.0111 | 13000/ 5122 batches
[2025-05-20 18:30:29.048515]: text loss 1.0105 | 13200/ 5122 batches
[2025-05-20 18:32:09.924692]: text loss 1.0146 | 13400/ 5122 batches
[2025-05-20 18:33:50.780215]: text loss 1.0168 | 13600/ 5122 batches
[2025-05-20 18:35:31.747796]: text loss 1.0227 | 13800/ 5122 batches
[2025-05-20 18:37:12.966797]: text loss 1.0270 | 14000/ 5122 batches
[2025-05-20 18:38:53.948278]: text loss 1.0157 | 14200/ 5122 batches
[2025-05-20 18:40:35.408766]: text loss 1.0077 | 14400/ 5122 batches
[2025-05-20 18:42:16.239387]: text loss 1.0201 | 14600/ 5122 batches
[2025-05-20 18:43:57.148970]: text loss 1.0104 | 14800/ 5122 batches
[2025-05-20 18:45:38.161845]: text loss 1.0161 | 15000/ 5122 batches
[2025-05-20 18:47:19.020739]: text loss 1.0154 | 15200/ 5122 batches
[2025-05-20 18:48:42.971885]: text loss 1.0121 | 15366/ 5122 batches
[2025-05-20 18:48:42.972792]: validation
[2025-05-20 18:48:45.133217]: explanation loss 2.1541
[2025-05-20 18:48:49.036443]: rationale loss 1.0616
[2025-05-20 18:48:58.212287]: sequential loss 2.3270
[2025-05-20 18:49:57.780163]: top-N loss 1.4353
[2025-05-20 18:49:58.236515]: epoch 4
[2025-05-20 18:50:15.557024]: text loss 0.9878 | 15400/ 5122 batches
[2025-05-20 18:51:56.474230]: text loss 0.9998 | 15600/ 5122 batches
[2025-05-20 18:53:37.530676]: text loss 0.9949 | 15800/ 5122 batches
[2025-05-20 18:55:18.612202]: text loss 0.9933 | 16000/ 5122 batches
[2025-05-20 18:56:59.550493]: text loss 0.9884 | 16200/ 5122 batches
[2025-05-20 18:58:40.571335]: text loss 0.9977 | 16400/ 5122 batches
[2025-05-20 19:00:21.750303]: text loss 1.0087 | 16600/ 5122 batches
[2025-05-20 19:02:02.381417]: text loss 0.9945 | 16800/ 5122 batches
[2025-05-20 19:03:43.304602]: text loss 0.9921 | 17000/ 5122 batches
[2025-05-20 19:05:24.281051]: text loss 0.9989 | 17200/ 5122 batches
[2025-05-20 19:07:05.239225]: text loss 1.0006 | 17400/ 5122 batches
[2025-05-20 19:08:45.858184]: text loss 0.9973 | 17600/ 5122 batches
[2025-05-20 19:10:26.919487]: text loss 1.0076 | 17800/ 5122 batches
[2025-05-20 19:12:07.815051]: text loss 0.9890 | 18000/ 5122 batches
[2025-05-20 19:13:48.774568]: text loss 1.0055 | 18200/ 5122 batches
[2025-05-20 19:15:29.656373]: text loss 0.9907 | 18400/ 5122 batches
[2025-05-20 19:17:10.463743]: text loss 0.9909 | 18600/ 5122 batches
[2025-05-20 19:18:51.427406]: text loss 0.9953 | 18800/ 5122 batches
[2025-05-20 19:20:32.340639]: text loss 0.9967 | 19000/ 5122 batches
[2025-05-20 19:22:13.153882]: text loss 0.9832 | 19200/ 5122 batches
[2025-05-20 19:23:54.405908]: text loss 1.0022 | 19400/ 5122 batches
[2025-05-20 19:25:35.268930]: text loss 0.9897 | 19600/ 5122 batches
[2025-05-20 19:27:16.263345]: text loss 0.9965 | 19800/ 5122 batches
[2025-05-20 19:28:57.345543]: text loss 0.9920 | 20000/ 5122 batches
[2025-05-20 19:30:38.471351]: text loss 1.0001 | 20200/ 5122 batches
[2025-05-20 19:32:19.824094]: text loss 0.9869 | 20400/ 5122 batches
[2025-05-20 19:33:04.544535]: text loss 0.9861 | 20488/ 5122 batches
[2025-05-20 19:33:04.545633]: validation
[2025-05-20 19:33:06.694886]: explanation loss 2.3820
[2025-05-20 19:33:10.589388]: rationale loss 1.0558
[2025-05-20 19:33:19.769922]: sequential loss 2.2795
[2025-05-20 19:34:19.252854]: top-N loss 1.4076
[2025-05-20 19:34:19.252950]: Endured 2 time(s)
[2025-05-20 19:34:19.252980]: epoch 5
[2025-05-20 19:35:15.785454]: text loss 0.9896 | 20600/ 5122 batches
[2025-05-20 19:36:56.924625]: text loss 0.9682 | 20800/ 5122 batches
[2025-05-20 19:38:37.554831]: text loss 0.9754 | 21000/ 5122 batches
[2025-05-20 19:40:18.518266]: text loss 0.9831 | 21200/ 5122 batches
[2025-05-20 19:41:59.652862]: text loss 0.9819 | 21400/ 5122 batches
[2025-05-20 19:43:40.766833]: text loss 0.9874 | 21600/ 5122 batches
[2025-05-20 19:45:21.540067]: text loss 0.9813 | 21800/ 5122 batches
[2025-05-20 19:47:02.483914]: text loss 0.9804 | 22000/ 5122 batches
[2025-05-20 19:48:43.934223]: text loss 0.9806 | 22200/ 5122 batches
[2025-05-20 19:50:24.670432]: text loss 0.9692 | 22400/ 5122 batches
[2025-05-20 19:52:05.695061]: text loss 0.9862 | 22600/ 5122 batches
[2025-05-20 19:53:46.895168]: text loss 0.9697 | 22800/ 5122 batches
[2025-05-20 19:55:28.060692]: text loss 0.9782 | 23000/ 5122 batches
[2025-05-20 19:57:09.111644]: text loss 0.9819 | 23200/ 5122 batches
[2025-05-20 19:58:49.908615]: text loss 0.9793 | 23400/ 5122 batches
[2025-05-20 20:00:31.047790]: text loss 0.9700 | 23600/ 5122 batches
[2025-05-20 20:02:11.730548]: text loss 0.9779 | 23800/ 5122 batches
[2025-05-20 20:03:52.410428]: text loss 0.9788 | 24000/ 5122 batches
[2025-05-20 20:05:33.661999]: text loss 0.9818 | 24200/ 5122 batches
[2025-05-20 20:07:14.724846]: text loss 0.9743 | 24400/ 5122 batches
[2025-05-20 20:08:55.504481]: text loss 0.9778 | 24600/ 5122 batches
[2025-05-20 20:10:36.430191]: text loss 0.9707 | 24800/ 5122 batches
[2025-05-20 20:12:17.441466]: text loss 0.9743 | 25000/ 5122 batches
[2025-05-20 20:13:58.858367]: text loss 0.9784 | 25200/ 5122 batches
[2025-05-20 20:15:39.942443]: text loss 0.9733 | 25400/ 5122 batches
[2025-05-20 20:17:21.052675]: text loss 0.9865 | 25600/ 5122 batches
[2025-05-20 20:17:26.085853]: text loss 0.9794 | 25610/ 5122 batches
[2025-05-20 20:17:26.086708]: validation
[2025-05-20 20:17:28.243348]: explanation loss 2.2889
[2025-05-20 20:17:32.153705]: rationale loss 1.0499
[2025-05-20 20:17:41.316019]: sequential loss 2.2862
[2025-05-20 20:18:40.664237]: top-N loss 1.3813
[2025-05-20 20:18:40.664340]: Endured 3 time(s)
[2025-05-20 20:18:40.664370]: epoch 6
[2025-05-20 20:20:16.665047]: text loss 0.9580 | 25800/ 5122 batches
[2025-05-20 20:21:57.972536]: text loss 0.9548 | 26000/ 5122 batches
[2025-05-20 20:23:39.196916]: text loss 0.9625 | 26200/ 5122 batches
[2025-05-20 20:25:20.127554]: text loss 0.9647 | 26400/ 5122 batches
[2025-05-20 20:27:00.796767]: text loss 0.9675 | 26600/ 5122 batches
[2025-05-20 20:28:41.724677]: text loss 0.9662 | 26800/ 5122 batches
[2025-05-20 20:30:22.452679]: text loss 0.9624 | 27000/ 5122 batches
[2025-05-20 20:32:03.084997]: text loss 0.9588 | 27200/ 5122 batches
[2025-05-20 20:33:44.026739]: text loss 0.9679 | 27400/ 5122 batches
[2025-05-20 20:35:25.123233]: text loss 0.9741 | 27600/ 5122 batches
[2025-05-20 20:37:06.152574]: text loss 0.9616 | 27800/ 5122 batches
[2025-05-20 20:38:47.082049]: text loss 0.9603 | 28000/ 5122 batches
[2025-05-20 20:40:27.959873]: text loss 0.9719 | 28200/ 5122 batches
[2025-05-20 20:42:09.181556]: text loss 0.9552 | 28400/ 5122 batches
[2025-05-20 20:43:50.368277]: text loss 0.9617 | 28600/ 5122 batches
[2025-05-20 20:45:30.774722]: text loss 0.9595 | 28800/ 5122 batches
[2025-05-20 20:47:11.707804]: text loss 0.9783 | 29000/ 5122 batches
[2025-05-20 20:48:52.953917]: text loss 0.9708 | 29200/ 5122 batches
[2025-05-20 20:50:33.875213]: text loss 0.9622 | 29400/ 5122 batches
[2025-05-20 20:52:14.546673]: text loss 0.9606 | 29600/ 5122 batches
[2025-05-20 20:53:55.637203]: text loss 0.9646 | 29800/ 5122 batches
[2025-05-20 20:55:37.025344]: text loss 0.9615 | 30000/ 5122 batches
[2025-05-20 20:57:18.492251]: text loss 0.9701 | 30200/ 5122 batches
[2025-05-20 20:58:59.833040]: text loss 0.9617 | 30400/ 5122 batches
[2025-05-20 21:00:40.886143]: text loss 0.9641 | 30600/ 5122 batches
[2025-05-20 21:01:47.680973]: text loss 0.9613 | 30732/ 5122 batches
[2025-05-20 21:01:47.681809]: validation
[2025-05-20 21:01:49.930496]: explanation loss 2.1853
[2025-05-20 21:01:53.825594]: rationale loss 1.0383
[2025-05-20 21:02:02.969978]: sequential loss 2.2730
[2025-05-20 21:03:01.558795]: top-N loss 1.3544
[2025-05-20 21:03:02.012019]: epoch 7
[2025-05-20 21:03:36.303105]: text loss 0.9542 | 30800/ 5122 batches
[2025-05-20 21:05:18.008312]: text loss 0.9465 | 31000/ 5122 batches
[2025-05-20 21:06:59.382247]: text loss 0.9454 | 31200/ 5122 batches
[2025-05-20 21:08:40.198030]: text loss 0.9455 | 31400/ 5122 batches
[2025-05-20 21:10:21.262339]: text loss 0.9511 | 31600/ 5122 batches
[2025-05-20 21:12:02.689159]: text loss 0.9487 | 31800/ 5122 batches
[2025-05-20 21:13:44.118263]: text loss 0.9534 | 32000/ 5122 batches
[2025-05-20 21:15:25.545779]: text loss 0.9556 | 32200/ 5122 batches
[2025-05-20 21:17:07.468067]: text loss 0.9449 | 32400/ 5122 batches
[2025-05-20 21:18:49.211893]: text loss 0.9530 | 32600/ 5122 batches
[2025-05-20 21:20:31.434277]: text loss 0.9564 | 32800/ 5122 batches
[2025-05-20 21:22:13.532735]: text loss 0.9558 | 33000/ 5122 batches
[2025-05-20 21:23:54.833467]: text loss 0.9442 | 33200/ 5122 batches
[2025-05-20 21:25:36.556786]: text loss 0.9469 | 33400/ 5122 batches
[2025-05-20 21:27:18.168550]: text loss 0.9456 | 33600/ 5122 batches
[2025-05-20 21:29:00.141208]: text loss 0.9558 | 33800/ 5122 batches
[2025-05-20 21:30:41.575192]: text loss 0.9512 | 34000/ 5122 batches
[2025-05-20 21:32:23.282801]: text loss 0.9574 | 34200/ 5122 batches
[2025-05-20 21:34:04.948484]: text loss 0.9484 | 34400/ 5122 batches
[2025-05-20 21:35:46.149915]: text loss 0.9499 | 34600/ 5122 batches
[2025-05-20 21:37:27.276354]: text loss 0.9578 | 34800/ 5122 batches
[2025-05-20 21:39:09.338266]: text loss 0.9511 | 35000/ 5122 batches
[2025-05-20 21:40:51.460606]: text loss 0.9387 | 35200/ 5122 batches
[2025-05-20 21:42:32.793688]: text loss 0.9545 | 35400/ 5122 batches
[2025-05-20 21:44:14.116790]: text loss 0.9446 | 35600/ 5122 batches
[2025-05-20 21:45:56.015706]: text loss 0.9579 | 35800/ 5122 batches
[2025-05-20 21:46:23.611275]: text loss 0.9678 | 35854/ 5122 batches
[2025-05-20 21:46:23.611928]: validation
[2025-05-20 21:46:25.835281]: explanation loss 2.1253
[2025-05-20 21:46:29.720737]: rationale loss 1.0389
[2025-05-20 21:46:38.767750]: sequential loss 2.2702
[2025-05-20 21:47:38.159288]: top-N loss 1.3351
[2025-05-20 21:47:38.588512]: epoch 8
[2025-05-20 21:48:52.502814]: text loss 0.9495 | 36000/ 5122 batches
[2025-05-20 21:50:34.628724]: text loss 0.9340 | 36200/ 5122 batches
[2025-05-20 21:52:16.193828]: text loss 0.9302 | 36400/ 5122 batches
[2025-05-20 21:53:58.121018]: text loss 0.9266 | 36600/ 5122 batches
[2025-05-20 21:55:39.713277]: text loss 0.9324 | 36800/ 5122 batches
[2025-05-20 21:57:21.985479]: text loss 0.9429 | 37000/ 5122 batches
[2025-05-20 21:59:03.540551]: text loss 0.9312 | 37200/ 5122 batches
[2025-05-20 22:00:44.602748]: text loss 0.9423 | 37400/ 5122 batches
[2025-05-20 22:02:25.961733]: text loss 0.9324 | 37600/ 5122 batches
[2025-05-20 22:04:07.115135]: text loss 0.9346 | 37800/ 5122 batches
[2025-05-20 22:05:48.924409]: text loss 0.9335 | 38000/ 5122 batches
[2025-05-20 22:07:30.182193]: text loss 0.9461 | 38200/ 5122 batches
[2025-05-20 22:09:11.303205]: text loss 0.9383 | 38400/ 5122 batches
[2025-05-20 22:10:53.122599]: text loss 0.9382 | 38600/ 5122 batches
[2025-05-20 22:12:33.742364]: text loss 0.9467 | 38800/ 5122 batches
[2025-05-20 22:14:15.529391]: text loss 0.9417 | 39000/ 5122 batches
[2025-05-20 22:15:57.761372]: text loss 0.9401 | 39200/ 5122 batches
[2025-05-20 22:17:39.681081]: text loss 0.9493 | 39400/ 5122 batches
[2025-05-20 22:19:21.491092]: text loss 0.9405 | 39600/ 5122 batches
[2025-05-20 22:21:02.430337]: text loss 0.9389 | 39800/ 5122 batches
[2025-05-20 22:22:44.456821]: text loss 0.9439 | 40000/ 5122 batches
[2025-05-20 22:24:27.176781]: text loss 0.9408 | 40200/ 5122 batches
[2025-05-20 22:26:09.163518]: text loss 0.9404 | 40400/ 5122 batches
[2025-05-20 22:27:50.842179]: text loss 0.9368 | 40600/ 5122 batches
[2025-05-20 22:29:31.857041]: text loss 0.9427 | 40800/ 5122 batches
[2025-05-20 22:31:00.541346]: text loss 0.9474 | 40976/ 5122 batches
[2025-05-20 22:31:00.541967]: validation
[2025-05-20 22:31:02.728610]: explanation loss 2.1220
[2025-05-20 22:31:06.609962]: rationale loss 1.0392
[2025-05-20 22:31:15.591599]: sequential loss 2.2709
[2025-05-20 22:32:13.767289]: top-N loss 1.3316
[2025-05-20 22:32:14.191274]: epoch 9
[2025-05-20 22:32:26.214944]: text loss 0.9124 | 41000/ 5122 batches
[2025-05-20 22:34:07.643461]: text loss 0.9250 | 41200/ 5122 batches
[2025-05-20 22:35:48.119356]: text loss 0.9267 | 41400/ 5122 batches
[2025-05-20 22:37:29.425122]: text loss 0.9297 | 41600/ 5122 batches
[2025-05-20 22:39:11.677942]: text loss 0.9307 | 41800/ 5122 batches
[2025-05-20 22:40:53.314061]: text loss 0.9321 | 42000/ 5122 batches
[2025-05-20 22:42:34.795164]: text loss 0.9277 | 42200/ 5122 batches
[2025-05-20 22:44:16.021090]: text loss 0.9249 | 42400/ 5122 batches
[2025-05-20 22:45:56.587399]: text loss 0.9239 | 42600/ 5122 batches
[2025-05-20 22:47:37.436224]: text loss 0.9285 | 42800/ 5122 batches
[2025-05-20 22:49:18.684868]: text loss 0.9267 | 43000/ 5122 batches
[2025-05-20 22:50:59.446739]: text loss 0.9352 | 43200/ 5122 batches
[2025-05-20 22:52:41.072165]: text loss 0.9203 | 43400/ 5122 batches
[2025-05-20 22:54:22.258039]: text loss 0.9298 | 43600/ 5122 batches
[2025-05-20 22:56:02.950912]: text loss 0.9284 | 43800/ 5122 batches
[2025-05-20 22:57:44.659634]: text loss 0.9340 | 44000/ 5122 batches
[2025-05-20 22:59:25.528618]: text loss 0.9244 | 44200/ 5122 batches
[2025-05-20 23:01:06.882660]: text loss 0.9166 | 44400/ 5122 batches
[2025-05-20 23:02:48.632984]: text loss 0.9173 | 44600/ 5122 batches
[2025-05-20 23:04:29.934648]: text loss 0.9314 | 44800/ 5122 batches
[2025-05-20 23:06:12.036734]: text loss 0.9328 | 45000/ 5122 batches
[2025-05-20 23:07:54.038685]: text loss 0.9244 | 45200/ 5122 batches
[2025-05-20 23:09:36.089956]: text loss 0.9230 | 45400/ 5122 batches
[2025-05-20 23:11:17.014851]: text loss 0.9320 | 45600/ 5122 batches
[2025-05-20 23:12:58.093495]: text loss 0.9250 | 45800/ 5122 batches
[2025-05-20 23:14:39.229820]: text loss 0.9328 | 46000/ 5122 batches
[2025-05-20 23:15:29.314422]: text loss 0.9315 | 46098/ 5122 batches
[2025-05-20 23:15:29.315261]: validation
[2025-05-20 23:15:31.566857]: explanation loss 2.1247
[2025-05-20 23:15:35.459417]: rationale loss 1.0366
[2025-05-20 23:15:44.670194]: sequential loss 2.2628
[2025-05-20 23:16:42.660396]: top-N loss 1.3144
[2025-05-20 23:16:43.087675]: epoch 10
[2025-05-20 23:17:35.068975]: text loss 0.9108 | 46200/ 5122 batches
[2025-05-20 23:19:15.198740]: text loss 0.9062 | 46400/ 5122 batches
[2025-05-20 23:20:55.694668]: text loss 0.9061 | 46600/ 5122 batches
[2025-05-20 23:22:37.450836]: text loss 0.9114 | 46800/ 5122 batches
[2025-05-20 23:24:18.857186]: text loss 0.9114 | 47000/ 5122 batches
[2025-05-20 23:26:00.565571]: text loss 0.9148 | 47200/ 5122 batches
[2025-05-20 23:27:42.804548]: text loss 0.9117 | 47400/ 5122 batches
[2025-05-20 23:29:24.695478]: text loss 0.9140 | 47600/ 5122 batches
[2025-05-20 23:31:06.618787]: text loss 0.9208 | 47800/ 5122 batches
[2025-05-20 23:32:48.548821]: text loss 0.9150 | 48000/ 5122 batches
[2025-05-20 23:34:30.651478]: text loss 0.9118 | 48200/ 5122 batches
[2025-05-20 23:36:11.639313]: text loss 0.9110 | 48400/ 5122 batches
[2025-05-20 23:37:53.726027]: text loss 0.9107 | 48600/ 5122 batches
[2025-05-20 23:39:34.568935]: text loss 0.9238 | 48800/ 5122 batches
[2025-05-20 23:41:15.683718]: text loss 0.9148 | 49000/ 5122 batches
[2025-05-20 23:42:58.272866]: text loss 0.9194 | 49200/ 5122 batches
[2025-05-20 23:44:39.959736]: text loss 0.9221 | 49400/ 5122 batches
[2025-05-20 23:46:21.589710]: text loss 0.9166 | 49600/ 5122 batches
[2025-05-20 23:48:01.887445]: text loss 0.9188 | 49800/ 5122 batches
[2025-05-20 23:49:42.637999]: text loss 0.9168 | 50000/ 5122 batches
[2025-05-20 23:51:24.314708]: text loss 0.9227 | 50200/ 5122 batches
[2025-05-20 23:53:05.032773]: text loss 0.9213 | 50400/ 5122 batches
[2025-05-20 23:54:46.071213]: text loss 0.9224 | 50600/ 5122 batches
[2025-05-20 23:56:28.205215]: text loss 0.9230 | 50800/ 5122 batches
[2025-05-20 23:58:09.203576]: text loss 0.9175 | 51000/ 5122 batches
[2025-05-20 23:59:51.015032]: text loss 0.9157 | 51200/ 5122 batches
[2025-05-21 00:00:01.121155]: text loss 0.9349 | 51220/ 5122 batches
[2025-05-21 00:00:01.122014]: validation
[2025-05-21 00:00:03.322247]: explanation loss 2.1230
[2025-05-21 00:00:07.216232]: rationale loss 1.0332
[2025-05-21 00:00:16.444452]: sequential loss 2.2783
[2025-05-21 00:01:15.079778]: top-N loss 1.3105
[2025-05-21 00:01:15.079867]: Endured 4 time(s)
[2025-05-21 00:01:15.079890]: epoch 11
[2025-05-21 00:02:46.265290]: text loss 0.8913 | 51400/ 5122 batches
[2025-05-21 00:04:27.269699]: text loss 0.8939 | 51600/ 5122 batches
[2025-05-21 00:06:08.324045]: text loss 0.8967 | 51800/ 5122 batches
[2025-05-21 00:07:50.048593]: text loss 0.8884 | 52000/ 5122 batches
[2025-05-21 00:09:32.307421]: text loss 0.9027 | 52200/ 5122 batches
[2025-05-21 00:11:13.526163]: text loss 0.8936 | 52400/ 5122 batches
[2025-05-21 00:12:55.660470]: text loss 0.9107 | 52600/ 5122 batches
[2025-05-21 00:14:37.756774]: text loss 0.9007 | 52800/ 5122 batches
[2025-05-21 00:16:19.994725]: text loss 0.9116 | 53000/ 5122 batches
[2025-05-21 00:18:01.268915]: text loss 0.9016 | 53200/ 5122 batches
[2025-05-21 00:19:41.571324]: text loss 0.8988 | 53400/ 5122 batches
[2025-05-21 00:21:22.096349]: text loss 0.9142 | 53600/ 5122 batches
[2025-05-21 00:23:04.092262]: text loss 0.9166 | 53800/ 5122 batches
[2025-05-21 00:24:45.155909]: text loss 0.9057 | 54000/ 5122 batches
[2025-05-21 00:26:25.613140]: text loss 0.9094 | 54200/ 5122 batches
[2025-05-21 00:28:08.020619]: text loss 0.9010 | 54400/ 5122 batches
[2025-05-21 00:29:50.194666]: text loss 0.9043 | 54600/ 5122 batches
[2025-05-21 00:31:32.323117]: text loss 0.9089 | 54800/ 5122 batches
[2025-05-21 00:33:13.144326]: text loss 0.9071 | 55000/ 5122 batches
[2025-05-21 00:34:53.708007]: text loss 0.9187 | 55200/ 5122 batches
[2025-05-21 00:36:34.366251]: text loss 0.9079 | 55400/ 5122 batches
[2025-05-21 00:38:16.472001]: text loss 0.9095 | 55600/ 5122 batches
[2025-05-21 00:39:58.226831]: text loss 0.9145 | 55800/ 5122 batches
[2025-05-21 00:41:39.659098]: text loss 0.8970 | 56000/ 5122 batches
[2025-05-21 00:43:22.052304]: text loss 0.9121 | 56200/ 5122 batches
[2025-05-21 00:44:34.839236]: text loss 0.9114 | 56342/ 5122 batches
[2025-05-21 00:44:34.839869]: validation
[2025-05-21 00:44:36.998463]: explanation loss 2.1247
[2025-05-21 00:44:40.888109]: rationale loss 1.0329
[2025-05-21 00:44:49.880608]: sequential loss 2.2860
[2025-05-21 00:45:47.651553]: top-N loss 1.3035
[2025-05-21 00:45:47.651638]: Endured 5 time(s)
[2025-05-21 00:45:47.651660]: Cannot endure it anymore | Exiting from early stop
