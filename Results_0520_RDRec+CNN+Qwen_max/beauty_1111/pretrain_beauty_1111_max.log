nohup: ignoring input
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/home/zhongyikun/anaconda3/envs/RDRec/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of Solomon were not initialized from the model checkpoint at t5-small and are newly initialized: ['conv1d.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------------------------------ARGUMENTS----------------------------------------
data_dir                                 ./data/beauty/
model_version                            0
task_num                                 5
prompt_num                               3
lr                                       0.0005
ratio                                    1:1:1:1
epochs                                   100
batch_size                               64
cuda                                     True
log_interval                             200
checkpoint                               ./checkpoint_0520_max/beauty_1111/
endure_times                             5
exp_len                                  20
negative_num                             99
----------------------------------------ARGUMENTS----------------------------------------
[2025-05-20 01:22:21.728603]: Loading data
[2025-05-20 01:22:34.389523]: Start training
[2025-05-20 01:22:34.389606]: epoch 1
[2025-05-20 01:24:16.668948]: text loss 1.5486 |   200/ 6642 batches
[2025-05-20 01:25:58.480028]: text loss 1.2858 |   400/ 6642 batches
[2025-05-20 01:27:39.538289]: text loss 1.2250 |   600/ 6642 batches
[2025-05-20 01:29:21.096585]: text loss 1.2384 |   800/ 6642 batches
[2025-05-20 01:31:02.725662]: text loss 1.2010 |  1000/ 6642 batches
[2025-05-20 01:32:44.984544]: text loss 1.2127 |  1200/ 6642 batches
[2025-05-20 01:34:25.914884]: text loss 1.1773 |  1400/ 6642 batches
[2025-05-20 01:36:08.197173]: text loss 1.1761 |  1600/ 6642 batches
[2025-05-20 01:37:49.945275]: text loss 1.1755 |  1800/ 6642 batches
[2025-05-20 01:39:32.179042]: text loss 1.1282 |  2000/ 6642 batches
[2025-05-20 01:41:13.351984]: text loss 1.1611 |  2200/ 6642 batches
[2025-05-20 01:42:55.342761]: text loss 1.1348 |  2400/ 6642 batches
[2025-05-20 01:44:37.283428]: text loss 1.1585 |  2600/ 6642 batches
[2025-05-20 01:46:19.408644]: text loss 1.1515 |  2800/ 6642 batches
[2025-05-20 01:48:01.310665]: text loss 1.1306 |  3000/ 6642 batches
[2025-05-20 01:49:43.384677]: text loss 1.1280 |  3200/ 6642 batches
[2025-05-20 01:51:25.692196]: text loss 1.1484 |  3400/ 6642 batches
[2025-05-20 01:53:06.923080]: text loss 1.1295 |  3600/ 6642 batches
[2025-05-20 01:54:48.590743]: text loss 1.1377 |  3800/ 6642 batches
[2025-05-20 01:56:30.200235]: text loss 1.1330 |  4000/ 6642 batches
[2025-05-20 01:58:12.652338]: text loss 1.1175 |  4200/ 6642 batches
[2025-05-20 01:59:54.929502]: text loss 1.1147 |  4400/ 6642 batches
[2025-05-20 02:01:37.058256]: text loss 1.1477 |  4600/ 6642 batches
[2025-05-20 02:03:19.214654]: text loss 1.1131 |  4800/ 6642 batches
[2025-05-20 02:05:01.396381]: text loss 1.1034 |  5000/ 6642 batches
[2025-05-20 02:06:43.417886]: text loss 1.1180 |  5200/ 6642 batches
[2025-05-20 02:08:25.566495]: text loss 1.1087 |  5400/ 6642 batches
[2025-05-20 02:10:06.571181]: text loss 1.0890 |  5600/ 6642 batches
[2025-05-20 02:11:48.866506]: text loss 1.1256 |  5800/ 6642 batches
[2025-05-20 02:13:30.917671]: text loss 1.1523 |  6000/ 6642 batches
[2025-05-20 02:15:13.008419]: text loss 1.0926 |  6200/ 6642 batches
[2025-05-20 02:16:55.029654]: text loss 1.0949 |  6400/ 6642 batches
[2025-05-20 02:18:37.127901]: text loss 1.0697 |  6600/ 6642 batches
[2025-05-20 02:18:58.672504]: text loss 0.9942 |  6642/ 6642 batches
[2025-05-20 02:18:58.673336]: validation
[2025-05-20 02:19:01.653055]: explanation loss 2.4065
[2025-05-20 02:19:06.816017]: rationale loss 1.1851
[2025-05-20 02:19:17.088854]: sequential loss 2.4531
[2025-05-20 02:20:24.521721]: top-N loss 1.4860
[2025-05-20 02:20:24.985546]: epoch 2
[2025-05-20 02:21:45.897979]: text loss 1.0787 |  6800/ 6642 batches
[2025-05-20 02:23:28.089477]: text loss 1.0908 |  7000/ 6642 batches
[2025-05-20 02:25:10.161034]: text loss 1.0841 |  7200/ 6642 batches
[2025-05-20 02:26:52.475571]: text loss 1.0869 |  7400/ 6642 batches
[2025-05-20 02:28:34.472109]: text loss 1.0774 |  7600/ 6642 batches
[2025-05-20 02:30:16.063779]: text loss 1.0683 |  7800/ 6642 batches
[2025-05-20 02:31:57.934808]: text loss 1.0736 |  8000/ 6642 batches
[2025-05-20 02:33:39.459965]: text loss 1.0731 |  8200/ 6642 batches
[2025-05-20 02:35:21.723014]: text loss 1.0577 |  8400/ 6642 batches
[2025-05-20 02:37:04.038992]: text loss 1.0718 |  8600/ 6642 batches
[2025-05-20 02:38:45.703756]: text loss 1.0670 |  8800/ 6642 batches
[2025-05-20 02:40:27.954568]: text loss 1.0692 |  9000/ 6642 batches
[2025-05-20 02:42:10.339396]: text loss 1.0742 |  9200/ 6642 batches
[2025-05-20 02:43:52.223937]: text loss 1.0688 |  9400/ 6642 batches
[2025-05-20 02:45:34.494467]: text loss 1.0545 |  9600/ 6642 batches
[2025-05-20 02:47:16.890506]: text loss 1.0632 |  9800/ 6642 batches
[2025-05-20 02:48:59.127361]: text loss 1.0649 | 10000/ 6642 batches
[2025-05-20 02:50:40.822522]: text loss 1.0712 | 10200/ 6642 batches
[2025-05-20 02:52:22.746310]: text loss 1.0782 | 10400/ 6642 batches
[2025-05-20 02:54:04.553033]: text loss 1.0663 | 10600/ 6642 batches
[2025-05-20 02:55:46.509003]: text loss 1.0569 | 10800/ 6642 batches
[2025-05-20 02:57:27.693752]: text loss 1.0649 | 11000/ 6642 batches
[2025-05-20 02:59:09.424140]: text loss 1.0656 | 11200/ 6642 batches
[2025-05-20 03:00:51.587292]: text loss 1.0665 | 11400/ 6642 batches
[2025-05-20 03:02:33.670156]: text loss 1.0486 | 11600/ 6642 batches
[2025-05-20 03:04:15.581293]: text loss 1.0608 | 11800/ 6642 batches
[2025-05-20 03:05:58.129117]: text loss 1.0540 | 12000/ 6642 batches
[2025-05-20 03:07:40.362279]: text loss 1.0515 | 12200/ 6642 batches
[2025-05-20 03:09:22.929312]: text loss 1.0609 | 12400/ 6642 batches
[2025-05-20 03:11:04.689568]: text loss 1.0426 | 12600/ 6642 batches
[2025-05-20 03:12:46.812631]: text loss 1.0560 | 12800/ 6642 batches
[2025-05-20 03:14:29.296805]: text loss 1.0632 | 13000/ 6642 batches
[2025-05-20 03:16:11.499566]: text loss 1.0552 | 13200/ 6642 batches
[2025-05-20 03:16:54.240352]: text loss 1.0476 | 13284/ 6642 batches
[2025-05-20 03:16:54.241116]: validation
[2025-05-20 03:16:57.076241]: explanation loss 2.2034
[2025-05-20 03:17:02.207889]: rationale loss 1.1070
[2025-05-20 03:17:12.466427]: sequential loss 2.3630
[2025-05-20 03:18:20.770304]: top-N loss 1.4234
[2025-05-20 03:18:21.282767]: epoch 3
[2025-05-20 03:19:20.756330]: text loss 1.0179 | 13400/ 6642 batches
[2025-05-20 03:21:02.996471]: text loss 1.0423 | 13600/ 6642 batches
[2025-05-20 03:22:45.263773]: text loss 1.0324 | 13800/ 6642 batches
[2025-05-20 03:24:27.420978]: text loss 1.0344 | 14000/ 6642 batches
[2025-05-20 03:26:09.759851]: text loss 1.0348 | 14200/ 6642 batches
[2025-05-20 03:27:52.261547]: text loss 1.0483 | 14400/ 6642 batches
[2025-05-20 03:29:34.309480]: text loss 1.0428 | 14600/ 6642 batches
[2025-05-20 03:31:16.830258]: text loss 1.0356 | 14800/ 6642 batches
[2025-05-20 03:32:58.188090]: text loss 1.0495 | 15000/ 6642 batches
[2025-05-20 03:34:40.511660]: text loss 1.0378 | 15200/ 6642 batches
[2025-05-20 03:36:22.590584]: text loss 1.0281 | 15400/ 6642 batches
[2025-05-20 03:38:04.809220]: text loss 1.0400 | 15600/ 6642 batches
[2025-05-20 03:39:47.101728]: text loss 1.0269 | 15800/ 6642 batches
[2025-05-20 03:41:29.040873]: text loss 1.0311 | 16000/ 6642 batches
[2025-05-20 03:43:11.057187]: text loss 1.0328 | 16200/ 6642 batches
[2025-05-20 03:44:52.838550]: text loss 1.0329 | 16400/ 6642 batches
[2025-05-20 03:46:34.652653]: text loss 1.0209 | 16600/ 6642 batches
[2025-05-20 03:48:17.279677]: text loss 1.0309 | 16800/ 6642 batches
[2025-05-20 03:49:59.315562]: text loss 1.0394 | 17000/ 6642 batches
[2025-05-20 03:51:41.156940]: text loss 1.0346 | 17200/ 6642 batches
[2025-05-20 03:53:23.356724]: text loss 1.0250 | 17400/ 6642 batches
[2025-05-20 03:55:05.701696]: text loss 1.0244 | 17600/ 6642 batches
[2025-05-20 03:56:47.734706]: text loss 1.0341 | 17800/ 6642 batches
[2025-05-20 03:58:29.791724]: text loss 1.0313 | 18000/ 6642 batches
[2025-05-20 04:00:11.939864]: text loss 1.0269 | 18200/ 6642 batches
[2025-05-20 04:01:54.424890]: text loss 1.0231 | 18400/ 6642 batches
[2025-05-20 04:03:36.546474]: text loss 1.0336 | 18600/ 6642 batches
[2025-05-20 04:05:18.762696]: text loss 1.0321 | 18800/ 6642 batches
[2025-05-20 04:07:00.817550]: text loss 1.0304 | 19000/ 6642 batches
[2025-05-20 04:08:42.836644]: text loss 1.0259 | 19200/ 6642 batches
[2025-05-20 04:10:25.025493]: text loss 1.0310 | 19400/ 6642 batches
[2025-05-20 04:12:07.709218]: text loss 1.0334 | 19600/ 6642 batches
[2025-05-20 04:13:49.439389]: text loss 1.0264 | 19800/ 6642 batches
[2025-05-20 04:14:53.402800]: text loss 1.0269 | 19926/ 6642 batches
[2025-05-20 04:14:53.403607]: validation
[2025-05-20 04:14:56.366459]: explanation loss 2.1878
[2025-05-20 04:15:01.525615]: rationale loss 1.0883
[2025-05-20 04:15:12.114389]: sequential loss 2.3467
[2025-05-20 04:16:20.590066]: top-N loss 1.3627
[2025-05-20 04:16:21.058324]: epoch 4
[2025-05-20 04:16:58.739955]: text loss 1.0119 | 20000/ 6642 batches
[2025-05-20 04:18:40.152171]: text loss 1.0053 | 20200/ 6642 batches
[2025-05-20 04:20:21.950581]: text loss 1.0006 | 20400/ 6642 batches
[2025-05-20 04:22:03.367372]: text loss 1.0159 | 20600/ 6642 batches
[2025-05-20 04:23:45.349008]: text loss 1.0213 | 20800/ 6642 batches
[2025-05-20 04:25:27.374913]: text loss 1.0042 | 21000/ 6642 batches
[2025-05-20 04:27:08.952808]: text loss 1.0010 | 21200/ 6642 batches
[2025-05-20 04:28:50.899006]: text loss 1.0162 | 21400/ 6642 batches
[2025-05-20 04:30:32.809806]: text loss 1.0142 | 21600/ 6642 batches
[2025-05-20 04:32:15.245583]: text loss 1.0138 | 21800/ 6642 batches
[2025-05-20 04:33:58.060530]: text loss 1.0106 | 22000/ 6642 batches
[2025-05-20 04:35:40.576640]: text loss 1.0140 | 22200/ 6642 batches
[2025-05-20 04:37:23.096223]: text loss 1.0111 | 22400/ 6642 batches
[2025-05-20 04:39:05.692224]: text loss 1.0030 | 22600/ 6642 batches
[2025-05-20 04:40:48.084161]: text loss 1.0201 | 22800/ 6642 batches
[2025-05-20 04:42:30.366518]: text loss 1.0148 | 23000/ 6642 batches
[2025-05-20 04:44:12.386016]: text loss 1.0018 | 23200/ 6642 batches
[2025-05-20 04:45:53.839456]: text loss 1.0067 | 23400/ 6642 batches
[2025-05-20 04:47:35.971867]: text loss 1.0123 | 23600/ 6642 batches
[2025-05-20 04:49:17.850671]: text loss 1.0073 | 23800/ 6642 batches
[2025-05-20 04:50:59.667614]: text loss 1.0091 | 24000/ 6642 batches
[2025-05-20 04:52:42.101888]: text loss 1.0093 | 24200/ 6642 batches
[2025-05-20 04:54:24.613284]: text loss 1.0140 | 24400/ 6642 batches
[2025-05-20 04:56:06.995921]: text loss 1.0083 | 24600/ 6642 batches
[2025-05-20 04:57:48.691954]: text loss 1.0050 | 24800/ 6642 batches
[2025-05-20 04:59:31.085605]: text loss 1.0028 | 25000/ 6642 batches
[2025-05-20 05:01:13.295183]: text loss 1.0068 | 25200/ 6642 batches
[2025-05-20 05:02:55.175537]: text loss 1.0023 | 25400/ 6642 batches
[2025-05-20 05:04:37.534489]: text loss 1.0071 | 25600/ 6642 batches
[2025-05-20 05:06:19.778633]: text loss 1.0107 | 25800/ 6642 batches
[2025-05-20 05:08:01.449900]: text loss 0.9982 | 26000/ 6642 batches
[2025-05-20 05:09:43.726283]: text loss 1.0148 | 26200/ 6642 batches
[2025-05-20 05:11:25.434993]: text loss 1.0083 | 26400/ 6642 batches
[2025-05-20 05:12:50.503953]: text loss 1.0013 | 26568/ 6642 batches
[2025-05-20 05:12:50.504838]: validation
[2025-05-20 05:12:53.329901]: explanation loss 2.1646
[2025-05-20 05:12:58.469874]: rationale loss 1.0763
[2025-05-20 05:13:08.756078]: sequential loss 2.3130
[2025-05-20 05:14:15.933237]: top-N loss 1.3487
[2025-05-20 05:14:16.416489]: epoch 5
[2025-05-20 05:14:32.728815]: text loss 1.0129 | 26600/ 6642 batches
[2025-05-20 05:16:14.411126]: text loss 0.9742 | 26800/ 6642 batches
[2025-05-20 05:17:56.566980]: text loss 0.9851 | 27000/ 6642 batches
[2025-05-20 05:19:38.728191]: text loss 0.9917 | 27200/ 6642 batches
[2025-05-20 05:21:21.048199]: text loss 0.9884 | 27400/ 6642 batches
[2025-05-20 05:23:03.381614]: text loss 0.9912 | 27600/ 6642 batches
[2025-05-20 05:24:45.314580]: text loss 0.9934 | 27800/ 6642 batches
[2025-05-20 05:26:26.936875]: text loss 0.9859 | 28000/ 6642 batches
[2025-05-20 05:28:08.406028]: text loss 0.9854 | 28200/ 6642 batches
[2025-05-20 05:29:50.677951]: text loss 0.9880 | 28400/ 6642 batches
[2025-05-20 05:31:32.226112]: text loss 1.0021 | 28600/ 6642 batches
[2025-05-20 05:33:14.425363]: text loss 0.9940 | 28800/ 6642 batches
[2025-05-20 05:34:56.491012]: text loss 0.9867 | 29000/ 6642 batches
[2025-05-20 05:36:38.871980]: text loss 0.9836 | 29200/ 6642 batches
[2025-05-20 05:38:20.627505]: text loss 0.9927 | 29400/ 6642 batches
[2025-05-20 05:40:02.895589]: text loss 0.9872 | 29600/ 6642 batches
[2025-05-20 05:41:44.539040]: text loss 0.9943 | 29800/ 6642 batches
[2025-05-20 05:43:26.413475]: text loss 0.9946 | 30000/ 6642 batches
[2025-05-20 05:45:08.627998]: text loss 0.9890 | 30200/ 6642 batches
[2025-05-20 05:46:51.191772]: text loss 0.9892 | 30400/ 6642 batches
[2025-05-20 05:48:33.135184]: text loss 0.9892 | 30600/ 6642 batches
[2025-05-20 05:50:15.395828]: text loss 0.9879 | 30800/ 6642 batches
[2025-05-20 05:51:57.555122]: text loss 1.0013 | 31000/ 6642 batches
[2025-05-20 05:53:39.905222]: text loss 0.9908 | 31200/ 6642 batches
[2025-05-20 05:55:21.758133]: text loss 0.9922 | 31400/ 6642 batches
[2025-05-20 05:57:03.848011]: text loss 0.9886 | 31600/ 6642 batches
[2025-05-20 05:58:46.034094]: text loss 0.9835 | 31800/ 6642 batches
[2025-05-20 06:00:28.058577]: text loss 0.9820 | 32000/ 6642 batches
[2025-05-20 06:02:09.792119]: text loss 0.9946 | 32200/ 6642 batches
[2025-05-20 06:03:50.831779]: text loss 0.9982 | 32400/ 6642 batches
[2025-05-20 06:05:32.459714]: text loss 0.9838 | 32600/ 6642 batches
[2025-05-20 06:07:14.290119]: text loss 0.9916 | 32800/ 6642 batches
[2025-05-20 06:08:56.464254]: text loss 0.9778 | 33000/ 6642 batches
[2025-05-20 06:10:38.523836]: text loss 0.9750 | 33200/ 6642 batches
[2025-05-20 06:10:43.620277]: text loss 0.9859 | 33210/ 6642 batches
[2025-05-20 06:10:43.621371]: validation
[2025-05-20 06:10:46.522600]: explanation loss 2.1509
[2025-05-20 06:10:51.657984]: rationale loss 1.0674
[2025-05-20 06:11:01.878183]: sequential loss 2.3039
[2025-05-20 06:12:10.382867]: top-N loss 1.3207
[2025-05-20 06:12:10.864161]: epoch 6
[2025-05-20 06:13:47.855304]: text loss 0.9656 | 33400/ 6642 batches
[2025-05-20 06:15:29.838463]: text loss 0.9635 | 33600/ 6642 batches
[2025-05-20 06:17:11.999918]: text loss 0.9700 | 33800/ 6642 batches
[2025-05-20 06:18:53.731443]: text loss 0.9681 | 34000/ 6642 batches
[2025-05-20 06:20:35.945371]: text loss 0.9783 | 34200/ 6642 batches
[2025-05-20 06:22:17.810072]: text loss 0.9651 | 34400/ 6642 batches
[2025-05-20 06:23:59.775532]: text loss 0.9750 | 34600/ 6642 batches
[2025-05-20 06:25:41.765553]: text loss 0.9715 | 34800/ 6642 batches
[2025-05-20 06:27:23.962622]: text loss 0.9739 | 35000/ 6642 batches
[2025-05-20 06:29:06.363193]: text loss 0.9775 | 35200/ 6642 batches
[2025-05-20 06:30:47.365722]: text loss 0.9768 | 35400/ 6642 batches
[2025-05-20 06:32:29.232981]: text loss 0.9756 | 35600/ 6642 batches
[2025-05-20 06:34:11.310572]: text loss 0.9700 | 35800/ 6642 batches
[2025-05-20 06:35:53.291038]: text loss 0.9718 | 36000/ 6642 batches
[2025-05-20 06:37:35.407067]: text loss 0.9685 | 36200/ 6642 batches
[2025-05-20 06:39:17.327989]: text loss 0.9736 | 36400/ 6642 batches
[2025-05-20 06:40:58.822995]: text loss 0.9704 | 36600/ 6642 batches
[2025-05-20 06:42:40.518686]: text loss 0.9697 | 36800/ 6642 batches
[2025-05-20 06:44:22.484678]: text loss 0.9738 | 37000/ 6642 batches
[2025-05-20 06:46:04.143054]: text loss 0.9659 | 37200/ 6642 batches
[2025-05-20 06:47:45.939749]: text loss 0.9690 | 37400/ 6642 batches
[2025-05-20 06:49:28.446306]: text loss 0.9694 | 37600/ 6642 batches
[2025-05-20 06:51:10.434140]: text loss 0.9738 | 37800/ 6642 batches
[2025-05-20 06:52:52.352001]: text loss 0.9725 | 38000/ 6642 batches
[2025-05-20 06:54:34.231510]: text loss 0.9774 | 38200/ 6642 batches
[2025-05-20 06:56:15.815909]: text loss 0.9695 | 38400/ 6642 batches
[2025-05-20 06:57:57.944505]: text loss 0.9757 | 38600/ 6642 batches
[2025-05-20 06:59:39.798106]: text loss 0.9715 | 38800/ 6642 batches
[2025-05-20 07:01:21.978654]: text loss 0.9793 | 39000/ 6642 batches
[2025-05-20 07:03:03.721519]: text loss 0.9776 | 39200/ 6642 batches
[2025-05-20 07:04:45.624843]: text loss 0.9632 | 39400/ 6642 batches
[2025-05-20 07:06:27.861584]: text loss 0.9633 | 39600/ 6642 batches
[2025-05-20 07:08:09.997128]: text loss 0.9758 | 39800/ 6642 batches
[2025-05-20 07:08:36.562200]: text loss 0.9599 | 39852/ 6642 batches
[2025-05-20 07:08:36.563023]: validation
[2025-05-20 07:08:39.484564]: explanation loss 2.1376
[2025-05-20 07:08:44.616349]: rationale loss 1.0622
[2025-05-20 07:08:54.839200]: sequential loss 2.2478
[2025-05-20 07:10:02.926928]: top-N loss 1.3008
[2025-05-20 07:10:03.391222]: epoch 7
[2025-05-20 07:11:19.030155]: text loss 0.9540 | 40000/ 6642 batches
[2025-05-20 07:13:01.256956]: text loss 0.9510 | 40200/ 6642 batches
[2025-05-20 07:14:43.505889]: text loss 0.9504 | 40400/ 6642 batches
[2025-05-20 07:16:25.594428]: text loss 0.9527 | 40600/ 6642 batches
[2025-05-20 07:18:07.664779]: text loss 0.9502 | 40800/ 6642 batches
[2025-05-20 07:19:50.352959]: text loss 0.9485 | 41000/ 6642 batches
[2025-05-20 07:21:32.318250]: text loss 0.9600 | 41200/ 6642 batches
[2025-05-20 07:23:14.387411]: text loss 0.9622 | 41400/ 6642 batches
[2025-05-20 07:24:56.370922]: text loss 0.9679 | 41600/ 6642 batches
[2025-05-20 07:26:38.332854]: text loss 0.9591 | 41800/ 6642 batches
[2025-05-20 07:28:20.590505]: text loss 0.9564 | 42000/ 6642 batches
[2025-05-20 07:30:01.901394]: text loss 0.9650 | 42200/ 6642 batches
[2025-05-20 07:31:44.031164]: text loss 0.9480 | 42400/ 6642 batches
[2025-05-20 07:33:26.479382]: text loss 0.9548 | 42600/ 6642 batches
[2025-05-20 07:35:08.694832]: text loss 0.9594 | 42800/ 6642 batches
[2025-05-20 07:36:51.069689]: text loss 0.9549 | 43000/ 6642 batches
[2025-05-20 07:38:33.538071]: text loss 0.9632 | 43200/ 6642 batches
[2025-05-20 07:40:15.863274]: text loss 0.9582 | 43400/ 6642 batches
[2025-05-20 07:41:56.950088]: text loss 0.9539 | 43600/ 6642 batches
[2025-05-20 07:43:39.225641]: text loss 0.9468 | 43800/ 6642 batches
[2025-05-20 07:45:21.365867]: text loss 0.9517 | 44000/ 6642 batches
[2025-05-20 07:47:03.635702]: text loss 0.9507 | 44200/ 6642 batches
[2025-05-20 07:48:45.884093]: text loss 0.9447 | 44400/ 6642 batches
[2025-05-20 07:50:28.229353]: text loss 0.9504 | 44600/ 6642 batches
[2025-05-20 07:52:10.741870]: text loss 0.9604 | 44800/ 6642 batches
[2025-05-20 07:53:52.744366]: text loss 0.9643 | 45000/ 6642 batches
[2025-05-20 07:55:34.892309]: text loss 0.9564 | 45200/ 6642 batches
[2025-05-20 07:57:16.981582]: text loss 0.9659 | 45400/ 6642 batches
[2025-05-20 07:58:59.236560]: text loss 0.9507 | 45600/ 6642 batches
[2025-05-20 08:00:41.585913]: text loss 0.9565 | 45800/ 6642 batches
[2025-05-20 08:02:23.568895]: text loss 0.9623 | 46000/ 6642 batches
[2025-05-20 08:04:05.727054]: text loss 0.9532 | 46200/ 6642 batches
[2025-05-20 08:05:47.527915]: text loss 0.9544 | 46400/ 6642 batches
[2025-05-20 08:06:35.263026]: text loss 0.9502 | 46494/ 6642 batches
[2025-05-20 08:06:35.263822]: validation
[2025-05-20 08:06:38.213253]: explanation loss 2.1254
[2025-05-20 08:06:43.370742]: rationale loss 1.0580
[2025-05-20 08:06:53.591242]: sequential loss 2.2366
[2025-05-20 08:08:01.381456]: top-N loss 1.2769
[2025-05-20 08:08:01.868384]: epoch 8
[2025-05-20 08:08:55.822030]: text loss 0.9329 | 46600/ 6642 batches
[2025-05-20 08:10:37.354423]: text loss 0.9309 | 46800/ 6642 batches
[2025-05-20 08:12:19.613041]: text loss 0.9330 | 47000/ 6642 batches
[2025-05-20 08:14:02.135783]: text loss 0.9450 | 47200/ 6642 batches
[2025-05-20 08:15:44.363670]: text loss 0.9351 | 47400/ 6642 batches
[2025-05-20 08:17:26.823155]: text loss 0.9358 | 47600/ 6642 batches
[2025-05-20 08:19:08.846421]: text loss 0.9434 | 47800/ 6642 batches
[2025-05-20 08:20:51.024782]: text loss 0.9363 | 48000/ 6642 batches
[2025-05-20 08:22:33.042055]: text loss 0.9490 | 48200/ 6642 batches
[2025-05-20 08:24:14.486405]: text loss 0.9369 | 48400/ 6642 batches
[2025-05-20 08:25:56.526705]: text loss 0.9509 | 48600/ 6642 batches
[2025-05-20 08:27:38.494342]: text loss 0.9436 | 48800/ 6642 batches
[2025-05-20 08:29:20.435096]: text loss 0.9370 | 49000/ 6642 batches
[2025-05-20 08:31:02.334882]: text loss 0.9346 | 49200/ 6642 batches
[2025-05-20 08:32:44.074774]: text loss 0.9449 | 49400/ 6642 batches
[2025-05-20 08:34:26.294056]: text loss 0.9476 | 49600/ 6642 batches
[2025-05-20 08:36:08.141316]: text loss 0.9452 | 49800/ 6642 batches
[2025-05-20 08:37:49.219226]: text loss 0.9381 | 50000/ 6642 batches
[2025-05-20 08:39:30.716290]: text loss 0.9444 | 50200/ 6642 batches
[2025-05-20 08:41:11.913485]: text loss 0.9429 | 50400/ 6642 batches
[2025-05-20 08:42:53.894872]: text loss 0.9386 | 50600/ 6642 batches
[2025-05-20 08:44:36.040926]: text loss 0.9433 | 50800/ 6642 batches
[2025-05-20 08:46:17.846226]: text loss 0.9448 | 51000/ 6642 batches
[2025-05-20 08:47:59.265600]: text loss 0.9364 | 51200/ 6642 batches
[2025-05-20 08:49:40.646018]: text loss 0.9475 | 51400/ 6642 batches
[2025-05-20 08:51:22.438829]: text loss 0.9344 | 51600/ 6642 batches
[2025-05-20 08:53:04.034810]: text loss 0.9420 | 51800/ 6642 batches
[2025-05-20 08:54:45.992644]: text loss 0.9330 | 52000/ 6642 batches
[2025-05-20 08:56:27.804262]: text loss 0.9421 | 52200/ 6642 batches
[2025-05-20 08:58:09.848603]: text loss 0.9370 | 52400/ 6642 batches
[2025-05-20 08:59:52.004501]: text loss 0.9493 | 52600/ 6642 batches
[2025-05-20 09:01:34.226769]: text loss 0.9397 | 52800/ 6642 batches
[2025-05-20 09:03:16.328257]: text loss 0.9413 | 53000/ 6642 batches
[2025-05-20 09:04:25.215750]: text loss 0.9432 | 53136/ 6642 batches
[2025-05-20 09:04:25.216488]: validation
[2025-05-20 09:04:28.039129]: explanation loss 2.1266
[2025-05-20 09:04:33.179700]: rationale loss 1.0560
[2025-05-20 09:04:43.429910]: sequential loss 2.2250
[2025-05-20 09:05:52.100186]: top-N loss 1.2703
[2025-05-20 09:05:52.566892]: epoch 9
[2025-05-20 09:06:25.294561]: text loss 0.9130 | 53200/ 6642 batches
[2025-05-20 09:08:07.346000]: text loss 0.9205 | 53400/ 6642 batches
[2025-05-20 09:09:49.564340]: text loss 0.9253 | 53600/ 6642 batches
[2025-05-20 09:11:31.299357]: text loss 0.9291 | 53800/ 6642 batches
[2025-05-20 09:13:13.254023]: text loss 0.9250 | 54000/ 6642 batches
[2025-05-20 09:14:54.994165]: text loss 0.9279 | 54200/ 6642 batches
[2025-05-20 09:16:36.630466]: text loss 0.9237 | 54400/ 6642 batches
[2025-05-20 09:18:18.617545]: text loss 0.9250 | 54600/ 6642 batches
[2025-05-20 09:20:00.640963]: text loss 0.9254 | 54800/ 6642 batches
[2025-05-20 09:21:42.674153]: text loss 0.9353 | 55000/ 6642 batches
[2025-05-20 09:23:24.377752]: text loss 0.9183 | 55200/ 6642 batches
[2025-05-20 09:25:06.299368]: text loss 0.9190 | 55400/ 6642 batches
[2025-05-20 09:26:48.156706]: text loss 0.9296 | 55600/ 6642 batches
[2025-05-20 09:28:30.169825]: text loss 0.9239 | 55800/ 6642 batches
[2025-05-20 09:30:11.797619]: text loss 0.9311 | 56000/ 6642 batches
[2025-05-20 09:31:54.035812]: text loss 0.9260 | 56200/ 6642 batches
[2025-05-20 09:33:35.943602]: text loss 0.9286 | 56400/ 6642 batches
[2025-05-20 09:35:18.094440]: text loss 0.9328 | 56600/ 6642 batches
[2025-05-20 09:37:00.046574]: text loss 0.9214 | 56800/ 6642 batches
[2025-05-20 09:38:42.299005]: text loss 0.9299 | 57000/ 6642 batches
[2025-05-20 09:40:24.385665]: text loss 0.9360 | 57200/ 6642 batches
[2025-05-20 09:42:06.147902]: text loss 0.9202 | 57400/ 6642 batches
[2025-05-20 09:43:48.299677]: text loss 0.9235 | 57600/ 6642 batches
[2025-05-20 09:45:30.430964]: text loss 0.9269 | 57800/ 6642 batches
[2025-05-20 09:47:12.359291]: text loss 0.9272 | 58000/ 6642 batches
[2025-05-20 09:48:54.041184]: text loss 0.9261 | 58200/ 6642 batches
[2025-05-20 09:50:35.965137]: text loss 0.9299 | 58400/ 6642 batches
[2025-05-20 09:52:17.052130]: text loss 0.9333 | 58600/ 6642 batches
[2025-05-20 09:53:58.891045]: text loss 0.9472 | 58800/ 6642 batches
[2025-05-20 09:55:40.341610]: text loss 0.9252 | 59000/ 6642 batches
[2025-05-20 09:57:22.033897]: text loss 0.9287 | 59200/ 6642 batches
[2025-05-20 09:59:03.701621]: text loss 0.9263 | 59400/ 6642 batches
[2025-05-20 10:00:45.426082]: text loss 0.9218 | 59600/ 6642 batches
[2025-05-20 10:02:16.254866]: text loss 0.9211 | 59778/ 6642 batches
[2025-05-20 10:02:16.255652]: validation
[2025-05-20 10:02:19.203977]: explanation loss 2.1245
[2025-05-20 10:02:24.364425]: rationale loss 1.0533
[2025-05-20 10:02:34.879369]: sequential loss 2.2398
[2025-05-20 10:03:43.236087]: top-N loss 1.2474
[2025-05-20 10:03:43.700003]: epoch 10
[2025-05-20 10:03:54.869198]: text loss 0.9402 | 59800/ 6642 batches
[2025-05-20 10:05:36.999007]: text loss 0.9143 | 60000/ 6642 batches
[2025-05-20 10:07:19.281634]: text loss 0.9063 | 60200/ 6642 batches
[2025-05-20 10:09:01.171414]: text loss 0.9123 | 60400/ 6642 batches
[2025-05-20 10:10:43.251668]: text loss 0.9183 | 60600/ 6642 batches
[2025-05-20 10:12:25.026139]: text loss 0.9052 | 60800/ 6642 batches
[2025-05-20 10:14:06.365410]: text loss 0.9174 | 61000/ 6642 batches
[2025-05-20 10:15:48.098012]: text loss 0.9155 | 61200/ 6642 batches
[2025-05-20 10:17:29.732453]: text loss 0.9170 | 61400/ 6642 batches
[2025-05-20 10:19:11.633237]: text loss 0.9059 | 61600/ 6642 batches
[2025-05-20 10:20:53.143841]: text loss 0.9125 | 61800/ 6642 batches
[2025-05-20 10:22:34.323397]: text loss 0.9115 | 62000/ 6642 batches
[2025-05-20 10:24:15.820433]: text loss 0.9091 | 62200/ 6642 batches
[2025-05-20 10:25:57.893810]: text loss 0.9106 | 62400/ 6642 batches
[2025-05-20 10:27:39.845496]: text loss 0.9124 | 62600/ 6642 batches
[2025-05-20 10:29:21.507844]: text loss 0.9175 | 62800/ 6642 batches
[2025-05-20 10:31:03.127537]: text loss 0.9088 | 63000/ 6642 batches
[2025-05-20 10:32:44.298029]: text loss 0.9178 | 63200/ 6642 batches
[2025-05-20 10:34:25.660869]: text loss 0.9121 | 63400/ 6642 batches
[2025-05-20 10:36:07.123770]: text loss 0.9125 | 63600/ 6642 batches
[2025-05-20 10:37:48.180464]: text loss 0.9113 | 63800/ 6642 batches
[2025-05-20 10:39:30.338394]: text loss 0.9161 | 64000/ 6642 batches
[2025-05-20 10:41:12.100604]: text loss 0.9178 | 64200/ 6642 batches
[2025-05-20 10:42:53.971263]: text loss 0.9207 | 64400/ 6642 batches
[2025-05-20 10:44:36.069832]: text loss 0.9181 | 64600/ 6642 batches
[2025-05-20 10:46:17.757302]: text loss 0.9121 | 64800/ 6642 batches
[2025-05-20 10:48:00.045418]: text loss 0.9144 | 65000/ 6642 batches
[2025-05-20 10:49:42.135449]: text loss 0.9050 | 65200/ 6642 batches
[2025-05-20 10:51:23.973491]: text loss 0.9218 | 65400/ 6642 batches
[2025-05-20 10:53:05.054477]: text loss 0.9231 | 65600/ 6642 batches
[2025-05-20 10:54:46.375545]: text loss 0.9096 | 65800/ 6642 batches
[2025-05-20 10:56:27.091139]: text loss 0.9120 | 66000/ 6642 batches
[2025-05-20 10:58:07.783124]: text loss 0.9128 | 66200/ 6642 batches
[2025-05-20 10:59:48.287890]: text loss 0.9240 | 66400/ 6642 batches
[2025-05-20 10:59:58.395836]: text loss 0.8954 | 66420/ 6642 batches
[2025-05-20 10:59:58.396614]: validation
[2025-05-20 11:00:01.290564]: explanation loss 2.1257
[2025-05-20 11:00:06.416583]: rationale loss 1.0531
[2025-05-20 11:00:16.856253]: sequential loss 2.2425
[2025-05-20 11:01:25.090788]: top-N loss 1.2367
[2025-05-20 11:01:25.544165]: epoch 11
[2025-05-20 11:02:56.971149]: text loss 0.8929 | 66600/ 6642 batches
[2025-05-20 11:04:38.593318]: text loss 0.8963 | 66800/ 6642 batches
[2025-05-20 11:06:19.392567]: text loss 0.9006 | 67000/ 6642 batches
[2025-05-20 11:08:00.205331]: text loss 0.8998 | 67200/ 6642 batches
[2025-05-20 11:09:40.987976]: text loss 0.8990 | 67400/ 6642 batches
[2025-05-20 11:11:21.620449]: text loss 0.8929 | 67600/ 6642 batches
[2025-05-20 11:13:03.659640]: text loss 0.8976 | 67800/ 6642 batches
[2025-05-20 11:14:44.877016]: text loss 0.8967 | 68000/ 6642 batches
[2025-05-20 11:16:25.962466]: text loss 0.8975 | 68200/ 6642 batches
[2025-05-20 11:18:06.821316]: text loss 0.8939 | 68400/ 6642 batches
[2025-05-20 11:19:47.341570]: text loss 0.9002 | 68600/ 6642 batches
[2025-05-20 11:21:28.452615]: text loss 0.9059 | 68800/ 6642 batches
[2025-05-20 11:23:09.895686]: text loss 0.8983 | 69000/ 6642 batches
[2025-05-20 11:24:50.662136]: text loss 0.9035 | 69200/ 6642 batches
[2025-05-20 11:26:31.805488]: text loss 0.9030 | 69400/ 6642 batches
[2025-05-20 11:28:13.228748]: text loss 0.8999 | 69600/ 6642 batches
[2025-05-20 11:29:54.842555]: text loss 0.9022 | 69800/ 6642 batches
[2025-05-20 11:31:36.898767]: text loss 0.8992 | 70000/ 6642 batches
[2025-05-20 11:33:18.487641]: text loss 0.9056 | 70200/ 6642 batches
[2025-05-20 11:34:59.944288]: text loss 0.9029 | 70400/ 6642 batches
[2025-05-20 11:36:41.950793]: text loss 0.9002 | 70600/ 6642 batches
[2025-05-20 11:38:23.923118]: text loss 0.9041 | 70800/ 6642 batches
[2025-05-20 11:40:05.335026]: text loss 0.9024 | 71000/ 6642 batches
[2025-05-20 11:41:47.504118]: text loss 0.8996 | 71200/ 6642 batches
[2025-05-20 11:43:29.603101]: text loss 0.9132 | 71400/ 6642 batches
[2025-05-20 11:45:10.822434]: text loss 0.9068 | 71600/ 6642 batches
[2025-05-20 11:46:52.418072]: text loss 0.8955 | 71800/ 6642 batches
[2025-05-20 11:48:33.906335]: text loss 0.9081 | 72000/ 6642 batches
[2025-05-20 11:50:15.191230]: text loss 0.9039 | 72200/ 6642 batches
[2025-05-20 11:51:56.156079]: text loss 0.9038 | 72400/ 6642 batches
[2025-05-20 11:53:37.015035]: text loss 0.9016 | 72600/ 6642 batches
[2025-05-20 11:55:18.113884]: text loss 0.9139 | 72800/ 6642 batches
[2025-05-20 11:56:59.073478]: text loss 0.9082 | 73000/ 6642 batches
[2025-05-20 11:57:30.518706]: text loss 0.8834 | 73062/ 6642 batches
[2025-05-20 11:57:30.519729]: validation
[2025-05-20 11:57:33.458595]: explanation loss 2.1245
[2025-05-20 11:57:38.598727]: rationale loss 1.0529
[2025-05-20 11:57:48.889474]: sequential loss 2.2262
[2025-05-20 11:58:56.343731]: top-N loss 1.2324
[2025-05-20 11:58:56.807760]: epoch 12
[2025-05-20 12:00:06.726512]: text loss 0.8871 | 73200/ 6642 batches
[2025-05-20 12:01:48.115646]: text loss 0.8822 | 73400/ 6642 batches
[2025-05-20 12:03:29.020014]: text loss 0.8870 | 73600/ 6642 batches
[2025-05-20 12:05:10.324600]: text loss 0.8862 | 73800/ 6642 batches
[2025-05-20 12:06:51.790360]: text loss 0.8897 | 74000/ 6642 batches
[2025-05-20 12:08:32.893984]: text loss 0.8795 | 74200/ 6642 batches
[2025-05-20 12:10:14.235285]: text loss 0.8728 | 74400/ 6642 batches
[2025-05-20 12:11:56.191499]: text loss 0.8955 | 74600/ 6642 batches
[2025-05-20 12:13:38.304227]: text loss 0.8858 | 74800/ 6642 batches
[2025-05-20 12:15:20.493544]: text loss 0.8920 | 75000/ 6642 batches
[2025-05-20 12:17:01.733866]: text loss 0.8843 | 75200/ 6642 batches
[2025-05-20 12:18:43.098635]: text loss 0.8917 | 75400/ 6642 batches
[2025-05-20 12:20:24.818622]: text loss 0.8970 | 75600/ 6642 batches
[2025-05-20 12:22:06.477895]: text loss 0.8913 | 75800/ 6642 batches
[2025-05-20 12:23:47.830739]: text loss 0.8925 | 76000/ 6642 batches
[2025-05-20 12:25:29.359658]: text loss 0.8906 | 76200/ 6642 batches
[2025-05-20 12:27:10.995843]: text loss 0.8824 | 76400/ 6642 batches
[2025-05-20 12:28:52.256707]: text loss 0.8878 | 76600/ 6642 batches
[2025-05-20 12:30:34.327002]: text loss 0.8856 | 76800/ 6642 batches
[2025-05-20 12:32:15.629883]: text loss 0.8853 | 77000/ 6642 batches
[2025-05-20 12:33:57.269432]: text loss 0.8843 | 77200/ 6642 batches
[2025-05-20 12:35:39.280628]: text loss 0.8952 | 77400/ 6642 batches
[2025-05-20 12:37:20.638931]: text loss 0.8962 | 77600/ 6642 batches
[2025-05-20 12:39:02.768900]: text loss 0.8936 | 77800/ 6642 batches
[2025-05-20 12:40:45.083456]: text loss 0.8919 | 78000/ 6642 batches
[2025-05-20 12:42:27.498921]: text loss 0.8918 | 78200/ 6642 batches
[2025-05-20 12:44:09.964300]: text loss 0.8910 | 78400/ 6642 batches
[2025-05-20 12:45:52.312556]: text loss 0.8918 | 78600/ 6642 batches
[2025-05-20 12:47:33.698882]: text loss 0.8863 | 78800/ 6642 batches
[2025-05-20 12:49:15.292200]: text loss 0.8849 | 79000/ 6642 batches
[2025-05-20 12:50:57.492238]: text loss 0.8911 | 79200/ 6642 batches
[2025-05-20 12:52:39.833415]: text loss 0.9008 | 79400/ 6642 batches
[2025-05-20 12:54:22.311926]: text loss 0.8937 | 79600/ 6642 batches
[2025-05-20 12:55:15.718623]: text loss 0.8921 | 79704/ 6642 batches
[2025-05-20 12:55:15.719433]: validation
[2025-05-20 12:55:18.661652]: explanation loss 2.1224
[2025-05-20 12:55:23.806398]: rationale loss 1.0532
[2025-05-20 12:55:34.026370]: sequential loss 2.2278
[2025-05-20 12:56:40.997553]: top-N loss 1.2314
[2025-05-20 12:56:41.462884]: epoch 13
[2025-05-20 12:57:30.464069]: text loss 0.8688 | 79800/ 6642 batches
[2025-05-20 12:59:12.732316]: text loss 0.8676 | 80000/ 6642 batches
[2025-05-20 13:00:54.824134]: text loss 0.8698 | 80200/ 6642 batches
[2025-05-20 13:02:36.832015]: text loss 0.8606 | 80400/ 6642 batches
[2025-05-20 13:04:18.873326]: text loss 0.8701 | 80600/ 6642 batches
[2025-05-20 13:06:01.157511]: text loss 0.8669 | 80800/ 6642 batches
[2025-05-20 13:07:43.423514]: text loss 0.8806 | 81000/ 6642 batches
[2025-05-20 13:09:25.991041]: text loss 0.8657 | 81200/ 6642 batches
[2025-05-20 13:11:08.473287]: text loss 0.8738 | 81400/ 6642 batches
[2025-05-20 13:12:49.823471]: text loss 0.8763 | 81600/ 6642 batches
[2025-05-20 13:14:30.642073]: text loss 0.8759 | 81800/ 6642 batches
[2025-05-20 13:16:12.499627]: text loss 0.8780 | 82000/ 6642 batches
[2025-05-20 13:17:54.761267]: text loss 0.8728 | 82200/ 6642 batches
[2025-05-20 13:19:36.169302]: text loss 0.8759 | 82400/ 6642 batches
[2025-05-20 13:21:17.982671]: text loss 0.8660 | 82600/ 6642 batches
[2025-05-20 13:23:00.095693]: text loss 0.8793 | 82800/ 6642 batches
[2025-05-20 13:24:41.330583]: text loss 0.8778 | 83000/ 6642 batches
[2025-05-20 13:26:23.475455]: text loss 0.8906 | 83200/ 6642 batches
[2025-05-20 13:28:05.552297]: text loss 0.8749 | 83400/ 6642 batches
[2025-05-20 13:29:47.921954]: text loss 0.8807 | 83600/ 6642 batches
[2025-05-20 13:31:29.171496]: text loss 0.8818 | 83800/ 6642 batches
[2025-05-20 13:33:10.050195]: text loss 0.8823 | 84000/ 6642 batches
[2025-05-20 13:34:50.610372]: text loss 0.8804 | 84200/ 6642 batches
[2025-05-20 13:36:32.392369]: text loss 0.8773 | 84400/ 6642 batches
[2025-05-20 13:38:14.475633]: text loss 0.8798 | 84600/ 6642 batches
[2025-05-20 13:39:56.646886]: text loss 0.8781 | 84800/ 6642 batches
[2025-05-20 13:41:38.678167]: text loss 0.8831 | 85000/ 6642 batches
[2025-05-20 13:43:20.847207]: text loss 0.8786 | 85200/ 6642 batches
[2025-05-20 13:45:01.966702]: text loss 0.8827 | 85400/ 6642 batches
[2025-05-20 13:46:43.685731]: text loss 0.8841 | 85600/ 6642 batches
[2025-05-20 13:48:26.162267]: text loss 0.8740 | 85800/ 6642 batches
[2025-05-20 13:50:08.362532]: text loss 0.8860 | 86000/ 6642 batches
[2025-05-20 13:51:50.531146]: text loss 0.8847 | 86200/ 6642 batches
[2025-05-20 13:53:04.355281]: text loss 0.8782 | 86346/ 6642 batches
[2025-05-20 13:53:04.356089]: validation
[2025-05-20 13:53:07.225676]: explanation loss 2.1306
[2025-05-20 13:53:12.388805]: rationale loss 1.0528
[2025-05-20 13:53:22.731081]: sequential loss 2.2809
[2025-05-20 13:54:30.547689]: top-N loss 1.2151
[2025-05-20 13:54:30.547796]: Endured 1 time(s)
[2025-05-20 13:54:30.547823]: epoch 14
[2025-05-20 13:54:57.814281]: text loss 0.8461 | 86400/ 6642 batches
[2025-05-20 13:56:38.532174]: text loss 0.8607 | 86600/ 6642 batches
[2025-05-20 13:58:19.414504]: text loss 0.8539 | 86800/ 6642 batches
[2025-05-20 14:00:00.227696]: text loss 0.8534 | 87000/ 6642 batches
[2025-05-20 14:01:40.894338]: text loss 0.8564 | 87200/ 6642 batches
[2025-05-20 14:03:21.796594]: text loss 0.8582 | 87400/ 6642 batches
[2025-05-20 14:05:02.588224]: text loss 0.8602 | 87600/ 6642 batches
[2025-05-20 14:06:43.817418]: text loss 0.8656 | 87800/ 6642 batches
[2025-05-20 14:08:24.792544]: text loss 0.8601 | 88000/ 6642 batches
[2025-05-20 14:10:05.760367]: text loss 0.8657 | 88200/ 6642 batches
[2025-05-20 14:11:46.574658]: text loss 0.8677 | 88400/ 6642 batches
[2025-05-20 14:13:27.451191]: text loss 0.8582 | 88600/ 6642 batches
[2025-05-20 14:15:08.322692]: text loss 0.8673 | 88800/ 6642 batches
[2025-05-20 14:16:49.444975]: text loss 0.8617 | 89000/ 6642 batches
[2025-05-20 14:18:30.882839]: text loss 0.8647 | 89200/ 6642 batches
[2025-05-20 14:20:11.945107]: text loss 0.8578 | 89400/ 6642 batches
[2025-05-20 14:21:52.960995]: text loss 0.8599 | 89600/ 6642 batches
[2025-05-20 14:23:34.471346]: text loss 0.8631 | 89800/ 6642 batches
[2025-05-20 14:25:15.574689]: text loss 0.8728 | 90000/ 6642 batches
[2025-05-20 14:26:57.569301]: text loss 0.8615 | 90200/ 6642 batches
[2025-05-20 14:28:39.044701]: text loss 0.8820 | 90400/ 6642 batches
[2025-05-20 14:30:20.360483]: text loss 0.8734 | 90600/ 6642 batches
[2025-05-20 14:32:01.075268]: text loss 0.8568 | 90800/ 6642 batches
[2025-05-20 14:33:42.637794]: text loss 0.8691 | 91000/ 6642 batches
[2025-05-20 14:35:24.006645]: text loss 0.8709 | 91200/ 6642 batches
[2025-05-20 14:37:05.176275]: text loss 0.8649 | 91400/ 6642 batches
[2025-05-20 14:38:46.813946]: text loss 0.8691 | 91600/ 6642 batches
[2025-05-20 14:40:28.930058]: text loss 0.8664 | 91800/ 6642 batches
[2025-05-20 14:42:11.279749]: text loss 0.8711 | 92000/ 6642 batches
[2025-05-20 14:43:52.946666]: text loss 0.8759 | 92200/ 6642 batches
[2025-05-20 14:45:34.626276]: text loss 0.8627 | 92400/ 6642 batches
[2025-05-20 14:47:16.717038]: text loss 0.8640 | 92600/ 6642 batches
[2025-05-20 14:48:58.710101]: text loss 0.8689 | 92800/ 6642 batches
[2025-05-20 14:50:34.873160]: text loss 0.8604 | 92988/ 6642 batches
[2025-05-20 14:50:34.874331]: validation
[2025-05-20 14:50:37.841492]: explanation loss 2.1310
[2025-05-20 14:50:43.004965]: rationale loss 1.0540
[2025-05-20 14:50:53.411375]: sequential loss 2.2905
[2025-05-20 14:52:01.182912]: top-N loss 1.2017
[2025-05-20 14:52:01.183014]: Endured 2 time(s)
[2025-05-20 14:52:01.183040]: epoch 15
[2025-05-20 14:52:07.253088]: text loss 0.8646 | 93000/ 6642 batches
[2025-05-20 14:53:49.317030]: text loss 0.8472 | 93200/ 6642 batches
[2025-05-20 14:55:31.080927]: text loss 0.8464 | 93400/ 6642 batches
[2025-05-20 14:57:12.976090]: text loss 0.8485 | 93600/ 6642 batches
[2025-05-20 14:58:54.679099]: text loss 0.8391 | 93800/ 6642 batches
[2025-05-20 15:00:35.893245]: text loss 0.8438 | 94000/ 6642 batches
[2025-05-20 15:02:16.969077]: text loss 0.8442 | 94200/ 6642 batches
[2025-05-20 15:03:58.083339]: text loss 0.8489 | 94400/ 6642 batches
[2025-05-20 15:05:39.098960]: text loss 0.8389 | 94600/ 6642 batches
[2025-05-20 15:07:19.991023]: text loss 0.8503 | 94800/ 6642 batches
[2025-05-20 15:09:01.295684]: text loss 0.8530 | 95000/ 6642 batches
[2025-05-20 15:10:43.142855]: text loss 0.8484 | 95200/ 6642 batches
[2025-05-20 15:12:24.891269]: text loss 0.8595 | 95400/ 6642 batches
[2025-05-20 15:14:06.766383]: text loss 0.8464 | 95600/ 6642 batches
[2025-05-20 15:15:48.048451]: text loss 0.8520 | 95800/ 6642 batches
[2025-05-20 15:17:28.686828]: text loss 0.8505 | 96000/ 6642 batches
[2025-05-20 15:19:09.733287]: text loss 0.8581 | 96200/ 6642 batches
[2025-05-20 15:20:50.658336]: text loss 0.8495 | 96400/ 6642 batches
[2025-05-20 15:22:31.360987]: text loss 0.8551 | 96600/ 6642 batches
[2025-05-20 15:24:12.116023]: text loss 0.8389 | 96800/ 6642 batches
[2025-05-20 15:25:52.800122]: text loss 0.8561 | 97000/ 6642 batches
[2025-05-20 15:27:33.415768]: text loss 0.8496 | 97200/ 6642 batches
[2025-05-20 15:29:14.426114]: text loss 0.8505 | 97400/ 6642 batches
[2025-05-20 15:30:55.560455]: text loss 0.8526 | 97600/ 6642 batches
[2025-05-20 15:32:36.506787]: text loss 0.8507 | 97800/ 6642 batches
[2025-05-20 15:34:17.517412]: text loss 0.8560 | 98000/ 6642 batches
[2025-05-20 15:35:58.364900]: text loss 0.8628 | 98200/ 6642 batches
[2025-05-20 15:37:39.077603]: text loss 0.8528 | 98400/ 6642 batches
[2025-05-20 15:39:20.042178]: text loss 0.8564 | 98600/ 6642 batches
[2025-05-20 15:41:00.914982]: text loss 0.8590 | 98800/ 6642 batches
[2025-05-20 15:42:41.900130]: text loss 0.8583 | 99000/ 6642 batches
[2025-05-20 15:44:23.037942]: text loss 0.8595 | 99200/ 6642 batches
[2025-05-20 15:46:04.316565]: text loss 0.8620 | 99400/ 6642 batches
[2025-05-20 15:47:45.296588]: text loss 0.8680 | 99600/ 6642 batches
[2025-05-20 15:48:00.570721]: text loss 0.8746 | 99630/ 6642 batches
[2025-05-20 15:48:00.571701]: validation
[2025-05-20 15:48:03.423861]: explanation loss 2.1365
[2025-05-20 15:48:08.586267]: rationale loss 1.0561
[2025-05-20 15:48:19.186066]: sequential loss 2.3132
[2025-05-20 15:49:28.084665]: top-N loss 1.2013
[2025-05-20 15:49:28.084784]: Endured 3 time(s)
[2025-05-20 15:49:28.084809]: epoch 16
[2025-05-20 15:50:53.970588]: text loss 0.8268 | 99800/ 6642 batches
[2025-05-20 15:52:35.163175]: text loss 0.8243 | 100000/ 6642 batches
[2025-05-20 15:54:16.512261]: text loss 0.8309 | 100200/ 6642 batches
[2025-05-20 15:55:57.771627]: text loss 0.8388 | 100400/ 6642 batches
[2025-05-20 15:57:38.816096]: text loss 0.8311 | 100600/ 6642 batches
[2025-05-20 15:59:20.112122]: text loss 0.8312 | 100800/ 6642 batches
[2025-05-20 16:01:01.316833]: text loss 0.8339 | 101000/ 6642 batches
[2025-05-20 16:02:42.394482]: text loss 0.8317 | 101200/ 6642 batches
[2025-05-20 16:04:23.262394]: text loss 0.8307 | 101400/ 6642 batches
[2025-05-20 16:06:04.638617]: text loss 0.8411 | 101600/ 6642 batches
[2025-05-20 16:07:45.861635]: text loss 0.8399 | 101800/ 6642 batches
[2025-05-20 16:09:27.046710]: text loss 0.8380 | 102000/ 6642 batches
[2025-05-20 16:11:08.460336]: text loss 0.8307 | 102200/ 6642 batches
[2025-05-20 16:12:50.104449]: text loss 0.8302 | 102400/ 6642 batches
[2025-05-20 16:14:31.149316]: text loss 0.8447 | 102600/ 6642 batches
[2025-05-20 16:16:12.197105]: text loss 0.8367 | 102800/ 6642 batches
[2025-05-20 16:17:53.118179]: text loss 0.8409 | 103000/ 6642 batches
[2025-05-20 16:19:34.047026]: text loss 0.8478 | 103200/ 6642 batches
[2025-05-20 16:21:15.376942]: text loss 0.8438 | 103400/ 6642 batches
[2025-05-20 16:22:56.631279]: text loss 0.8397 | 103600/ 6642 batches
[2025-05-20 16:24:37.759758]: text loss 0.8470 | 103800/ 6642 batches
[2025-05-20 16:26:18.799036]: text loss 0.8395 | 104000/ 6642 batches
[2025-05-20 16:28:00.154304]: text loss 0.8396 | 104200/ 6642 batches
[2025-05-20 16:29:41.213978]: text loss 0.8447 | 104400/ 6642 batches
[2025-05-20 16:31:22.281433]: text loss 0.8444 | 104600/ 6642 batches
[2025-05-20 16:33:03.437552]: text loss 0.8435 | 104800/ 6642 batches
[2025-05-20 16:34:44.509663]: text loss 0.8485 | 105000/ 6642 batches
[2025-05-20 16:36:25.560851]: text loss 0.8397 | 105200/ 6642 batches
[2025-05-20 16:38:06.848074]: text loss 0.8477 | 105400/ 6642 batches
[2025-05-20 16:39:47.979046]: text loss 0.8413 | 105600/ 6642 batches
[2025-05-20 16:41:28.982250]: text loss 0.8491 | 105800/ 6642 batches
[2025-05-20 16:43:10.268048]: text loss 0.8502 | 106000/ 6642 batches
[2025-05-20 16:44:51.617948]: text loss 0.8501 | 106200/ 6642 batches
[2025-05-20 16:45:28.056137]: text loss 0.8403 | 106272/ 6642 batches
[2025-05-20 16:45:28.056904]: validation
[2025-05-20 16:45:30.923741]: explanation loss 2.1479
[2025-05-20 16:45:36.098000]: rationale loss 1.0580
[2025-05-20 16:45:46.548031]: sequential loss 2.3456
[2025-05-20 16:46:55.520458]: top-N loss 1.2007
[2025-05-20 16:46:55.520588]: Endured 4 time(s)
[2025-05-20 16:46:55.520624]: epoch 17
[2025-05-20 16:48:00.155968]: text loss 0.8137 | 106400/ 6642 batches
[2025-05-20 16:49:40.924362]: text loss 0.8184 | 106600/ 6642 batches
[2025-05-20 16:51:21.609883]: text loss 0.8121 | 106800/ 6642 batches
[2025-05-20 16:53:02.350794]: text loss 0.8222 | 107000/ 6642 batches
[2025-05-20 16:54:43.190521]: text loss 0.8241 | 107200/ 6642 batches
[2025-05-20 16:56:24.171727]: text loss 0.8335 | 107400/ 6642 batches
[2025-05-20 16:58:04.964421]: text loss 0.8204 | 107600/ 6642 batches
[2025-05-20 16:59:46.109670]: text loss 0.8227 | 107800/ 6642 batches
[2025-05-20 17:01:27.060428]: text loss 0.8268 | 108000/ 6642 batches
[2025-05-20 17:03:07.918591]: text loss 0.8308 | 108200/ 6642 batches
[2025-05-20 17:04:48.904717]: text loss 0.8242 | 108400/ 6642 batches
[2025-05-20 17:06:29.712936]: text loss 0.8269 | 108600/ 6642 batches
[2025-05-20 17:08:10.637249]: text loss 0.8233 | 108800/ 6642 batches
[2025-05-20 17:09:51.647146]: text loss 0.8253 | 109000/ 6642 batches
[2025-05-20 17:11:32.723191]: text loss 0.8255 | 109200/ 6642 batches
[2025-05-20 17:13:13.666259]: text loss 0.8220 | 109400/ 6642 batches
[2025-05-20 17:14:54.653708]: text loss 0.8201 | 109600/ 6642 batches
[2025-05-20 17:16:35.579097]: text loss 0.8226 | 109800/ 6642 batches
[2025-05-20 17:18:16.512309]: text loss 0.8255 | 110000/ 6642 batches
[2025-05-20 17:19:57.454494]: text loss 0.8303 | 110200/ 6642 batches
[2025-05-20 17:21:38.567505]: text loss 0.8345 | 110400/ 6642 batches
[2025-05-20 17:23:19.746201]: text loss 0.8365 | 110600/ 6642 batches
[2025-05-20 17:25:00.744226]: text loss 0.8297 | 110800/ 6642 batches
[2025-05-20 17:26:41.438527]: text loss 0.8339 | 111000/ 6642 batches
[2025-05-20 17:28:22.344800]: text loss 0.8287 | 111200/ 6642 batches
[2025-05-20 17:30:03.314657]: text loss 0.8312 | 111400/ 6642 batches
[2025-05-20 17:31:44.360818]: text loss 0.8302 | 111600/ 6642 batches
[2025-05-20 17:33:25.349057]: text loss 0.8281 | 111800/ 6642 batches
[2025-05-20 17:35:06.277430]: text loss 0.8298 | 112000/ 6642 batches
[2025-05-20 17:36:47.074775]: text loss 0.8229 | 112200/ 6642 batches
[2025-05-20 17:38:28.010158]: text loss 0.8314 | 112400/ 6642 batches
[2025-05-20 17:40:08.924561]: text loss 0.8337 | 112600/ 6642 batches
[2025-05-20 17:41:50.150558]: text loss 0.8360 | 112800/ 6642 batches
[2025-05-20 17:42:47.698684]: text loss 0.8388 | 112914/ 6642 batches
[2025-05-20 17:42:47.699539]: validation
[2025-05-20 17:42:50.579787]: explanation loss 2.1559
[2025-05-20 17:42:55.755967]: rationale loss 1.0641
[2025-05-20 17:43:06.277326]: sequential loss 2.4371
[2025-05-20 17:44:15.616771]: top-N loss 1.1971
[2025-05-20 17:44:15.616879]: Endured 5 time(s)
[2025-05-20 17:44:15.616901]: Cannot endure it anymore | Exiting from early stop
